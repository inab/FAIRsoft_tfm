[
    {
        "ciation": null,
        "code_file": null,
        "command": "ARTS.pl -i $input -o $out -b $batch -c \"$column\" -cc $conts -cd $dates -cb $bins -bn $bname -s $seed -mmi -v l ",
        "dataFormats": {
            "inputs": [
                "tabular"
            ],
            "outputs": [
                "tabular"
            ]
        },
        "description": "automated study randomization",
        "help": "\n  \n**Purpose**\n\nThis tool completes automated study randomization for a selected number of traits over the samples in your data set by minimizing a mutual information-based objective function using a genetic algorithm.\n\nNOTE: in the history output, click the i (view details) icon (between the save-to-disk and rerun icons), then click on stdout to see a summary of the run. This allows you to confirm which traits are being considered, and gives you a snapshot of how randomized individual traits are (it does not inform you about combinations of traits, which ARE ALSO being randomized).\n\n-----\n\n**Input traits per sample**\n\n- A list of traits associated with each sample, including a header line giving the name of each type of trait. For example::\n     \n       ID      Sex    Age  Sample date  Diseased\n       Sample1   M     15     6/7/2011         Y\n       Sample2   M     25     8/5/2012         Y\n       Sample3   F     23    1/30/2012         N\n       Sample4   F     45     4/1/2013         N\n       Sample5   M     52    3/21/2011         Y\n       Sample6   F     37    3/12/2013         N\n       Sample7   M     31    7/17/2011         N\n\n-----\n\n**Batch size**\n\n- The size of each batch. You can specify this with a single number (e.g., 50), or a list of numbers (separated by commas, for example 50,50,49,49.\n\n- The first choice will fill up full batches as much as possible, and put all remaining samples in a smaller batch. Thus, the latter choice may be better if the batch size does not evenly divide the number of samples. For example, lets say you have 105 samples and can do a batch size of up to 30. Then::\n\n       (First option)  Batch size = 30 --> batch sizes of 30, 30, 30, and 15\n        -or-\n       (Second option) Batch size = 27,26,26,26\n     \nThe second option has a more evenly distributed batch size, and will give better results.\n\n-----\n\n**Traits to randomize**\n\n- Which traits should be randomized. On Macs, hold command to multi-select. You do not need to select all columns (it would be silly, for example, to randomize over sample ID).\n\n- Note missing values for traits will be treated as an additional trait value (i.e., empty).\n\n- For the example above, we would select c2, c3, c4, and c5 (Sex, Age, Sample date, and Diseased). Not all traits need be selected, just the relevant ones (we may not care about Sample date, for example).\n\n-----\n\n**Continuous- and date-valued columns (optional)**\n\n- Use if you have columns with continuous values (e.g., age, blood pressure) or dates. They will be discretized prior to running.\n\n- For the example above, we would select c3 and c4 (Age, Sample date).\n\n-----\n\n**Date-valued columns (optional)**\n\n- Use if any of the columns selected as continuous are dates (MUST be formatted M/D/Y, where month is a number, for example 7/9/1985).\n\n- For the example above, we would select c4 (Sample date).\n\n-----\n\n**Bin sizes**\n\n- This only relates to any columns selected as continuous, and determines how many discrete bins the data will be split up in to.\n\n- You can set it to a single number, and all columns will use that number of bins. Or you can set it to a list of numbers to specify a different number of bins for each column.\n\n- For the example above, where we selected c3 and c4 as continuous, we could set::\n \n      Bin sizes=5,6\n\n- which would split the Age column (c3) into 5 bins, and the Sample date column (c4) into 6 bins.\n\n-----\n\n**Batch name**\n\n- The output file will look exactly the same as the input, except an additional column will be added indicated which batch each sample should belong to. You can specify the name of that column here.\n\n-----\n\n**Random number seed**\n\n- This will not be need to be changed in general, but if you want to force the use of a different seed, you can.\n\n\n\n\n",
        "id": "ARTS",
        "interpreter": "perl",
        "language": null,
        "name": "ARTS",
        "readme": true,
        "tests": false,
        "version": null
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btr174"
            }
        ],
        "code_file": null,
        "command": "\n        \n            cp '$script_file' '$out_file2' &&\n            ln -s '${input_bam}' localbam.bam &&\n            ln -s '${input_bam.metadata.bam_index}' localbam.bam.bai &&\n            cat '$script_file' &&\n            bamtools filter -script '$script_file' -in localbam.bam -out '$out_file1'\n        \n    ",
        "dataFormats": {
            "inputs": [
                "bam"
            ],
            "outputs": [
                "txt",
                "bam"
            ]
        },
        "description": "BAM datasets on a variety of attributes",
        "help": "\n\n**What is does**\n\nBAMTools filter is a very powerful utility to perform complex filtering of BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools).\n\n-----\n\n**How it works**\n\nThe tool use logic relies on the three concepts: (1) input BAM, (2) groups, and (3) filters.\n\n*Input BAM(s)*\n\nThe input BAM is self-explanatory. This is the dataset you will be filtering. The tool can accept just one or multiple BAM files. To filter on multiple BAMs just add them by clicking **Add new BAM dataset(s) to filter**\n\n*Conditions and Filters*\n\nConditions for filtering BAM files can be arranged in **Groups and Filters**. While it can be confusing at first this is what gives ultimate power to this tools. So try to look at the examples we are supplying below.\n\n-----\n\n**Example 1. Using a single filter**\n\nWhen filtering on a single condition there is no need to worry about filters and conditions. Just choose a filter from the **Select BAM property to filter on:** dropdown and enter a value (or click a checkbox for binary filters).\nFor example, for retaining reads with mapping quality of at least 20 one would set the tool interface as shown below:\n\n.. image:: single-filter.png\n\n-----\n\n**Example 2. Using multiple filters**\n\nNow suppose one needs to extract reads that (1) have mapping quality of at least 20, (2) contain at least 1 mismatch, and (3) are mapping onto forward strand only.\nTo do so we will use three filters as shown below (multiple filters are added to the interface by clicking on the **Add new Filter** button):\n\n.. image:: multiple-filters.png\n\nIn this case (you can see that the three filters are grouped within a single Condition - **Condition 1**) the filter too use logical **AND** to perform filtering.\nIn other words only reads that (1) have mapping quality of at least 20 **AND** (2) contain at least 1 mismatch **AND** are mapping onto forward strand will be returned in this example.\n\n-----\n\n**Example 3. Complex filtering with multiple conditions**\n\nSuppose now you would like to select **either** reads that (**1**) have (*1.1*) no mismatches and (*1.2*) are on the forward strand **OR** (**2**) reads that have (*2.1*)\nat least one mismatch and (*2.2*) are on the reverse strand. In this scenario we have to set up two conditions: (**1**) and (**2**) each with two filters: *1.1* and *1.2* as well as *2.1* and *2.2*.\nThe following screenshot expalins how this can be done:\n\n.. image:: complex-filters.png\n\n-----\n\n**Example 4. Even more complex filtering with Rules**\n\nIn the above example we have used two conditions (Condition 1 and Condition 2). Using multiple conditions allows to combine them and a variety of ways to enable even more powerful filtering.\nFor example, suppose get all reads that (**1**) do NOT map to mitochondria and either (**2**) have mapping quality over 20, or (**3**) are in properly mapped pairs. The logical rule to enable such\nfiltering will look like this::\n\n !(1) & (2 | 3)\n\nHere, numbers 1, 2, and 3 represent conditions. The following screenshot illustrates how to do this in Galaxy:\n\n.. image:: rule.png\n\nThere are three conditions here, each with a single filter. A text entry area that can be opened by clicking on the **Would you like to set rules?** checkbox enables you to enter a rule.\nHere numbers correspond to numbers of conditions as they are shown in the interface. E.g., 1 corresponds to condition 1, 2 to condition 2 and so on... In human language this means::\n\n NOT condition 1 AND (condition 2 OR condition 3)\n\n-----\n\n**JSON script file**\n\nThis tool produces two outputs. One of the them is a BAM file containing filtered reads. The other is a JSONified script. It can help you to see how your instructions are sent to BAMTools.\nFor instance, the example 4 looks like this in the JSON form::\n\n       {\n        \"filters\":\n        [\n          { \"id\": \"1\",\n            \"tag\":\"NM:=0\",\n            \"isReverseStrand\":\"false\"\n          },\n          { \"id\": \"2\",\n            \"tag\":\"NM:>0\",\n            \"isReverseStrand\":\"true\"\n          }\n        ]\n      }\n\n\n-----\n\n**More information**\n\n.. class:: infomark\n\nAdditional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki\n\n    ",
        "id": "bamFilter",
        "interpreter": null,
        "language": null,
        "name": "Filter",
        "readme": false,
        "tests": true,
        "version": "2.4.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "beast.py -T 4 -r $strict -s $seed -i $inputxml -o $operators -l $log -t $trees -d $trees.id -p $__new_file_path__",
        "dataFormats": {
            "inputs": [
                "xml"
            ],
            "outputs": [
                "txt",
                "txt",
                "nexus"
            ]
        },
        "description": "Bayesian MCMC analysis of molecular sequences.",
        "help": "\n.. class:: warningmark\n\nThe input dataset needs to be in BEAST XML format. The names of the log output files configured in the xml file should have the '.log' extension and the trees file(s) should have the '.tree' extension for the best presentation.\n\nIf the random seed is not chosen \"12345\" will be used.\n\n-----\n\n**BEAST v1.7.1, 2002-2012**\n\nBayesian Evolutionary Analysis Sampling Trees\n\nDesigned and developed by\n\n\nAlexei J. Drummond - \nDepartment of Computer Science, \nUniversity of Auckland, \nalexei@cs.auckland.ac.nz\n\nAndrew Rambaut - \nInstitute of Evolutionary Biology, \nUniversity of Edinburgh, \na.rambaut@ed.ac.uk\n\nMarc A. Suchard - David Geffen School of Medicine, \nUniversity of California, Los Angeles, \nmsuchard@ucla.edu\n\nDownloads, Help and Resources: http://beast.bio.ed.ac.uk\n\nSource code distributed under the GNU Lesser General Public License:\nhttp://code.google.com/p/beast-mcmc\n\nBEAST developers:\nAlex Alekseyenko, Trevor Bedford, Erik Bloomquist, Joseph Heled, Sebastian\nHoehna, Denise Kuehnert, Philippe Lemey, Wai Lok Sibon Li, Gerton Lunter,\nSidney Markowitz, Vladimir Minin, Michael Defoin Platel, Oliver Pybus,\nChieh-Hsi Wu, Walter Xie\n\nThanks to: Roald Forsberg, Beth Shapiro and Korbinian Strimmer\n",
        "id": "beast",
        "interpreter": "python",
        "language": null,
        "name": "Beast",
        "readme": false,
        "tests": false,
        "version": "1.1"
    },
    {
        "ciation": [
            {
                "citation": "10.7717/peerj.167"
            },
            {
                "citation": "10.1093/nar/gkn176"
            },
            {
                "citation": "10.1155/2008/619832"
            },
            {
                "citation": "10.1093/bioinformatics/bti610"
            }
        ],
        "code_file": null,
        "command": "\n        blast2go.py \"${xml}\" \"${prop.fields.path}\" \"${tab}\"\n    ",
        "dataFormats": {
            "inputs": [
                "blastxml"
            ],
            "outputs": [
                "tabular"
            ]
        },
        "description": "Maps BLAST results to GO annotation terms",
        "help": "\n.. class:: warningmark\n\n**Note**. Blast2GO may take a substantial amount of time, especially if\nrunning against the public server in Spain. For large input datasets it\nis advisable to allow overnight processing, or consider subdividing.\n\n-----\n\n**What it does**\n\nThis runs b2g4Pipe v2.5, which is the command line (no GUI) version of\nBlast2GO designed for use in pipelines.\n\nIt takes as input BLAST XML results against a protein database, typically\nthe NCBI non-redundant (NR) database. This tool will accept concatenated\nBLAST XML files (although they are technically invalid XML), which is very\nuseful if you have sub-divided your protein FASTA files and run BLAST on\nthem in batches.\n\nThe BLAST matches are used to assign Gene Ontology (GO) annotation terms\nto each query sequence.\n\nThe output from this tool is a tabular file containing three columns, with\nthe order taken from query order in the original BLAST XML file:\n\n====== ====================\nColumn Description\n------ --------------------\n     1 ID of query sequence\n     2 GO term\n     3 GO description\n====== ====================\n\nNote that if no GO terms are assigned to a sequence (e.g. if it had no\nBLAST matches), then it will not be present in the output file.\n\nThis tabular file is called an \"Annotation File\" in the Blast2GO GUI.\nIf you download the tabular file, and rename it to use the extension\n\".annot\", then it can be opened with the Blast2GO GUI via the \"File\",\n\"Load Annotation (.annot)\" menu (keyboard shortcut ALT+L). You can\nthen run some of the interactive analyses offered in the GUI tool.\n\n\n**Advanced Settings**\n\nBlast2GO has a properties setting file which includes which database\nserver to connect to (e.g. the public server in Valencia, Spain, or a\nlocal server), as well as more advanced options such as thresholds and\nevidence code weights. To change these settings, your Galaxy administrator\nmust create a new properties file, and add it to the drop down menu above.\n\n\n**References**\n\nIf you use this Galaxy tool in work leading to a scientific publication please\ncite the following papers:\n\nPeter Cock, Bjoern Gruening, Konrad Paszkiewicz and Leighton Pritchard (2013).\nGalaxy tools and workflows for sequence analysis with applications\nin molecular plant pathology. PeerJ 1:e167\nhttp://dx.doi.org/10.7717/peerj.167\n\nS. G\u00f6tz et al. (2008).\nHigh-throughput functional annotation and data mining with the Blast2GO suite.\nNucleic Acids Res. 36(10):3420\u20133435.\nhttp://dx.doi.org/10.1093/nar/gkn176\n\nA. Conesa and S. G\u00f6tz (2008).\nBlast2GO: A Comprehensive Suite for Functional Analysis in Plant Genomics.\nInternational Journal of Plant Genomics. 619832.\nhttp://dx.doi.org/10.1155/2008/619832\n\nA. Conesa et al. (2005).\nBlast2GO: A universal tool for annotation, visualization and analysis in functional genomics research.\nBioinformatics 21:3674-3676.\nhttp://dx.doi.org/10.1093/bioinformatics/bti610\n\nSee also http://www.blast2go.com/\n\nThis wrapper is available to install into other Galaxy Instances via the Galaxy\nTool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/blast2go\n\n    ",
        "id": "blast2go",
        "interpreter": "python",
        "language": null,
        "name": "Blast2GO",
        "readme": true,
        "tests": true,
        "version": "0.0.9"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/bth459"
            }
        ],
        "code_file": null,
        "command": "\n        ln -s '$input' '$input.element_identifier'\".fasta\" &&\n\n        BlastAlign -i '$input.element_identifier'\".fasta\"\n        -m $advanced_option.m\n        #if $advanced_option.r != \"\"\n            -r $advanced_option.r\n        #end if\n        #if $advanced_option.x != \"\"\n            -x $advanced_option.x\n        #end if\n        -n $advanced_option.n\n        #if $advanced_option.s != 0\n            -s $advanced_option.s\n        #end if\n        &&\n        ln -s '$input.element_identifier'\".fasta.phy\" out.phy &&\n        ln -s '$input.element_identifier'\".fasta.nxs\" out.nxs\n        #if $output_format.value == \"fasta\"\n            && python $__tool_directory__/scripts/S01_phylip2fasta.py out.phy out.fasta\n        #end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "phylip",
                "nexus",
                "fasta"
            ]
        },
        "description": "\n        Align the nucleic acid sequences using BLASTN\n    ",
        "help": "\n    \n\n.. class:: infomark\n\n**Authors**  BlastAlign has been written by Robert Belshaw and Aris Katzourakis.\n\n.. class:: infomark\n\n**Galaxy integration** Julie Baffard and ABiMS TEAM, Roscoff Marine Station\n\n | Contact support.abims@sb-roscoff.fr for any questions or concerns about the Galaxy implementation of this tool.\n | Credits : Gildas le Corguill\u00c3\u00a9, Misharl Monsoor, Victor Mataigne\n\n--------\n\n**Description**\n\nBlastAlign takes a set of nucleic sequences in a file in fasta format and returns a multiple alignment (in Nexus and Phylip formats) using BLAST+. This Galaxy implementation works with dataset collections, which allows multiple parallels runs of BlastAlign at once on many files.\n\n--------\n\n**Parameters**\n\nThe choice of several parameters for the blast is possible.\n\n**-m : Proportion of gaps allowed in any one sequence in the final alignement**\n    integer (between 0 and 100).\n    By default : 95%, i.e. only removes sequences with extremely short matches.\n    We find 50 the most useful.\n\n**-r : Name of reference sequence**\n    text.\n    Default is searching for best candidate.\n    If entered, the sequence will be extracted, written to a separate file, and blasted against the original input file.\n  \n **-x : Name of sequences to be excluded from the analysis**\n    text.\n    names must be comma-separated.\n\n **-n**\n    If checked : retain original names in output files.\n    If not checked : output the 15 character name abbreviations (stripped of potentially problematic characters) that is used in the tool.\n    Default : checked.\n\n**-s : Number of sequences to be used in initial search for reference sequence**\n    integer (between 0 and total number of sequences).\n    Default : 0\n    Default is finding the reference sequence by blasting all sequences against all sequences, only randomly subsampling when it thinks the blast output file might be too large.\n\n** Output format :**\n    The user have the choice between three format (only one is returned per run) : fasta, phylip, and nexus.\n\n--------\n\n**Outputs**\n\nThe output format is \n \n    - 'Blastalign_on_{input.name}_phylip' :\n        the aligned sequences in Phylip format.\n\n    - 'Blastalign_on_{input.name}_nexus' :\n       the aligned sequences in Nexus format.\n\n    - 'Blastalign_on_{input_file}_fasta' :\n        the aligned sequences in Fasta format if the option \"fasta format\" is checked.\n\n---------\n\nChangelog\n---------\n\n**Version 2.1 - 17/01/2018**\n\n  - New parameter \"output format\"\n  - Applied some bugfixes and minor improvments\n\n**Version 2.0 - 21/04/2017**\n\n - NEW: BlastAlign will now be launched on one file at once. Although, it will manage a Dataset Collection to deal with numerous files.\n\n**Version 1.0 - 13/04/2017**\n\n - TEST: Add funtional test with planemo\n - IMPROVEMENT: Use conda dependencies for blastalign, blast-legacy, perl, python\n\n    \n\n    ",
        "id": "blastalign",
        "interpreter": null,
        "language": null,
        "name": "BlastAlign",
        "readme": true,
        "tests": true,
        "version": "2.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    blat\n    #if $database_source.database_source_select == 'fasta_db'\n      $database_source.database.fields.path\n    #else if $database_source.database_source_select == 'twobit_db'\n      #if $database_source.range\n        $database_source.database.fields.path:$database_source.range\n      #else\n        $database_source.database.fields.path\n      #end if\n    #else\n      $database_source.database\n    #end if\n    $query -t=$databaseType.databaseType_select -q=$databaseType.queryType\n    #if str($databaseType.tileSize)\n      -tileSize=$databaseType.tileSize\n    #end if\n    #if str($stepSize)\n      -stepSize=$stepSize\n    #end if\n    #if $oneOff\n      -oneOff=1\n    #end if\n    #if str($databaseType.minMatch)\n      -minMatch=$databaseType.minMatch\n    #end if\n    #if str($minScore)\n      -minScore=$minScore\n    #end if\n    #if str($databaseType.minIdentity)\n      -minIdentity=$databaseType.minIdentity\n    #end if\n    #if str($maxGap)\n      -maxGap=$maxGap\n    #end if\n    #if str($repMatch)\n      -repMatch=$repMatch\n    #end if\n    #if $mask.mask_select\n      -mask=$mask.mask_select\n    #else if $mask.repeats\n      -repeats=$mask.repeats\n    #end if\n    #if $qMask\n      -qMask=$qMask\n    #end if\n    #if str($dots)\n      -dots=$dots\n    #end if\n    #if $trimT\n      -trimT\n    #end if\n    #if $noTrimA\n      -noTrimA\n    #end if\n    #if $trimHardA\n      -trimHardA\n    #end if\n    #if $fastMap\n      -fastMap\n    #end if\n    #if $fine\n      -fine\n    #end if\n    #if str($maxIntron)\n      -maxIntron=$maxIntron\n    #end if\n    #if $extendThroughN\n      -extendThroughN\n    #end if\n    -out=$out\n    $output > $logfile\n  ",
        "dataFormats": {
            "inputs": [
                "fasta,twobit",
                "fasta,twobit"
            ],
            "outputs": [
                "txt",
                "txt"
            ]
        },
        "description": "",
        "help": "\n**What it does**\n\nBLAT produces two major classes of alignments:\n\n- at the DNA level between two sequences that are of 95% or greater identity, but which may include large inserts;\n- at the protein or translated DNA level between sequences that are of 80% or greater identity and may also include large inserts.\n\nThe output of BLAT is flexible. By default it is a simple tab-delimited file which describes the alignment, but which does not include the sequence of the alignment itself. Optionally it can produce BLAST and WU-BLAST compatible output as well as a number of other formats.\n\n**License and citation**\n\nThis Galaxy tool is Copyright \u00a9 2013 `CRS4 Srl.`_ and is released under the `MIT license`_.\n\n.. _CRS4 Srl.: http://www.crs4.it/\n.. _MIT license: http://opensource.org/licenses/MIT\n\nIf you use this tool in Galaxy, please cite |Cuccuru2013|_.\n\n.. |Cuccuru2013| replace:: Cuccuru, G., Orsini, M., Pinna, A., Sbardellati, A., Soranzo, N., Travaglione, A., Uva, P., Zanetti, G., Fotia, G. (2013) Orione, a web-based framework for NGS analysis in microbiology. *Submitted*\n.. _Cuccuru2013: http://orione.crs4.it/\n\nThis tool uses `BLAT`_, which is licensed separately. Please cite |Kent2002|_.\n\n.. _BLAT: http://genome.ucsc.edu/FAQ/FAQblat.html\n.. |Kent2002| replace:: Kent, W. J. (2002) BLAT \u2013 The BLAST-Like Alignment Tool. *Genome Res.* 12(4), 656-664\n.. _Kent2002: http://genome.cshlp.org/content/12/4/656\n  ",
        "id": "blat_wrapper",
        "interpreter": null,
        "language": null,
        "name": "BLAT",
        "readme": false,
        "tests": true,
        "version": "0.3"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btv145"
            }
        ],
        "code_file": null,
        "command": "\n        #set gtf = \"refgtf\"\n        #if $gtf_source.gtf_source_select == \"history\":\n            ln -s '${gtf_source.gtf_hist}' $gtf &&\n        #else if $gtf_source.gtf_source_select == \"cached\":\n            ln -s '${gtf_source.gtf_builtin.fields.path}' $gtf &&\n        #end if\n\n        #if $rscript:\n            cp '$__tool_directory__/chipseeker.R' '$out_rscript' &&\n        #end if\n\n        Rscript '$__tool_directory__/chipseeker.R'\n\n        -i '$peaks'\n        -G '$gtf'\n        -u $adv.upstream\n        -d $adv.downstream\n\n        #if $adv.flankgeneinfo:\n            -F $adv.flankgeneinfo\n        #end if\n\n        -D $adv.flankgenedist\n\n        #if $adv.ignoreUpstream:\n            -j $adv.ignoreUpstream\n        #end if\n\n        #if $adv.ignoreDownstream:\n            -k $adv.ignoreDownstream\n        #end if\n\n        -f $format\n\n        #if $pdf:\n            -p $pdf\n        #end if\n\n        -r $rdata\n    \n    ",
        "dataFormats": {
            "inputs": [
                "bed,interval,tabular",
                "gtf"
            ],
            "outputs": [
                "interval",
                "pdf",
                "txt",
                "rdata"
            ]
        },
        "description": "for ChIP peak annotation and visualization",
        "help": "\n\n.. class:: infomark\n\n**What it does**\n\nChIPseeker_ is a Bioconductor package for annotating ChIP-seq data analysis. Peak Annotation is performed by the annotatePeak function. The position and strand information of nearest genes are reported, in addition to the distance from the peak to the TSS of its nearest gene. Users can define the TSS (transcription start site) region under **Advanced Options**, by default the TSS region is defined from -3kb to +3kb. The genomic region of the peak is reported in the annotation column. Since some annotations may overlap for a peak, ChIPseeker adopts the following priority in genomic annotation:\n\n* Promoter\n* 5\u2019 UTR\n* 3\u2019 UTR\n* Exon\n* Intron\n* Downstream\n* Intergenic\n\nChIPseeker also produces plots to help users visualise the overlaps in annotation for peaks, for example, the vennpie and upsetplot. See the `ChIPseeker vignette`_ for more information.\n\n-----\n\n**Inputs**\n\nA peaks file in BED, Interval or Tabular format e.g from MACS2 or DiffBind.\n\nExample:\n\n    =====  ======  ======  ========  =====  ======\n    Chrom  Start   End     Name      Score  Strand\n    =====  ======  ======  ========  =====  ======\n    18     394599  396513  DiffBind  0      .\n    18     111566  112005  DiffBind  0      .\n    18     346463  347342  DiffBind  0      .\n    18     399013  400382  DiffBind  0      .\n    18     371109  372102  DiffBind  0      .\n    =====  ======  ======  ========  =====  ======\n\nA GTF file for annotation. The GTF file must have fields called \"gene_id\" and gene_name\".\n\n-----\n\n**Outputs**\n\nThis tool outputs\n\n    * a file of annotated peaks in Interval or Tabular format\n    * a PDF of plots (plotAnnoPie, vennpie, upsetplot)\n\nOptionally, you can choose to output\n\n    * the R script used by this tool\n    * an RData file\n\n**Annotated peaks**\n\nAnnotation similar to below will be added to the input file.\n\nExample - **Interval format**:\n\n    =====  ======  ======  =====================================================================================================================================================\n    Chrom  Start   End     Comment\n    =====  ======  ======  =====================================================================================================================================================\n    18     394599  396513  DiffBind|0|.|Intron (ENST00000400256/ENSG00000158270, intron 1 of 1)|1|346465|400382|53918|2|ENST00000400256| 3869|COLEC12|ENSG00000158270\n    18     346463  347342  DiffBind|0|.|Exon (ENST00000400256/ENSG00000158270, exon 1 of 1)|1|346465|400382|53918|2|ENST00000400256|53040|COLEC12|ENSG00000158270\n    18     399013  400382  DiffBind|0|.|Promoter (<=1kb)|1|346465|400382|53918|2|ENST00000400256|    0|COLEC12|ENSG00000158270\n    18     371109  372102  DiffBind|0|.|Intron (ENST00000400256/ENSG00000158270, intron 1 of 1)|1|346465|400382|53918|2|ENST00000400256|28280|COLEC12|ENSG00000158270\n    18     111566  112005  DiffBind|0|.|Promoter (<=1kb)|1|111568|112005|  438|1|ENST00000608049|    0|ROCK1P1|ENSG00000263006\n    =====  ======  ======  =====================================================================================================================================================\n\n    Columns contain the following data:\n\n* **Chrom**: Chromosome name\n* **Start**: Start position of site\n* **End**: End position of site\n* **Comment**: The pipe (\"|\") separated values in this column correspond to:\n\n    * *<Any additional input columns>*\n    * *annotation* (Promoter, 5\u2019 UTR, 3\u2019 UTR, Exon, Intron, Downstream, Intergenic)\n    * *geneChr*\n    * *geneStart*\n    * *geneEnd*\n    * *geneLength*\n    * *geneStrand*\n    * *transcriptId*\n    * *distanceToTSS*\n    * *geneName*\n    * *geneId*\n\nExample - **Tabular format**:\n\n    =====  ======  ======  ========  ====== ======  ===========================================  ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== ===============\n    Chrom  Start   End     Name      Score  Strand  Comment                                      annotation                                              geneChr geneStart geneEnd geneLength geneStrand transcriptId    distanceToTSS geneName geneId\n    =====  ======  ======  ========  ====== ======  ===========================================  ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== ===============\n    18     394599  396513  DiffBind    0    .       1914|7.15|5.55|7.89|-2.35|7.06e-24|9.84e-21  Intron (ENST00000400256/ENSG00000158270, intron 1 of 1) 1       346465    400382  53918      2          ENST00000400256 3869          COLEC12  ENSG00000158270\n    18     346463  347342  DiffBind    0    .       879|5|5.77|3.24|2.52|6.51e-06|0.00303        Exon (ENST00000400256/ENSG00000158270, exon 1 of 1)     1       346465    400382  53918      2          ENST00000400256 53040         COLEC12  ENSG00000158270\n    18     399013  400382  DiffBind    0    .       1369|7.62|7|8.05|-1.04|1.04e-05|0.00364      Promoter (<=1kb)                                        1       346465    400382  53918      2          ENST00000400256 0             COLEC12  ENSG00000158270\n    18     371109  372102  DiffBind    0    .       993|4.63|3.07|5.36|-2.3|8.1e-05|0.0226       Intron (ENST00000400256/ENSG00000158270, intron 1 of 1) 1       346465    400382  53918      2          ENST00000400256 28280         COLEC12  ENSG00000158270\n    18     111566  112005  DiffBind    0    .       439|5.71|6.53|3.63|2.89|1.27e-08|8.88e-06    Promoter (<=1kb)                                        1       111568    112005  438        1          ENST00000608049 0             ROCK1P1  ENSG00000263006\n    =====  ======  ======  ========  ====== ======  ===========================================  ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== ===============\n\n.. _ChIPseeker: https://bioconductor.org/packages/release/bioc/html/ChIPseeker.html\n.. _`ChIPseeker vignette`: http://bioconductor.org/packages/release/bioc/vignettes/ChIPseeker/inst/doc/ChIPseeker.html\n\n",
        "id": "chipseeker",
        "interpreter": null,
        "language": null,
        "name": "ChIPseeker",
        "readme": false,
        "tests": true,
        "version": "1.18.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1038/nbt.1621"
            }
        ],
        "code_file": null,
        "command": "\n        cufflinks_wrapper.py \n            --input=$input\n            --assembled-isoforms-output=$assembled_isoforms\n            --num-threads=\"\\${GALAXY_SLOTS:-4}\"\n            -I $max_intron_len\n            -F $min_isoform_fraction\n            -j $pre_mrna_fraction\n            $length_correction\n            \n            ## Include reference annotation?\n            #if $reference_annotation.use_ref == \"Use reference annotation\":\n                -G $reference_annotation.reference_annotation_file\n                $reference_annotation.compatible_hits_norm\n            #end if\n            #if $reference_annotation.use_ref == \"Use reference annotation guide\":\n                -g $reference_annotation.reference_annotation_guide_file\n                --3-overhang-tolerance=$reference_annotation.three_overhang_tolerance\n                --intron-overhang-tolerance=$reference_annotation.intron_overhang_tolerance\n                $reference_annotation.no_faux_reads\n            #end if\n            \n            ## Bias correction?\n            #if $bias_correction.do_bias_correction == \"Yes\":\n                -b\n                #if $bias_correction.seq_source.index_source == \"history\":\n                    --ref_file=$bias_correction.seq_source.ref_file\n                #else:\n                    --index=${bias_correction.seq_source.index.fields.path}\n                #end if\n            #end if\n\n            ## Multi-read correct?\n            #if str($multiread_correct) == \"Yes\":\n            -u\n            #end if\n\n            ## Include global model if available.\n            #if $global_model:\n                --global_model=$global_model\n            #end if\n\n            ## advanced settings\n            #if $advanced_settings.use_advanced_settings == \"Yes\":\n            --library-type=$advanced_settings.library_type\n            #if $advanced_settings.mask_file:\n                --mask-file=$advanced_settings.mask_file\n                #end if\n            --inner-mean-dist=$advanced_settings.inner_mean_dist\n            --inner-dist-std-dev=$advanced_settings.inner_dist_std_dev\n            --max-mle-iterations=$advanced_settings.max_mle_iterations\n            --junc-alpha=$advanced_settings.junc_alpha\n            --small-anchor-fraction=$advanced_settings.small_anchor_fraction\n            --overhang-tolerance=$advanced_settings.overhang_tolerance\n            --max-bundle-length=$advanced_settings.max_bundle_length\n            --max-bundle-frags=$advanced_settings.max_bundle_frags\n            --min-intron-length=$advanced_settings.min_intron_length\n            --trim-3-avgcov-thresh=$advanced_settings.trim_three_avgcov_thresh\n            --trim-3-dropoff-frac=$advanced_settings.trim_three_dropoff_frac\n            #end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "sam,bam",
                "gff3,gtf",
                "gff3,gtf",
                "fasta",
                "gff3,gtf"
            ],
            "outputs": [
                "tabular",
                "tabular",
                "gtf",
                "txt",
                "gtf"
            ]
        },
        "description": "transcript assembly and FPKM (RPKM) estimates for RNA-Seq data",
        "help": "\n**Cufflinks Overview**\n\nCufflinks_ assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. It accepts aligned RNA-Seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one.  Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621\n\n.. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/\n\n------\n\n**Know what you are doing**\n\n.. class:: warningmark\n\nThere is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to **understand** the parameters by carefully reading the `documentation`__ and experimenting. Fortunately, Galaxy makes experimenting easy.\n\n.. __: http://cole-trapnell-lab.github.io/cufflinks/cufflinks/\n\n------\n\n**Input formats**\n\nCufflinks takes a text file of SAM alignments as input. The RNA-Seq read mapper TopHat produces output in this format, and is recommended for use with Cufflinks. However Cufflinks will accept SAM alignments generated by any read mapper. Here's an example of an alignment Cufflinks will accept::\n\n  s6.25mer.txt-913508    16    chr1 4482736 255 14M431N11M * 0 0 \\\n     CAAGATGCTAGGCAAGTCTTGGAAG IIIIIIIIIIIIIIIIIIIIIIIII NM:i:0 XS:A:-\n    \nNote the use of the custom tag XS. This attribute, which must have a value of \"+\" or \"-\", indicates which strand the RNA that produced this read came from. While this tag can be applied to any alignment, including unspliced ones, it must be present for all spliced alignment records (those with a 'N' operation in the CIGAR string).\nThe SAM file supplied to Cufflinks must be sorted by reference position. If you aligned your reads with TopHat, your alignments will be properly sorted already. If you used another tool, you may want to make sure they are properly sorted as follows::\n\n  sort -k 3,3 -k 4,4n hits.sam > hits.sam.sorted\n\nNOTE: Cufflinks currently only supports SAM alignments with the CIGAR match ('M') and reference skip ('N') operations. Support for the other operations, such as insertions, deletions, and clipping, will be added in the future.\n\n------\n\n**Outputs**\n\nCufflinks produces three output files:\n\nTranscripts and Genes:\n\nThis GTF file contains Cufflinks' assembled isoforms. The first 7 columns are standard GTF, and the last column contains attributes, some of which are also standardized (e.g. gene_id, transcript_id). There one GTF record per row, and each record represents either a transcript or an exon within a transcript. The columns are defined as follows::\n\n  Column number   Column name   Example     Description\n  -----------------------------------------------------\n  1               seqname       chrX        Chromosome or contig name\n  2               source        Cufflinks   The name of the program that generated this file (always 'Cufflinks')\n  3               feature       exon        The type of record (always either \"transcript\" or \"exon\").\n  4               start         77696957    The leftmost coordinate of this record (where 0 is the leftmost possible coordinate)\n  5               end           77712009    The rightmost coordinate of this record, inclusive.\n  6               score         77712009    The most abundant isoform for each gene is assigned a score of 1000. Minor isoforms are scored by the ratio (minor FPKM/major FPKM)\n  7               strand        +           Cufflinks' guess for which strand the isoform came from. Always one of '+', '-' '.'\n  7               frame         .           Cufflinks does not predict where the start and stop codons (if any) are located within each transcript, so this field is not used.\n  8               attributes    See below\n  \nEach GTF record is decorated with the following attributes::\n\n  Attribute       Example       Description\n  -----------------------------------------\n  gene_id         CUFF.1        Cufflinks gene id\n  transcript_id   CUFF.1.1      Cufflinks transcript id\n  FPKM            101.267       Isoform-level relative abundance in Reads Per Kilobase of exon model per Million mapped reads\n  frac            0.7647        Reserved. Please ignore, as this attribute may be deprecated in the future\n  conf_lo         0.07          Lower bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, lower bound = FPKM * (1.0 - conf_lo)\n  conf_hi         0.1102        Upper bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, upper bound = FPKM * (1.0 + conf_lo)\n  cov             100.765       Estimate for the absolute depth of read coverage across the whole transcript\n  \n\nTranscripts only:\n  This file is simply a tab delimited file containing one row per transcript and with columns containing the attributes above. There are a few additional attributes not in the table above, but these are reserved for debugging, and may change or disappear in the future.\n    \nGenes only:\nThis file contains gene-level coordinates and expression values.\n    \n-------\n\n**Cufflinks settings**\n\nAll of the options have a default value. You can change any of them. Most of the options in Cufflinks have been implemented here.\n\n------\n\n**Cufflinks parameter list**\n\nThis is a list of implemented Cufflinks options::\n\n  -m INT    This is the expected (mean) inner distance between mate pairs. For, example, for paired end runs with fragments selected at 300bp, where each end is 50bp, you should set -r to be 200. The default is 45bp.\n  -s INT    The standard deviation for the distribution on inner distances between mate pairs. The default is 20bp.\n  -I INT    The minimum intron length. Cufflinks will not report transcripts with introns longer than this, and will ignore SAM alignments with REF_SKIP CIGAR operations longer than this. The default is 300,000.\n  -F         After calculating isoform abundance for a gene, Cufflinks filters out transcripts that it believes are very low abundance, because isoforms expressed at extremely low levels often cannot reliably be assembled, and may even be artifacts of incompletely spliced precursors of processed transcripts. This parameter is also used to filter out introns that have far fewer spliced alignments supporting them. The default is 0.05, or 5% of the most abundant isoform (the major isoform) of the gene.\n  -j        Some RNA-Seq protocols produce a significant amount of reads that originate from incompletely spliced transcripts, and these reads can confound the assembly of fully spliced mRNAs. Cufflinks uses this parameter to filter out alignments that lie within the intronic intervals implied by the spliced alignments. The minimum depth of coverage in the intronic region covered by the alignment is divided by the number of spliced reads, and if the result is lower than this parameter value, the intronic alignments are ignored. The default is 5%.\n  -G        Tells Cufflinks to use the supplied reference annotation to estimate isoform expression. It will not assemble novel transcripts, and the program will ignore alignments not structurally compatible with any reference transcript.  \n  -N        With this option, Cufflinks excludes the contribution of the top 25 percent most highly expressed genes from the number of mapped fragments used in the FPKM denominator. This can improve robustness of differential expression calls for less abundant genes and transcripts.\n    ",
        "id": "cufflinks",
        "interpreter": "python",
        "language": null,
        "name": "Cufflinks",
        "readme": false,
        "tests": true,
        "version": "2.2.1.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "cutadapt\n\t\t#if $input.extension.startswith( \"fastq\"):\n\t\t    --format=fastq\n            #if $input.extension == \"fastqillumina\":\n                --quality-base=64\n            #end if\t\n            #if $input.extension == \"fastqsolexa\":\n                --quality-base=64\n            #end if\t\n\t\t#else\n\t\t--format=$input.extension\n\t\t#end if \n\t\t#for $a in $adapters\n\t\t--adapter='${a.adapter_source.adapter}'\n\t\t#end for\n\t\t#for $aa in $anywhere_adapters\n\t\t--anywhere='${aa.anywhere_adapter_source.anywhere_adapter}'\n\t\t#end for\n\t\t#for $fa in $front_adapters\n\t\t--front='${fa.front_adapter_source.front_adapter}'\n\t\t#end for\n\t\t--error-rate=$error_rate\n\t\t--times=$count\n\t\t--overlap=$overlap\n        $match_read_wildcards\n        $no_match_adapters_wildcards\n\n        #if str( $output_filtering_options.output_filtering) == \"filter\":\n\t\t    $output_filtering_options.discard\n\t\t    #if str($output_filtering_options.min) != '0':\n\t\t        --minimum-length=$output_filtering_options.min\n\t\t    #end if\n\t\t    #if str($output_filtering_options.max) != '0':\n\t\t        --maximum-length=$output_filtering_options.max\n\t\t    #end if\n        #end if\n\n\t\t--output='$output' \n\t\t#if str( $output_params.output_type ) == \"additional\":\n\t\t\t#if $output_params.rest_file:\n\t\t\t    --rest-file=$rest_output\n\t\t\t#end if\n\t\t\t#if $output_params.wildcard_file:\n\t\t\t    --wildcard-file=$wild_output\n\t\t\t#end if\n\t\t\t#if $output_params.too_short_file:\n\t\t\t    --too-short-output=$too_short_output\n\t\t\t#end if\n\t\t\t#if $output_params.untrimmed_file:\n\t\t\t    --untrimmed-output=$untrimmed_output\n\t\t\t#end if\n\t\t#end if\n\n\t\t#if str( $read_modification_params.read_modification) == \"modify\":\n \t\t    #if str($read_modification_params.quality_cutoff) != '0':\n\t\t       --quality-cutoff=$read_modification_params.quality_cutoff\n\t\t    #end if\n            #if $read_modification_params.prefix != '':\n                --prefix=\"$read_modification_params.prefix\"\n            #end if\n            #if $read_modification_params.suffix != '':\n                --suffix=\"$read_modification_params.suffix\"\n            #end if\n            #if $read_modification_params.length_tag != '':\n                --length-tag=\"$read_modification_params.length_tag\"\n            #end if\n            $read_modification_params.zero_cap\n        #end if\n\n\t\t'$input'\n\t\t> $report\n\t",
        "dataFormats": {
            "inputs": [
                "fastqsanger, fastqillumina, fastqsolexa, fasta"
            ],
            "outputs": [
                "txt",
                "input",
                "input",
                "txt",
                "input",
                "input"
            ]
        },
        "description": "Remove adapter sequences from Fastq/Fasta",
        "help": "\nSummary\n-------\nThis tool removes adapter sequences from DNA high-throughput\nsequencing data. This is usually necessary when the read length of the\nmachine is longer than the molecule that is sequenced, such as in\nmicroRNA data.\n\nThe tool is based on the opensource cutadapt_ tool.\n\n-----\n\nAlgorithm\n---------\n\ncutadapt uses a simple semi-global alignment algorithm, without any special optimizations.\nFor speed, the algorithm is implemented as a Python extension module in ``calignmodule.c``.\n\n\nPartial adapter matches\n-----------------------\n\nCutadapt correctly deals with partial adapter matches. As an example, suppose\nyour adapter sequence is ``ADAPTER`` (specified via 3' Adapters parameter).\nIf you have these input sequences::\n\n\tMYSEQUENCEADAPTER\n\tMYSEQUENCEADAP\n\tMYSEQUENCEADAPTERSOMETHINGELSE\n\nAll of them will be trimmed to ``MYSEQUENCE``. If the sequence starts with an\nadapter, like this::\n\n\tADAPTERSOMETHING\n\nIt will be empty after trimming.\n\nWhen the allowed error rate is sufficiently high, errors in\nthe adapter sequence are allowed. For example, ``ADABTER`` (1 mismatch), ``ADAPTR`` (1 deletion),\nand ``ADAPPTER`` (1 insertion) will all be recognized if the error rate is set to 0.15.\n\n\nAnchoring 5' adapters\n---------------------\n\nIf you specify a 5' (Front) adapter, the adapter may overlap the beginning of the read or\noccur anywhere whithin it. If it appears withing the read, the sequence that precedes it \nwill also be trimmed in addition to the adapter. For example when the adapter sequence is\n``ADAPTER``::\n\n    HELLOADAPTERTHERE\n    APTERTHERE\n\nwill both be trimmed to ``THERE``. To avoid this, you can prefix the adapter with the character\n``^``. This will restrict the search, forcing the adapter to be a prefix of the read. With\nthe adapter sequence set to ``^ADAPTER``, only reads like this will be trimmed::\n\n    ADAPTERHELLO\n\n\nAllowing adapters anywhere\n--------------------------\n\nCutadapt assumes that any adapter specified via the 3' Adapter parameter\nwas ligated to the 3\\' end of the sequence. This is the correct assumption for\nat least the SOLiD and Illumina small RNA protocols and probably others.\nThe assumption is enforced by the alignment algorithm, which only finds the adapter\nwhen its starting position is within the read. In other words, the 5' base of\nthe adapter must appear within the read. The adapter and all bases following\nit are remved.\n\nIf, on the other hand, your adapter can also be ligated to the 5' end (on\npurpose or by accident), you should tell cutadapt so by using the Anywhere Adapter\nparameter. It will then use a slightly different alignment algorithm\n(so-called semiglobal alignment), which allows any type of overlap between the\nadapter and the sequence. In particular, the adapter may appear only partially\nin the beginning of the read, like this::\n\n    PTERMYSEQUENCE\n\nThe decision which part of the read to remove is made as follows: If there is at\nleast one base before the found adapter, then the adapter is considered to be\na 3' adapter and the adapter itself and everything following it is removed.\nOtherwise, the adapter is considered to be a 5' adapter and it is removed from\nthe read.\n\nHere are some examples, which may make this clearer (left: read, right: trimmed\nread)::\n\n    MYSEQUENCEADAPTER -> MYSEQUENCE (3' adapter)\n    MADAPTER -> M (3' adapter)\n    ADAPTERMYSEQUENCE -> MYSEQUENCE (5' adapter)\n    PTERMYSEQUENCE -> MYSEQUENCE (5' adapter)\n\nThe regular algorithm (3' Adapter) would trim the first two examples in the same way,\nbut trim the third to an empty sequence and trim the fourth not at all.\n\n\n.. _cutadapt: http://code.google.com/p/cutadapt/\n\t",
        "id": "cutadapt",
        "interpreter": null,
        "language": null,
        "name": "Cutadapt",
        "readme": true,
        "tests": true,
        "version": "1.1.a"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n        data_manager_diamond_database_builder.py \"${out_file}\"\n    ",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "data_manager_json"
            ]
        },
        "description": " Database builder",
        "help": "\n\n.. class:: infomark\n\nNCBI databases can be downded from ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/\nFor example the NR database can be downloaded from ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz\n\n    ",
        "id": "diamond_database_builder",
        "interpreter": "python",
        "language": null,
        "name": "Diamond",
        "readme": true,
        "tests": false,
        "version": "0.0.1"
    },
    {
        "ciation": [
            {
                "citation": "10.1038/nmeth.3317"
            }
        ],
        "code_file": null,
        "command": "\n        python '$__tool_directory__/kallisto_index_builder.py' --output '${out_file}'\n            --fasta_filename '${all_fasta_source.fields.path}'\n            --fasta_dbkey '${all_fasta_source.fields.dbkey}'\n            --fasta_description '${all_fasta_source.fields.name}'\n            --data_table_name kallisto_indexes\n        \n    ",
        "dataFormats": {
            "inputs": [],
            "outputs": [
                "data_manager_json"
            ]
        },
        "description": "index builder",
        "help": "\n\n.. class:: infomark\n\n**Notice:** If you leave name, description, or id blank, it will be generated automatically.\n\nWhat is Kallisto?\n---------------\n\n`kallisto <https://pachterlab.github.io/kallisto/about>`__ is a program for\nquantifying abundances of transcripts from RNA-Seq data, or more generally of\ntarget sequences using high-throughput sequencing reads. It is based on the\nnovel idea of pseudoalignment for rapidly determining the compatibility of\nreads with targets, without the need for alignment. On benchmarks with standard\nRNA-Seq data, kallisto can quantify 30 million human reads in less than 3\nminutes on a Mac desktop computer using only the read sequences and a\ntranscriptome index that itself takes less than 10 minutes to build.\nPseudoalignment of reads preserves the key information needed for\nquantification, and kallisto is therefore not only fast, but also as accurate\nas existing quantification tools. In fact, because the pseudoalignment\nprocedure is robust to errors in the reads, in many benchmarks kallisto\nsignificantly outperforms existing tools.\n\n    ",
        "id": "kallisto_index_builder_data_manager",
        "interpreter": null,
        "language": null,
        "name": "Kallisto",
        "readme": false,
        "tests": false,
        "version": "0.43.1"
    },
    {
        "ciation": [
            {
                "citation": "10.1101/gr.133744.111"
            }
        ],
        "code_file": "dexseq_helper.py",
        "command": "\n mkdir ./html_out &&\n #import json\n Rscript '$__tool_directory__/dexseq.R'\n     -o '$dexseq_out'\n     -p \\${GALAXY_SLOTS:-4}\n     #set $temp_factor_names = list()\n     #for $factor in $rep_factorName:\n         #set $temp_factor = list()\n         #set $count_files1 = list()\n         #for $file in $factor.countFiles1:\n             $count_files1.append(str($file))\n         #end for\n         $temp_factor.append( {str($factor.factorLevel1): $count_files1} )\n\n         #set $count_files2 = list()\n         #for $file in $factor.countFiles2:\n             $count_files2.append(str($file))\n         #end for\n         $temp_factor.append( {str($factor.factorLevel2): $count_files2} )\n         $temp_factor_names.append([str($factor.factorName), $temp_factor])\n     #end for\n     -f '#echo json.dumps(temp_factor_names)#'\n     -a $gtf\n     -c $fdr_cutoff\n     -d $rds\n\n     #if $report:\n         -r ./html_out\n         &&\n         mkdir '$htmlreport.extra_files_path'\n         &&\n         cp ./html_out/testForDEU.html $htmlreport\n         &&\n         cp -r ./html_out/* '$htmlreport.extra_files_path'\n     #end if\n    ",
        "dataFormats": {
            "inputs": [
                "gtf,gff",
                "tabular",
                "tabular"
            ],
            "outputs": [
                "tabular",
                "html",
                "rdata"
            ]
        },
        "description": "Determines differential exon usage from count tables",
        "help": "\n.. class:: infomark\n\n**What it does**\n\nInference of differential exon usage in RNA-Seq.\n\n\n**Inputs**\n\nDEXSeq_ takes count tables generated from the dexseq_count as input. Count tables must be generated for each sample individually.\nDEXSeq_ is capable of handling multiple factors that affect your experiment. The first factor you input is considered to be the primary\nfactor that affects gene expressions. You can also input several secondary factors that might\ninfluence your experiment but the final output will be changes in genes due to primary factor in the presence of secondary factors. Each factor has two levels/states.\nYou need to select an appropriate count table from your history for each factor level.\n\nThe following table gives some examples of factors and their levels:\n\n========= ============== ===============\nFactor    Factor level 1 Factor level 2\n--------- -------------- ---------------\ncondition Knockdown      Wildtype\n--------- -------------- ---------------\ntreatment Treated        Untreated\n--------- -------------- ---------------\ntimePoint Day4           Day1\n--------- -------------- ---------------\nSeqType   SingleEnd      PairedEnd\n--------- -------------- ---------------\nGender    Female         Male\n========= ============== ===============\n\n*Note*: Output log2 fold changes are based on primary factor level 1 vs. factor level 2. Here the order of factor levels is important. For example, for the factor 'condition' given in the above table, DEXSeq computes fold changes of 'Knockdown' samples against 'Wildtype', i.e. the values correspond to up or down regulations of genes in Knockdown samples.\n\n**Output**\n\nDEXSeq_ generates a tabular file containing the different columns and an optional html report. It can also ouput the DEXSeqResults R object that can be used with the plotDEXSeq tool to visualise individual genes.\n\n====== ==========================================================\nColumn Description\n------ ----------------------------------------------------------\n     1 Gene and exon Identifiers\n     2 group/gene identifier\n     3 feature/exon identifier\n     4 mean of the counts across samples in each feature/exon\n     5 exon dispersion estimate\n     6 LRT statistic\n     7 LRT p-value\n     8 BH adjusted p-values\n     9 exon usage coefficient factorLevel 2\n    10 exon usage coefficient factorLevel 1\n    11 relative exon usage fold changes\n    12 GRanges object of the coordinates of the exon/feature\n    13 matrix of integer counts, of each column containing a sample\n    14 list of transcripts overlapping with the exon\n====== ==========================================================\n\n\n.. _DEXSeq: http://master.bioconductor.org/packages/release/bioc/html/DEXSeq.html\n    ",
        "id": "dexseq",
        "interpreter": null,
        "language": "py",
        "name": "DEXSeq",
        "readme": false,
        "tests": true,
        "version": "1.28.1+galaxy1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n\n\n    #if $ref_db_source.db_source == \"history\":\n        ln -s $ref_db_source.reference_database ./database.dmnd\n    #else:\n        ln -s ${ref_db_source.index.fields.db_path} ./database.dmnd\n    #end if\n\n    &&\n\n    diamond\n        $method_cond.method_select\n        --threads \"\\${GALAXY_SLOTS:-12}\"\n        --db ./database\n        --query '$query'\n        #if $method_cond.method_select == \"blastx\"\n          --query-gencode '$method_cond.query_gencode'\n          --strand '$method_cond.query_strand'\n\t  --min-orf $method_cond.min_orf\n\t  #if $method_cond.frameshift_cond.frameshift_select == 'yes'\n\t          --frameshift $method_cond.frameshift_cond.frameshift\n\t\t  $method_cond.frameshift_cond.range_culling\n          #end if\n        #end if\n\n        #if $output.outfmt == \"0\"\n            --outfmt '0'\n            --out '$blast_pairw'\n        #else if $output.outfmt == \"5\"\n            --outfmt '5'\n            --out '$blast_xml'\n        #else if $output.outfmt == \"6\"\n            --outfmt '6' #echo ' '.join(str($output.fields).split(','))\n            --out '$blast_tabular'\n        #else if $output.outfmt == \"100\"\n            --outfmt '100'\n            --out output.daa\n        #else if $output.outfmt == \"101\"\n            --outfmt '101'\n            --out '$sam_output'\n        #else if $output.outfmt == \"102\"\n            --outfmt '102'\n            --out '$tax_output'\n        #end if\n    \n\n        --compress '0'\n        #if $sensitivity == \"1\"\n          --sensitive\n        #else if $sensitivity == \"2\"\n          --more-sensitive\n        #end if\n\n        #if str($gapopen) != \"\":\n          --gapopen '$gapopen'\n        #end if\n        #if str($gapextend) != \"\":\n          --gapextend '$gapextend'\n        #end if\n        --matrix '$matrix'\n        --comp-based-stats '$comp_based_stats'\n        --masking '$masking'\n\n        #if str($hit_filter.hit_filter_select) == 'max':\n            --max-target-seqs '$hit_filter.max_target_seqs'\n        #else:\n            --top '$hit_filter.top'\n\t#end if\n    \n\n        #if str($filter_score.filter_score_select) == 'evalue':\n            --evalue '$filter_score.evalue'\n        #else:\n            --min-score '$filter_score.min_score'\n        #end if\n\n        --id '$id'\n        --query-cover '$query_cover'\n        --subject-cover '$subject_cover'\n        --block-size '$block_size'\n        #if str($unal) == '1':\n            --unal 1 --un '$unalqueries' \n        #end if\n        $no_self_hits\n        #if $tax_cond.tax_select == 'file':\n            --taxonlist `cat '$tax_cond.taxonlistfile' | grep -v \"^#\" | grep -v \"^$\" | tr \"\\n\" \",\" | sed 's/,$//'`\n        #else if  $tax_cond.tax_select == 'list':\n            --taxonlist '$tax_cond.taxonlist'\n        #end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "fasta,fastq",
                "dmnd",
                "tabular"
            ],
            "outputs": [
                "fasta"
            ]
        },
        "description": "alignment tool for short sequences against a protein database",
        "help": "\n\n\n**What it does**\n\nDIAMOND_ is a new alignment tool for aligning short DNA sequencing reads to a protein reference database such as NCBI-NR.\nOn Illumina reads of length 100-150bp, in fast mode, DIAMOND is about 20,000 times faster than BLASTX, while reporting\nabout 80-90% of all matches that BLASTX finds, with an e-value of at most 1e-5. In sensitive mode, DIAMOND ist about 2,500\ntimes faster than BLASTX, finding more than 94% of all matches.\n\nThe DIAMOND algorithm is designed for the alignment of large datasets. The algorithm is not efficient for a small number of query sequences or only a single one of them, and speed will be low. BLAST is recommended for small datasets.\n\n.. _DIAMOND: http://ab.inf.uni-tuebingen.de/software/diamond/\n\n**Input**\n\nInput data is a large protein or nucleotide sequence file.\n\n\n**Output**\n\nDiamond gives you a tabular output file with 12 columns:\n\nColumn \tDescription\n1 \t    Query Seq-id (ID of your sequence)\n2 \t    Subject Seq-id (ID of the database hit)\n3 \t    Percentage of identical matches\n4 \t    Alignment length\n5 \t    Number of mismatches\n6 \t    Number of gap openings\n7 \t    Start of alignment in query\n8 \t    End of alignment in query\n9 \t    Start of alignment in subject (database hit)\n10 \t    End of alignment in subject (database hit)\n11 \t    Expectation value (E-value)\n12 \t    Bit score\n\n\nSupported values for gap open and gap extend parameters depending on the selected scoring matrix.\n\n========  ============================================\nMatrix    Supported values for (gap open)/(gap extend)\n========  ============================================\nBLOSUM45  (10-13)/3; (12-16)/2; (16-19)/1\nBLOSUM50  (9-13)/3; (12-16)/2; (15-19)/1\nBLOSUM62  (6-11)/2; (9-13)/1\nBLOSUM80  (6-9)/2; 13/2; 25/2; (9-11)/1\nBLOSUM90  (6-9)/2; (9-11)/1\nPAM250    (11-15)/3; (13-17)/2; (17-21)/1\nPAM70     (6-8)/2; (9-11)/1\nPAM30     (5-7)/2; (8-10)/1\n========  ============================================\n\n\n\n    ",
        "id": "bg_diamond",
        "interpreter": null,
        "language": null,
        "name": "Diamond",
        "readme": false,
        "tests": true,
        "version": "0.9.21.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    ## need to link because diamont tries to open dataset_xxx.dat.daa \t\n    ln -s '$daa' input.daa &&\n    diamond\n        view\n        --daa input.daa\n        #if $output.outfmt == \"0\"\n            --outfmt '0'\n            --out '$blast_pairw'\n        #else if $output.outfmt == \"5\"\n            --outfmt '5'\n            --out '$blast_xml'\n        #else if $output.outfmt == \"6\"\n            --outfmt '6' #echo ' '.join(str($output.fields).split(','))\n            --out '$blast_tabular'\n        #else if $output.outfmt == \"100\"\n            --outfmt '100'\n            --out output.daa\n        #else if $output.outfmt == \"101\"\n            --outfmt '101'\n            --out '$sam_output'\n        #else if $output.outfmt == \"102\"\n            --outfmt '102'\n            --out '$tax_output'\n        #end if\n    \n        #if str($hit_filter.hit_filter_select) == 'max':\n            --max-target-seqs '$hit_filter.max_target_seqs'\n        #else:\n            --top '$hit_filter.top'\n\t#end if\n    \n        $forwardonly\n        --compress '0'\n    \n    ",
        "dataFormats": {
            "inputs": [
                "daa"
            ],
            "outputs": []
        },
        "description": "generate formatted output from DAA files",
        "help": "\n\n\n**What it does**\n\nConverts diamond daa files to multiple other formats.\n\n**Input**\n\nInput data is a daa file.\n\n\n**Output**\n\nAlignment results in BLAST format (pairwise/tabular), xml, sam, taxonomic (Note the latter does not work with the current diamond version. )\n\nBLAST tables contain the following columns.\n\nColumn \tDescription\n1 \t    Query Seq-id (ID of your sequence)\n2 \t    Subject Seq-id (ID of the database hit)\n3 \t    Percentage of identical matches\n4 \t    Alignment length\n5 \t    Number of mismatches\n6 \t    Number of gap openings\n7 \t    Start of alignment in query\n8 \t    End of alignment in query\n9 \t    Start of alignment in subject (database hit)\n10 \t    End of alignment in subject (database hit)\n11 \t    Expectation value (E-value)\n12 \t    Bit score\n\n    ",
        "id": "bg_diamond_view",
        "interpreter": null,
        "language": null,
        "name": "Diamond",
        "readme": false,
        "tests": true,
        "version": "0.9.21"
    },
    {
        "ciation": [
            {
                "citation": "doi:10.1038/nature10730"
            }
        ],
        "code_file": null,
        "command": "\n        ## seems that diffbind also needs file extensions to work properly\n        #set $counter = 1\n        #for $sample in $samples:\n            ln -s $sample.bamreads #echo str($counter) + \"_bamreads.bam\"# &&\n            ln -s ${sample.bamreads.metadata.bam_index} #echo str($counter) + \"_bamreads.bai\"# &&\n            #if str( $sample.bamcontrol ) != 'None':\n                ln -s $sample.bamcontrol #echo str($counter) + \"_bamcontrol.bam\"# &&\n                ln -s ${sample.bamcontrol.metadata.bam_index} #echo str($counter) + \"_bamcontrol.bai\"# &&\n            #end if\n            #set $counter = $counter + 1\n        #end for\n\n        Rscript '$__tool_directory__/diffbind.R'\n            -i $infile\n            -o '$outfile'\n            -p '$plots'\n            -f $format\n            -t $th\n\n            #if $binding_affinity_matrix:\n                -b\n            #end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "bam",
                "bam",
                "bed"
            ],
            "outputs": [
                "bed",
                "pdf",
                "tabular"
            ]
        },
        "description": " differential binding analysis of ChIP-Seq peak data",
        "help": "\n\n.. class:: infomark\n\n**What it does**\n\nDiffBind_ is a `Bioconductor package`_ that provides functions for processing ChIP-Seq data enriched for genomic loci where specific\nprotein/DNA binding occurs, including peak sets identified by ChIP-Seq peak callers and\naligned sequence read datasets. It is designed to work with multiple peak sets simultaneously,\nrepresenting different ChIP experiments (antibodies, transcription factor and/or histone\nmarks, experimental conditions, replicates) as well as managing the results of multiple peak\ncallers.\n\nThe primary emphasis of DiffBind is on identifying sites that are differentially bound\nbetween two sample groups. It includes functions to support the processing of peak sets,\nincluding overlapping and merging peak sets, counting sequencing reads overlapping intervals\nin peak sets, and identifying statistically significantly differentially bound sites based on\nevidence of binding affinity (measured by differences in read densities). To this end it uses\nstatistical routines developed in an RNA-Seq context (primarily the Bioconductor packages\nedgeR and DESeq2 ). Additionally, the package builds on Rgraphics routines to provide a\nset of standardized plots to aid in binding analysis.\n\nThe `DiffBind User Guide`_ includes a brief overview of the processing flow, followed by four sections of\nexamples: the first focusing on the core task of obtaining differentially bound sites based on\naffinity data, the second working through the main plotting routines, the third discussing the\nuse of a blocking factor, and the fourth revisiting occupancy data (peak calls) in more detail,\nas well as comparing the results of an occupancy-based analysis with an affinity-based one.\nFinally, certain technical aspects of the how these analyses are accomplished are detailed.\n\nNote DiffBind requires a minimum of four samples (two groups with two replicates each).\n\n.. _DiffBind: https://bioconductor.org/packages/release/bioc/html/DiffBind.html\n.. _`Bioconductor package`: https://bioconductor.org/packages/release/bioc/html/DiffBind.html\n.. _`DiffBind User Guide`: https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf\n\n**Inputs**\n\nDiffBind works primarily with peaksets, which are sets of genomic intervals representing\ncandidate protein binding sites. Each interval consists of a chromosome, a start and end\nposition, and usually a score of some type indicating confidence in, or strength of, the peak.\nAssociated with each peakset are metadata relating to the experiment from which the peakset\nwas derived. Additionally, files containing mapped sequencing reads (generally .bam files) can\nbe associated with each peakset (one for the ChIP data, and optionally another representing\na control sample)\n\n**Sample Information**\n\nYou have to specify your sample information in the tool form above.\n\nExample:\n\n    ============= ========== ========== ============= =============\n     **SampleID** **Tissue** **Factor** **Condition** **Replicate**\n    ------------- ---------- ---------- ------------- -------------\n    BT4741        BT474      ER         Resistant     1            \n    BT4742        BT474      ER         Resistant     2            \n    MCF71         MCF7       ER         Responsive    1            \n    MCF72         MCF7       ER         Responsive    2            \n    MCF73         MCF7       ER         Responsive    3            \n    T47D1         T47D       ER         Responsive    1            \n    T47D2         T47D       ER         Responsive    2            \n    MCF7r1        MCF7       ER         Resistant     1            \n    MCF7r2        MCF7       ER         Resistant     2            \n    ZR751         ZR75       ER         Responsive    1            \n    ZR752         ZR75       ER         Responsive    2            \n    ============= ========== ========== ============= =============\n\nOr provide a sample sheet tabular file such as below.\n\nExample:\n\n    ======== ======  ====== ========== ========== ========= ====================  ========= ===================== ================= ==========\n    SampleID Tissue  Factor Condition  Treatment  Replicate bamReads              ControlID bamControl            Peaks             PeakCaller\n    ======== ======  ====== ========== ========== ========= ====================  ========= ===================== ================= ==========\n    BT4741   BT474   ER     Resistant  Full-Media  1        Chr18_BT474_ER_1.bam  BT474c    Chr18_BT474_input.bam BT474_ER_1.bed.gz bed\n    BT4742   BT474   ER     Resistant  Full-Media  2        Chr18_BT474_ER_2.bam  BT474c    Chr18_BT474_input.bam BT474_ER_2.bed.gz bed\n    MCF71    MCF7    ER     Responsive Full-Media  1        Chr18_MCF7_ER_1.bam   MCF7c     Chr18_MCF7_input.bam  MCF7_ER_1.bed.gz  bed\n    MCF72    MCF7    ER     Responsive Full-Media  2        Chr18_MCF7_ER_2.bam   MCF7c     Chr18_MCF7_input.bam  MCF7_ER_2.bed.gz  bed\n    MCF73    MCF7    ER     Responsive Full-Media  3        Chr18_MCF7_ER_3.bam   MCF7c     Chr18_MCF7_input.bam  MCF7_ER_3.bed.gz  bed\n    T47D1    T47D    ER     Responsive Full-Media  1        Chr18_T47D_ER_1.bam   T47Dc     Chr18_T47D_input.bam  T47D_ER_1.bed.gz  bed\n    T47D2    T47D    ER     Responsive Full-Media  2        Chr18_T47D_ER_2.bam   T47Dc     Chr18_T47D_input.bam  T47D_ER_2.bed.gz  bed\n    MCF7r1   MCF7    ER     Resistant  Full-Media  1        Chr18_TAMR_ER_1.bam   TAMRc     Chr18_TAMR_input.bam  TAMR_ER_1.bed.gz  bed\n    MCF7r2   MCF7    ER     Resistant  Full-Media  2        Chr18_TAMR_ER_2.bam   TAMRc     Chr18_TAMR_input.bam  TAMR_ER_2.bed.gz  bed\n    ZR751    ZR75    ER     Responsive Full-Media  1        Chr18_ZR75_ER_1.bam   ZR75c     Chr18_ZR75_input.bam  ZR75_ER_1.bed.gz  bed\n    ZR752    ZR75    ER     Responsive Full-Media  2        Chr18_ZR75_ER_2.bam   ZR75c     Chr18_ZR75_input.bam  ZR75_ER_2.bed.gz  bed\n    ======== ======  ====== ========== ========== ========= ====================  ========= ===================== ================= ==========\n\n\n**Peak files**\n\nResult of your Peak calling experiment in bed format, one file for each sample is required.\n\nExample:\n\n    ======= ======= ======= =============== =======\n    1          2      3          4           **5**\n    ======= ======= ======= =============== =======\n    chr18   215562  216063  MACS_peak_16037 56.11\n    chr18   311530  312105  MACS_peak_16038 222.49\n    chr18   356656  357315  MACS_peak_16039 92.06\n    chr18   371110  372092  MACS_peak_16040 123.86\n    chr18   395116  396464  MACS_peak_16041 1545.39\n    chr18   399014  400382  MACS_peak_16042 1835.19\n    chr18   499134  500200  MACS_peak_16043 748.32\n    chr18   503518  504552  MACS_peak_16044 818.30\n    chr18   531672  532274  MACS_peak_16045 159.30\n    chr18   568326  569282  MACS_peak_16046 601.11\n    ======= ======= ======= =============== =======\n\n* BAM file which contains the mapped sequencing reads can be associated with each peakset\n* Control BAM file represents a control dataset and are optional, but have to specified for all when used.\n\n\n**Outputs**\n\nAs output format you can choose BED, GFF, WIG.\n\nExample:\n\n======== ====== =======+\nseqnames ranges strand             Conc Conc_Resistant\n\n2452     chr18 [64490686, 64491186] * | 6.36 1.39\n1291     chr18 [34597713, 34598213] * | 5.33 0.22\n976      chr18 [26860997, 26861497] * | 7.3 3.13\n2338     chr18 [60892900, 60893400] * | 7.13 1.84\n2077     chr18 [55569087, 55569587] * | 5.52 1.89\n\nConc_Responsive Fold p-value FDR\n<numeric> <numeric> <numeric> <numeric>\n2452 7 -5.61 3.57e-10 1.02e-06\n1291 5.97 -5.75 1.1e-09 1.57e-06\n976 7.92 -4.79 1.1e-08 1.05e-05\n2338 7.77 -5.93 1.68e-08 1.17e-05\n2077 6.13 -4.23 2.36e-08 1.17e-05\n\nThe value columns show the\nConc mean read concentration over all the samples (the default calculation uses log2 normalized ChIP read counts with control read counts subtracted) \nConc_Resistant mean concentration over the first (Resistant) group \nConc_Responsive mean concentration over second (Responsive) group \nFold column shows the difference in mean concentrations between the two groups (Conc_Resistant - Conc_Responsive), with a positive value indicating increased binding affinity in the Resistant group and a negative value indicating increased binding affinity in the Responsive group.\np-value confidence measure for identifying these sites as differentially bound \nFDR a multiple testing corrected FDR p-value\n\n\n**Binding Affinity Matrix**\n\nThe final result of counting is a binding affinity matrix containing a (normalized) read count for each sample at every potential binding site. With this matrix, the samples can be re-clustered using affinity, rather than occupancy, data. The binding affinity matrix can be used for QC plotting as well as for subsequent\ndifferential analysis.\n\nExample:\n\n    ====== ====== ====== ========== ========== ========= ====== ========= ====\n    ID     Tissue Factor Condition  Treatment  Replicate Caller Intervals FRiP\n    ====== ====== ====== ========== ========== ========= ====== ========= ====\n    BT4741 BT474  ER     Resistant  Full-Media 1         counts 2845      0.16\n    BT4742 BT474  ER     Resistant  Full-Media 2         counts 2845      0.15\n    MCF71  MCF7   ER     Responsive Full-Media 1         counts 2845      0.27\n    MCF72  MCF7   ER     Responsive Full-Media 2         counts 2845      0.17\n    MCF73  MCF7   ER     Responsive Full-Media 3         counts 2845      0.23\n    T47D1  T47D   ER     Responsive Full-Media 1         counts 2845      0.10\n    T47D2  T47D   ER     Responsive Full-Media 2         counts 2845      0.06\n    MCF7r1 MCF7   ER     Resistant  Full-Media 1         counts 2845      0.20\n    MCF7r2 MCF7   ER     Resistant  Full-Media 2         counts 2845      0.13\n    ZR751  ZR75   ER     Responsive Full-Media 1         counts 2845      0.32\n    ZR752  ZR75   ER     Responsive Full-Media 2         counts 2845      0.22\n    ====== ====== ====== ========== ========== ========= ====== ========= ====\n\n\n\n**More Information**\n\nGenerally, processing data with DiffBind involves five phases:\n\n #. Reading in peaksets\n #. Occupancy analysis\n #. Counting reads\n #. Differential binding affinity analysis\n #. Plotting and reporting\n\n\n * **Reading in peaksets**: \n\nThe first step is to read in a set of peaksets and associated\nmetadata. Peaksets are derived either from ChIP-Seq peak callers, such as MACS\n([1]), or using some other criterion (e.g. genomic windows, or all the promoter regions\nin a genome). The easiest way to read in peaksets is using a comma-separated value\n(csv) sample sheet with one line for each peakset. (Spreadsheets in Excel\u00ae format, with\na .xls or .xlsx suffix, are also accepted.) A single experiment can have more than\none associated peakset; e.g. if multiple peak callers are used for comparison purposes\neach sample would have more than one line in the sample sheet. Once the peaksets\nare read in, a merging function finds all overlapping peaks and derives a single set of\nunique genomic intervals covering all the supplied peaks (a consensus peakset for the\nexperiment).\n\n * **Occupancy analysis**: \n\nPeaksets, especially those generated by peak callers, provide\nan insight into the potential occupancy of the protein being ChIPed for at specific\ngenomic loci. After the peaksets have been loaded, it can be useful to perform some\nexploratory plotting to determine how these occupancy maps agree with each other,\ne.g. between experimental replicates (re-doing the ChIP under the same conditions),\nbetween different peak callers on the same experiment, and within groups of samples\nrepresenting a common experimental condition. DiffBind provides functions to enable\noverlaps to be examined, as well as functions to determine how well similar samples\ncluster together. Beyond quality control, the product of an occupancy analysis may be\na consensus peakset, representing an overall set of candidate binding sites to be used\nin further analysis.\n\n * **Counting reads**: \n\nOnce a consensus peakset has been derived, DiffBind can use the\nsupplied sequence read files to count how many reads overlap each interval for each\nunique sample. The peaks in the consensus peakset may be re-centered and trimmed\nbased on calculating their summits (point of greatest read overlap) in order to provide\nmore standardized peak intervals. The final result of counting is a binding affinity matrix\ncontaining a (normalized) read count for each sample at every potential binding site.\nWith this matrix, the samples can be re-clustered using affinity, rather than occupancy,\ndata. The binding affinity matrix is used for QC plotting as well as for subsequent\ndifferential analysis.\n\n * **Differential binding affinity analysis**: \n\nThe core functionality of DiffBind is the\ndifferential binding affinity analysis, which enables binding sites to be identified that\nare statistically significantly differentially bound between sample groups. To accomplish\nthis, first a contrast (or contrasts) is established, dividing the samples into groups to\nbe compared. Next the core analysis routines are executed, by default using DESeq2 .\nThis will assign a p-value and FDR to each candidate binding site indicating confidence\nthat they are differentially bound.\n\n * **Plotting and reporting**: \n\nOnce one or more contrasts have been run, DiffBind provides\na number of functions for reporting and plotting the results. MA plots give an\noverview of the results of the analysis, while correlation heatmaps and PCA plots show\nhow the groups cluster based on differentially bound sites. Boxplots show the distribution\nof reads within differentially bound sites corresponding to whether they gain or\nlose affinity between the two sample groups. A reporting mechanism enables differentially\nbound sites to be extracted for further processing, such as annotation, motif, and\npathway analyses.\n\n**References**\n\nDiffBind Authors:  Rory Stark, Gordon Brown (2011)\nWrapper authors: Bjoern Gruening, Pavankumar Videm\n\n\n    ",
        "id": "diffbind",
        "interpreter": null,
        "language": null,
        "name": "DiffBind",
        "readme": false,
        "tests": true,
        "version": "2.6.5.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "antigenic -sequence $input1 -outfile $out_file1 -minlen $minlen -rformat2 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "antigenic"
            ]
        },
        "description": "Predicts potentially antigenic regions of a protein sequence, using the method of Kolaskar and Tongaonkar.",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/antigenic.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n    \n  ",
        "id": "EMBOSS: antigenic1",
        "interpreter": null,
        "language": "py",
        "name": "antigenic",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "compseq -sequence $input1 -outfile $out_file1 -word $word -frame $frame -auto",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "compseq"
            ]
        },
        "description": "Count composition of dimer/trimer/etc words in a sequence",
        "help": "\n.. class:: warningmark\n\nThe input dataset needs to be sequences.\n\n-----\n\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/compseq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: compseq14",
        "interpreter": null,
        "language": null,
        "name": "compseq",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "emboss_cpgplot_wrapper.pl cpgplot -sequence $input1 -window $window -minlen $minlen -minpc $minpc -outfile $outfile -graph png -goutfile $goutfile -outfeat $outfeat -minoe $minoe -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "cpgplot",
                "png",
                "gff"
            ]
        },
        "description": "Plot CpG rich areas",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cpgplot.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: cpgplot15",
        "interpreter": "perl",
        "language": "py",
        "name": "cpgplot",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "descseq -sequence $input1 -outseq $out_file1 -name \"$seqname\" -description \"$desc\" -append $append -osformat2 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "fasta"
            ]
        },
        "description": "Alter the name or description of a sequence",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/descseq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: descseq21",
        "interpreter": null,
        "language": "py",
        "name": "descseq",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "freak -seqall $input1 -outfile $out_file1 -window $window -letters $letters -graph png -step $step -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "freak"
            ]
        },
        "description": "Residue/base frequency table or plot",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/freak.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: freak36",
        "interpreter": null,
        "language": null,
        "name": "freak",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "geecee -sequence $input1 -outfile $out_file1 -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "geecee"
            ]
        },
        "description": "Calculates fractional GC content of nucleic acid sequences",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/geecee.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: geecee41",
        "interpreter": null,
        "language": null,
        "name": "geecee",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "helixturnhelix -sequence $input1 -outfile $out_file1 -mean $mean -sd $sd -minsd $minsd -eightyseven $eightyseven -rformat2 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "motif"
            ]
        },
        "description": "Report nucleic acid binding motifs",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/helixturnhelix.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: helixturnhelix43",
        "interpreter": null,
        "language": "py",
        "name": "helixturnhelix",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "hmoment -seqall $input1 -outfile $out_file1 -window $window -aangle $aangle -graph png -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "hmoment"
            ]
        },
        "description": "Hydrophobic moment calculation",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/hmoment.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: hmoment44",
        "interpreter": null,
        "language": null,
        "name": "hmoment",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "infoseq -sequence $input1 -outfile $out_file1 -html $html_out1 -heading $heading -usa $usa -name $disname -accession $accession -gi $gi -version $version -type $type -length $length -pgc\n  $pgc -description $description -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": "Displays some simple information about sequences",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/infoseq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: infoseq46",
        "interpreter": null,
        "language": "py",
        "name": "infoseq",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "megamerger -asequence $input1 -bsequence $input2 -outseq $out_file1 -outfile $out_file2 -wordsize $wordsize -prefer $prefer -osformat3 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "data",
                "data"
            ],
            "outputs": [
                "fasta",
                "txt"
            ]
        },
        "description": "Merge two large overlapping nucleic acid sequences",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/megamerger.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: megamerger53",
        "interpreter": null,
        "language": "py",
        "name": "megamerger",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "needle -asequence $input1 -bsequence $input2 -outfile $out_file1 -gapopen $gapopen -gapextend $gapextend -brief $brief -aformat3 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fasta"
            ],
            "outputs": [
                "needle"
            ]
        },
        "description": "Needleman-Wunsch global alignment",
        "help": "\n\n.. class:: warningmark\n\nneedle reads any two sequences of the same type (DNA or protein).\n\n-----\n\n**Syntax**\n\nThis tool uses the Needleman-Wunsch global alignment algorithm to find the optimum alignment (including gaps) of two sequences when considering their entire length. \n\n- **Optimal alignment:** Dynamic programming methods ensure the optimal global alignment by exploring all possible alignments and choosing the best.\n\n- **The Needleman-Wunsch algorithm** is a member of the class of algorithms that can calculate the best score and alignment in the order of mn steps, (where 'n' and 'm' are the lengths of the two sequences).\n\n- **Gap open penalty:** [10.0 for any sequence] The gap open penalty is the score taken away when a gap is created. The best value depends on the choice of comparison matrix. The default value assumes you are using the EBLOSUM62 matrix for protein sequences, and the EDNAFULL matrix for nucleotide sequences. (Floating point number from 1.0 to 100.0)\n\n- **Gap extension penalty:** [0.5 for any sequence] The gap extension, penalty is added to the standard gap penalty for each base or residue in the gap. This is how long gaps are penalized. Usually you will expect a few long gaps rather than many short gaps, so the gap extension penalty should be lower than the gap penalty. An exception is where one or both sequences are single reads with possible sequencing errors in which case you would expect many single base gaps. You can get this result by setting the gap open penalty to zero (or very low) and using the gap extension penalty to control gap scoring. (Floating point number from 0.0 to 10.0)\n\nYou can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/needle.html\n\n-----\n\n**Example**\n\n- Input File::\n\n    >hg18_dna range=chrX:151073054-151073136 5'pad=0 3'pad=0 revComp=FALSE strand=? repeatMasking=none\n    TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA\n    GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG\n\n- If both Sequence1 and Sequence2 take the above file as input, Gap open penalty equals 10.0, Gap extension penalty equals 0.5, Brief identity and similarity is set to Yes, Output Alignment File Format is set to SRS pairs, the output file is::\n\n    ########################################\n    # Program: needle\n    # Rundate: Mon Apr 02 2007 14:23:16\n    # Align_format: srspair\n    # Report_file: ./database/files/dataset_7.dat\n    ########################################\n     \n    #=======================================\n    #\n    # Aligned_sequences: 2\n    # 1: hg18_dna\n    # 2: hg18_dna\n    # Matrix: EDNAFULL\n    # Gap_penalty: 10.0\n    # Extend_penalty: 0.5\n    #\n    # Length: 83\n    # Identity:      83/83 (100.0%)\n    # Similarity:    83/83 (100.0%)\n    # Gaps:           0/83 ( 0.0%)\n    # Score: 415.0\n    #\n    #=======================================\n\n    hg18_dna           1 TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA     50\n                       ||||||||||||||||||||||||||||||||||||||||||||||||||\n    hg18_dna           1 TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA     50\n        \n    hg18_dna          51 GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG     83\n                       |||||||||||||||||||||||||||||||||\n    hg18_dna          51 GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG     83\n        \n    #---------------------------------------\n    #---------------------------------------\n\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: needle56",
        "interpreter": null,
        "language": "py",
        "name": "needle",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "notseq -sequence $input1 -outseq $out_file1 -exclude \"$exclude\" -osformat3 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "fasta"
            ]
        },
        "description": "Exclude a set of sequences and write out the remaining ones",
        "help": "\n\n.. class:: warningmark\n\nThe input dataset needs to be sequences.\n\n-----\n\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/notseq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: notseq61",
        "interpreter": null,
        "language": "py",
        "name": "notseq",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "emboss_single_outputfile_wrapper.pl octanol -sequence $input1 -graph png -goutfile $out_file1 -width $width -octanolplot $octanolplot -interfaceplot $interfaceplot\n  -differenceplot $differenceplot -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "png"
            ]
        },
        "description": "Displays protein hydropathy",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/octanol.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: octanol63",
        "interpreter": "perl",
        "language": null,
        "name": "octanol",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "emboss_single_outputfile_wrapper.pl pepwindow -sequence $input1 -graph png -goutfile $out_file1 -length $length -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "png"
            ]
        },
        "description": "Displays protein hydropathy",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/pepwindow.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: pepwindow73",
        "interpreter": "perl",
        "language": null,
        "name": "pepwindow",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "emboss_single_outputfile_wrapper.pl pepwindowall -sequence $input1 -graph png -goutfile $out_file1 -length $length -auto",
        "dataFormats": {
            "inputs": [
                "data"
            ],
            "outputs": [
                "png"
            ]
        },
        "description": "Displays protein hydropathy of a set of sequences",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/pepwindowall.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: pepwindowall74",
        "interpreter": "perl",
        "language": null,
        "name": "pepwindowall",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "prettyseq -sequence $input1 -outfile $out_file1 -ruler $ruler -plabel $plabel -nlabel $nlabel -width $width -auto",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "prettyseq"
            ]
        },
        "description": "Output sequence with translated ranges",
        "help": "\n\n.. class:: warningmark\n\nThe input dataset needs to be sequences.\n\n-----\n\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/prettyseq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: prettyseq80",
        "interpreter": null,
        "language": null,
        "name": "prettyseq",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "seqmatchall -sequence $input1 -outfile $out_file1 -wordsize $wordsize -aformat2 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "seqmatchall"
            ]
        },
        "description": "All-against-all comparison of a set of sequences",
        "help": "\n\n.. class:: warningmark\n\nThe input dataset needs to be sequences.\n\n-----\n\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/seqmatchall.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: seqmatchall83",
        "interpreter": null,
        "language": "py",
        "name": "seqmatchall",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "supermatcher -asequence $input1 -bsequence $input2 -gapopen \"$gapopen\" -gapextend \"$gapextend\" -width \"$width\" -wordlen \"$wordlen\" -outfile $ofile1 -errorfile $ofile2 -aformat3\n  $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "fasta",
                "data"
            ],
            "outputs": [
                "simple",
                "supermatcher"
            ]
        },
        "description": "Match large sequences against one or more other sequences",
        "help": "\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/supermatcher.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: supermatcher95",
        "interpreter": null,
        "language": "py",
        "name": "supermatcher",
        "readme": false,
        "tests": false,
        "version": "5.0.0"
    },
    {
        "ciation": null,
        "code_file": "emboss_format_corrector.py",
        "command": "transeq -sequence $input1 -outseq $out_file1 -frame $frame -table $table -regions \"$regions\" -trim $trim -clean $clean -alternative $alternative -osformat2 $out_format1 -auto",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "fasta"
            ]
        },
        "description": "Translate nucleic acid sequences",
        "help": "\n\n.. class:: warningmark\n\nThe input dataset needs to be sequences.\n\n-----\n\n    You can view the original documentation here_.\n    \n    .. _here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/transeq.html\n\n------\n\n**Citation**\n\nFor the underlying tool, please cite `Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. <http://www.ncbi.nlm.nih.gov/pubmed/10827456>`_\n\nIf you use this tool in Galaxy, please cite `Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. <http://www.ncbi.nlm.nih.gov/pubmed/17568012>`_\n  ",
        "id": "EMBOSS: transeq101",
        "interpreter": null,
        "language": "py",
        "name": "transeq",
        "readme": false,
        "tests": true,
        "version": "5.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btu135"
            },
            {
                "citation": "10.1093/bioinformatics/bts526"
            }
        ],
        "code_file": null,
        "command": "\n    R CMD BATCH --no-save --no-restore '--args mypars=\"$args_file\"' $__tool_directory__/exomedepth.R\n  ",
        "dataFormats": {
            "inputs": [
                "bed",
                "bam"
            ],
            "outputs": [
                "tabular"
            ]
        },
        "description": "cnv caller",
        "help": "\n\n.. class:: warningmark\n\n**Warning about counts for chromosome X**\n\nCalling CNVs on the X chromosome can create issues if the exome sample of interest and the reference exome\nsamples it is being compared to are not gender matched.\nMake sure that the genders are matched properly (i.e. do not use male as a reference for female\nsamples and vice versa).\n\n**What it does**\n\nThis tool uses ExomeDepth to call copy number variants (CNVs) from targeted sequence data.\n\n**Output format**\n\n=========== ========================\nColumn      Description\n----------- ------------------------\nchr         Chromosome\nstart       Start of CNV region\nend         End of CNV region\ntype        CNV type (deletion, duplication)\nsample      Name of the sample with CNV\ncorr        Correlation between reference and test counts. To get meaningful result, this correlation should really be above 0.97. If this is not the case, consider the output of ExomeDepth as less reliable (i.e. most likely a high false positive rate) \nnexons      Number of target regions covered by the CNV\nBF          Bayes factor. It quantifies the statistical support for each CNV. It is in fact the log10 of the likelihood ratio of data for the CNV call divided by the null (normal copy number). The higher that number, the more confident one can be about the presence of a CNV. While it is difficult to give an ideal threshold, and for short exons the Bayes Factor are bound to be unconvincing, the most obvious large calls should be easily flagged by ranking them according to this quantity\nreads.ratio Observed/expected reads ratio\n=========== ========================\n\n\n**What ExomeDepth does and does not do**\n\nExomeDepth uses read depth data to call CNVs from exome sequencing experiments. A key idea is that the test \nexome should be compared to a matched aggregate reference set. This aggregate reference set should combine \nexomes from the same batch and it should also be optimized for each exome. It will certainly di\ufb00er from one exome \nto the next.\n\nImportantly, ExomeDepth assumes that the CNV of interest is absent from the aggregate reference set. Hence \nrelated individuals should be excluded from the aggregate reference. It also means that ExomeDepth can miss \ncommon CNVs, if the call is also present in the aggregate reference. ExomeDepth is really suited to detect rare \nCNV calls (typically for rare Mendelian disorder analysis).\n\nThe ideas used in this package are of course not speci\ufb01c to exome sequencing and could be applied to other \ntargeted sequencing datasets, as long as they contain a su\ufb03ciently large number of exons to estimate the parameters \n(at least 20 genes, say, but probably more would be useful). Also note that PCR based enrichment studies are often \nnot well suited for this type of read depth analysis. The reason is that as the number of cycles is often set to a high \nnumber in order to equalize the representation of each amplicon, which can discard the CNV information.\n\n**License and citation**\n\nThis Galaxy tool is Copyright \u00a9 2014 `CRS4 Srl.`_ and is released under the `MIT license`_.\n\n.. _CRS4 Srl.: http://www.crs4.it/\n.. _MIT license: http://opensource.org/licenses/MIT\n\nYou can use this tool only if you agree to the license terms of: `ExomeDepth`_.\n\n.. _ExomeDepth: http://cran.r-project.org/web/packages/ExomeDepth/\n\nIf you use this tool, please cite:\n\n- |Cuccuru2014|_\n- |Plagnol2012|_.\n\n.. |Cuccuru2014| replace:: Cuccuru, G., Orsini, M., Pinna, A., Sbardellati, A., Soranzo, N., Travaglione, A., Uva, P., Zanetti, G., Fotia, G. (2014) Orione, a web-based framework for NGS analysis in microbiology. *Bioinformatics* 30(13), 1928-1929\n.. _Cuccuru2014: http://bioinformatics.oxfordjournals.org/content/30/13/1928\n.. |Plagnol2012| replace:: Plagnol, V., *et al.* (2012) A robust model for read count data in exome sequencing experiments and implications for copy number variant calling. *Bioinformatics* 28(21), 2747-2754\n.. _Plagnol2012: http://bioinformatics.oxfordjournals.org/content/28/21/2747\n  ",
        "id": "exomedepth",
        "interpreter": null,
        "language": null,
        "name": "ExomeDepth",
        "readme": false,
        "tests": false,
        "version": "1.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/molbev/msv150"
            },
            {
                "citation": "10.1089/106652702761034136"
            }
        ],
        "code_file": null,
        "command": "fastme -f 6\n                    --input_data=$input\n                    --output_tree=$outputTree\n                    --output_matrix=$outputMatrix\n                    --output_info=$outputLog\n                    --nb_threads=\\${GALAXY_SLOTS:-1}\n\n                    #if $typeChoice.datatype ==\"d\"\n                        --dna=$typeChoice.modeldna\n\n                    #else if $typeChoice.datatype ==\"p\"\n                        --protein=$typeChoice.modelprot\n\n                    #else if $typeChoice.datatype ==\"cfg\":\n                         ## Read information of sequence type\n                         ## read an info file to choose which option set and set a model by default\n                         #set $info = open( str($input_info) ).read()\n                         #if 'dna' in $info:\n                                --dna=T\n                         #else if 'protein' in $info:\n                               --protein=L\n                         #end if\n                    #end if\n\n                    #if $gammaChoice.gamma == \"true\"\n                        --gamma=$gammaChoice.rate\n                    #end if\n\n                    $distance\n                    $equilibrium\n                    $removeGap\n                    $treeRefinement\n\n                    #if $bootChoice.boot == \"true\"\n                        --output_boot=$outputBoostrap\n                        --bootstrap=$bootChoice.replicates\n                    #end if\n\n                    >> tmp_stdout;\n                    cat tmp_stdout > $outputLog;\n\n    ",
        "dataFormats": {
            "inputs": [
                "phylip, phy",
                "txt"
            ],
            "outputs": [
                "nhx",
                "txt",
                "nhx",
                "txt"
            ]
        },
        "description": "Distance-based inference of phylogenetic trees",
        "help": "\n\n.. class:: infomark\n\n**FastME** version 2.1.5 http://www.atgc-montpellier.fr/fastme\n\n.. class:: infomark\n\n**Galaxy integration** Provided by Southgreen & Andres Gwendoline (Institut Fran\u00c3\u00a7ais de Bioinformatique) & Marcon Valentin (IFB & INRA)\n\n.. class:: infomark\n\n**Support** For any questions about Galaxy integration, please send an e-mail to alexis.dereeper@ird.fr\n\n-----\n\n\n############\nFastME 2.0\n############\n\n-----------\nDescription\n-----------\n\nComprehensive, accurate and fast distance-based phylogeny inference program\n\nFastME provides distance algorithms to infer phylogenies. It's based on balanced minimum evolution, which is the very principle of NJ.\n\nFastME included Nearest Neighbor Interchange (NNI) and also Subtree Pruning and Regrafting (SPR), while remaining as fast as NJ and providing a number of facilities: distance estimation for DNA and proteins with various models and options, bootstrapping, and parallel computations.\n\nFor further informations, please visit FastME website_\n\n.. _website: http://www.atgc-montpellier.fr/fastme/usersguide.php\n\n\n------------\nDependencies\n------------\nFASTME\n        fastme_ 2.1.5 , Conda version\n\n.. _fastme: https://anaconda.org/bioconda/fastme\n\n\n---------------------------------------------------\n\n---------------\nWorking example\n---------------\n\nInput files\n===========\n\n**Phylip file**::\n\n\n  4   120\n  Orangutan   CCAAACGACA TTTCATATGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTGTCCGGAA\n  Gorilla     CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTCTCCGAAA\n  Human       CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATAAGA GACAAGTGAG CTCTCCGAAA\n  Chimp       CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTCTCCGAAA\n\n              CCAACATATC AGACATATGT GAATTTCAAT TATTGTACGG GCATCCTGGG CTCTCAAAGT\n              CCAAGATATC AGACATTTAT GAATTTCAAT TATTGTACGG GCATCCTGGG CTCTCAAAGT\n              CCAAGATACC AGACATTTGT GAATTTCAAT TATTGTACTG GCATCCTGGG CTCTCAAAGT\n              CCAAGATATC AGACATTTAT GAATTTCAAT TATTGTACTG GCATCCTGGG CTCTCAAAGT\n\n\n\nParameters\n==========\n\n::\n\n  Output name: Newick tree\n  Evolutionary model : TN93\n  Distance method : BIONJ\n\n\nOutput file\n===========\n\n**Newick tree**::\n\n    ((Gorilla:0.005755,Orangutan:0.020680):0.001063,Human:0.006655,Chimp:0.002132);\n\n-----\n\nOPTIONS\n=======\n\n\n  -i file, --input_data=file\n        The input data file contains sequence alignment(s) or a distance matrix(ces).\n\n  -u input_tree_file, --user_tree=input_tree_file\n        FastME may use an existing topology available in the input user tree file which corresponds to the input dataset.\n\n  -o output_tree_file, --output_tree=output_tree_file\n        FastME will write the infered tree into the output tree file.\n\n  -O output_matrix_file, --output_matrix=output_matrix_file\n        Use output matrix file option if you want FastME to write the distances\n        matrix computed from the input alignment in the output matrix file.\n\n  -I output_information_file, --output_info=output_information_file\n        Use this option if you want FastME to write information\n        about its execution in the output information file.\n\n  -B output_bootstrap_trees_file, --output_boot=output_bootstrap_trees_file\n        Use this option if you want FastME to write bootstrap trees\n        in the bootstrap trees file.\n\n  -a, --append\n        Use this option to append results to existing output files (if any).\n        By default output files will be overwritten.\n\n  -m method, --method=method\n        FastME computes a tree using a distance algorithm.\n        You may choose this method from:\n        TaxAdd_(B)alME, TaxAdd_(O)LSME, B(I)ONJ (default),\n        (N)J or (U)NJ.\n\n  -d model, --dna=model\n        Use this option if your input data file contains DNA sequences alignment(s).\n        You may also indicate the evolutionary [model] which can be choosen from:\n        (p)-distance, R(Y) symmetric, (R)Y, (J)C69, (K)2P, F8(1), F8(4) (default), (T)N93, (L)ogDet.\n\n  -p model, --protein=model\n        Use this option if your input data file contains protein sequences alignment(s).\n        You may also indicate the evolutionary [model] which can be choosen from:\n        (p)-distance, (F)81 like, (L)G (default), (W)AG, (J)TT, Day(h)off, (D)CMut, (C)pRev,\n        (M)tREV, (R)tREV, HIV(b), H(I)Vw or FL(U).\n\n  -r, --remove_gap\n        Use this option to completely remove any site which has a gap in\n        any sequence. By default, FastME is doing pairwise deletion of gaps.\n\n  -e, --equilibrium\n        The equilibrium frequencies for DNA are always estimated by counting\n        the occurence of the nucleotides in the input alignment.\n        For amino-acid sequences, the equilibrium frequencies are estimated\n        using the frequencies defined by the substitution model.\n        Use this option if you whish to estimate the amino-acid frequencies\n        by counting their occurence in the input alignment.\n\n  -g alpha, --gamma=alpha\n       Use this option if you wish to have gamma distributed rates across sites.\n       By default, FastME runs with no gamma variation.\n       If running FastME with gamma distributed rates across sites, the [alpha] default value is 1.0.\n       Only helpful when the input data file contains sequences alignment(s).\n\n  -n NNI, --nni=NNI\n        Use this option to do [NNI] tree topology improvement.\n        You may choose the [NNI] type from:\n        NNI_(B)alME (default) or NNI_(O)LS.\n\n  -s, --spr\n        Use this option to do SPR tree topology improvement.\n\n  -w branch, --branch_length=branch\n        Use this option to indicate the branch length to assign to the tree.\n        Only helpful when not improving the tree topology (no NNI nor SPR).\n        You may choose the branch length from:\n        (B)alLS (default), (O)LS or (n)one. (n)one is only available with BIONJ, NJ or UNJ.\n\n  -D datasets, --datasets=datasets\n        Use this option to indicate the number of datasets in your input\n        data file. Default value is 1.\n\n  -b replicates, --bootstrap=replicates\n        Use this option to indicate the number of replicates FastME will\n        do for bootstrapping. Default value is 0.\n        Only helpful when the input data file contains sequences alignment(s).\n\n  -z seed, --seed=seed\n        Use this option to initialize randomization with seed value. Only helpful when bootstrapping.\n\n  -c  Use this option if you want FastME only to compute distance matrix.\n        Only helpful when the input data file contains sequences alignment(s).\n\n  -T number_of_threads, --nb_threads=number_of_threads\n        Use this option to set the number of threads to use.\n        Default number of threads is 4.\n\n  -v value, --verbose=value\n        Sets the verbose level to value [0-3]. Default value is 0\n\n  -V, --version\n        Prints the FastME version.\n\n  -h, --help  Display this usage.\n\n\n    ",
        "id": "fastme",
        "interpreter": null,
        "language": null,
        "name": "FastME",
        "readme": false,
        "tests": true,
        "version": "2.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/molbev/msv150"
            },
            {
                "citation": "10.1089/106652702761034136"
            }
        ],
        "code_file": null,
        "command": "fastme --input_data=$input\n                    --output_tree=$outputTree\n                    --output_matrix=$outputMatrix\n                    --output_info=$outputLog\n                    --nb_threads=\\${GALAXY_SLOTS:-1}\n\n                    #if $typeChoice.datatype ==\"d\"\n                        --dna=$typeChoice.modeldna\n\n                    #else if $typeChoice.datatype ==\"p\"\n                        --protein=$typeChoice.modelprot\n\n                    #else if $typeChoice.datatype ==\"cfg\":\n                         ## Read information of sequence type\n                         ## read an info file to choose which option set and set a model by default\n                         #set $info = open( str($input_info) ).read()\n                         #if 'dna' in $info:\n                                --dna=T\n                         #else if 'protein' in $info:\n                               --protein=L\n                         #end if\n                    #end if\n\n                    #if $gammaChoice.gamma == \"true\"\n                        --gamma=$gammaChoice.rate\n                    #end if\n\n                    $distance\n                    $equilibrium\n                    $removeGap\n                    $treeRefinement\n\n                    #if $bootChoice.boot == \"true\"\n                        --output_boot=$outputBoostrap\n                        --bootstrap=$bootChoice.replicates\n                    #end if\n\n                    >> tmp_stdout;\n                    cat tmp_stdout > $outputLog;\n\n    ",
        "dataFormats": {
            "inputs": [
                "phylip, phy",
                "txt"
            ],
            "outputs": [
                "nwk",
                "txt",
                "nwk",
                "txt"
            ]
        },
        "description": "Distance-based inference of phylogenetic trees",
        "help": "\n\n.. class:: infomark\n\n\n**FastME version 2.1.4**\n\n\n.. class:: infomark\n\n\n**Galaxy integration** Andres Gwendoline, Institut Fran\u00c3\u00a7ais de Bioinformatique. Correia Damien, CNRS.\n**Support** For any questions about Galaxy integration, please send an e-mail to vincent.lefort@lirmm.fr\n\n\n-----\n\n\n############\nFastME 2.0\n############\n\nComprehensive, accurate and fast distance-based phylogeny inference program\n\n\n-----------\nDescription\n-----------\n\nFastME provides distance algorithms to infer phylogenies. It's based on balanced minimum evolution, which is the very principle of NJ.\n\nFastME included Nearest Neighbor Interchange (NNI) and also Subtree Pruning and Regrafting (SPR), while remaining as fast as NJ and providing a number of facilities: distance estimation for DNA and proteins with various models and options, bootstrapping, and parallel computations.\n\n------------\nDependencies\n------------\nFastME\n    http://www.atgc-montpellier.fr/fastme\n\n\n\n---------------------------------------------------\n\n---------------\nWorking example\n---------------\n\nInput files\n===========\n\n**Phylip file**::\n\n\n  4   120\n  Orangutan   CCAAACGACA TTTCATATGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTGTCCGGAA\n  Gorilla     CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTCTCCGAAA\n  Human       CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATAAGA GACAAGTGAG CTCTCCGAAA\n  Chimp       CCAAACAACA TTTCATGTGC TGTCATTTCT GAAGATATGA GACAAGTGAG CTCTCCGAAA\n\n              CCAACATATC AGACATATGT GAATTTCAAT TATTGTACGG GCATCCTGGG CTCTCAAAGT\n              CCAAGATATC AGACATTTAT GAATTTCAAT TATTGTACGG GCATCCTGGG CTCTCAAAGT\n              CCAAGATACC AGACATTTGT GAATTTCAAT TATTGTACTG GCATCCTGGG CTCTCAAAGT\n              CCAAGATATC AGACATTTAT GAATTTCAAT TATTGTACTG GCATCCTGGG CTCTCAAAGT\n\n\n\nParameters\n==========\n\n::\n\n  Output name: Newick tree\n  Evolutionary model : TN93\n  Distance method : BIONJ\n\n\nOutput files\n============\n\n**Newick tree**::\n\n    ((Gorilla:0.005755,Orangutan:0.020680):0.001063,Human:0.006655,Chimp:0.002132);\n\n-----\n\nOPTIONS\n=======\n\n\n  -i file, --input_data=file\n        The input data file contains sequence alignment(s) or a distance matrix(ces).\n\n  -u input_tree_file, --user_tree=input_tree_file\n        FastME may use an existing topology available in the input user tree file which corresponds to the input dataset.\n\n  -o output_tree_file, --output_tree=output_tree_file\n        FastME will write the infered tree into the output tree file.\n\n  -O output_matrix_file, --output_matrix=output_matrix_file\n        Use output matrix file option if you want FastME to write the distances\n        matrix computed from the input alignment in the output matrix file.\n\n  -I output_information_file, --output_info=output_information_file\n        Use this option if you want FastME to write information\n        about its execution in the output information file.\n\n  -B output_bootstrap_trees_file, --output_boot=output_bootstrap_trees_file\n        Use this option if you want FastME to write bootstrap trees\n        in the bootstrap trees file.\n\n  -a, --append\n        Use this option to append results to existing output files (if any).\n        By default output files will be overwritten.\n\n  -m method, --method=method\n        FastME computes a tree using a distance algorithm.\n        You may choose this method from:\n        TaxAdd_(B)alME, TaxAdd_(O)LSME, B(I)ONJ (default),\n        (N)J or (U)NJ.\n\n  -d model, --dna=model\n        Use this option if your input data file contains DNA sequences alignment(s).\n        You may also indicate the evolutionary [model] which can be choosen from:\n        (p)-distance, R(Y) symmetric, (R)Y, (J)C69, (K)2P, F8(1), F8(4) (default), (T)N93, (L)ogDet.\n\n  -p model, --protein=model\n        Use this option if your input data file contains protein sequences alignment(s).\n        You may also indicate the evolutionary [model] which can be choosen from:\n        (p)-distance, (F)81 like, (L)G (default), (W)AG, (J)TT, Day(h)off, (D)CMut, (C)pRev,\n        (M)tREV, (R)tREV, HIV(b), H(I)Vw or FL(U).\n\n  -r, --remove_gap\n        Use this option to completely remove any site which has a gap in\n        any sequence. By default, FastME is doing pairwise deletion of gaps.\n\n  -e, --equilibrium\n        The equilibrium frequencies for DNA are always estimated by counting\n        the occurence of the nucleotides in the input alignment.\n        For amino-acid sequences, the equilibrium frequencies are estimated\n        using the frequencies defined by the substitution model.\n        Use this option if you whish to estimate the amino-acid frequencies\n        by counting their occurence in the input alignment.\n\n  -g alpha, --gamma=alpha\n       Use this option if you wish to have gamma distributed rates across sites.\n       By default, FastME runs with no gamma variation.\n       If running FastME with gamma distributed rates across sites, the [alpha] default value is 1.0.\n       Only helpful when the input data file contains sequences alignment(s).\n\n  -n NNI, --nni=NNI\n        Use this option to do [NNI] tree topology improvement.\n        You may choose the [NNI] type from:\n        NNI_(B)alME (default) or NNI_(O)LS.\n\n  -s, --spr\n        Use this option to do SPR tree topology improvement.\n\n  -w branch, --branch_length=branch\n        Use this option to indicate the branch length to assign to the tree.\n        Only helpful when not improving the tree topology (no NNI nor SPR).\n        You may choose the branch length from:\n        (B)alLS (default), (O)LS or (n)one. (n)one is only available with BIONJ, NJ or UNJ.\n\n  -D datasets, --datasets=datasets\n        Use this option to indicate the number of datasets in your input\n        data file. Default value is 1.\n\n  -b replicates, --bootstrap=replicates\n        Use this option to indicate the number of replicates FastME will\n        do for bootstrapping. Default value is 0.\n        Only helpful when the input data file contains sequences alignment(s).\n\n  -z seed, --seed=seed\n        Use this option to initialize randomization with seed value. Only helpful when bootstrapping.\n\n  -c  Use this option if you want FastME only to compute distance matrix.\n        Only helpful when the input data file contains sequences alignment(s).\n\n  -T number_of_threads, --nb_threads=number_of_threads\n        Use this option to set the number of threads to use.\n        Default number of threads is 4.\n\n  -v value, --verbose=value\n        Sets the verbose level to value [0-3]. Default value is 0\n\n  -V, --version\n        Prints the FastME version.\n\n  -h, --help  Display this usage.\n\n\nFor further informations, please visite FastME_\n\n.. _FastME: http://www.atgc-montpellier.fr/fastme/usersguide.php\n\n    ",
        "id": "fastme",
        "interpreter": null,
        "language": null,
        "name": "FastME",
        "readme": false,
        "tests": true,
        "version": "2.1.4.2"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n#if $input.is_of_type('fasta.gz', 'fastqsanger.gz', 'fastqsolexa.gz', 'fastqillumina.gz'):\n            gunzip -c -f '$input' |\n        #elif $input.is_of_type('fastqsanger.bz2', 'fastqsolexa.bz2', 'fastqillumina.bz2'):\n            bzcat -f '$input' |\n        #else:\n            cat '$input' |\n        #end if\n     fastx_clipper\n-l $minlength\n-a '$clip_source.clip_sequence'\n-d $keepdelta\n-o '$output'\n-v\n$KEEP_N\n$DISCARD_OPTIONS\n#if $input.is_of_type('fastqsanger', 'fastqsanger.gz', 'fastqsanger.bz2'):\n                -Q 33\n            #elif $input.is_of_type('fastqsolexa', 'fastqsolexa.gz', 'fastqsolexa.bz2', 'fastqillumina', 'fastqillumina.gz', 'fastqillumina.bz2'):\n                -Q 64\n            #end if\n        \n    \n#if $input.is_of_type('fasta.gz', 'fastqsanger.gz', 'fastqsolexa.gz', 'fastqillumina.gz'):\n                -z\n            #end if\n        \n    \n    ",
        "dataFormats": {
            "inputs": [],
            "outputs": []
        },
        "description": "adapter sequences",
        "help": "\n**What it does**\n\nThis tool clips adapters from the 3'-end of the sequences in a FASTA/FASTQ file.\n\n--------\n\n\n**Clipping Illustration:**\n\n.. image:: fastx_clipper_illustration.png\n\n**Clipping Example:**\n\n.. image:: fastx_clipper_example.png\n\n**In the above example:**\n\n* Sequence no. 1 was discarded since it wasn't clipped (i.e. didn't contain the adapter sequence). (**Output** parameter).\n* Sequence no. 5 was discarded --- it's length (after clipping) was shorter than 15 nt (**Minimum Sequence Length** parameter).\n\n------\n\nThis tool is based on `FASTX-toolkit`__ by Assaf Gordon.\n\n .. __: http://hannonlab.cshl.edu/fastx_toolkit/\n    ",
        "id": "cshl_fastx_clipper",
        "interpreter": null,
        "language": null,
        "name": "Clip",
        "readme": false,
        "tests": true,
        "version": "1.0.3"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btw191"
            }
        ],
        "code_file": null,
        "command": "\n      Rscript --slave --vanilla $GALAXY_ROOT_DIR/tools/flowtools/FCSflowAI.R --args \"${input}\" \"${remove}\" $alphaFR $chremFS $outFS $penFS \"${sideFM}\" \"${full_rep}\" $highfcs $lowfcs $qcfcs\n  \n  ",
        "dataFormats": {
            "inputs": [
                "fcs"
            ],
            "outputs": [
                "html",
                "fcs",
                "fcs",
                "fcs"
            ]
        },
        "description": " automatic quality control ",
        "help": "\n   This tool automatically performs quality control of flow cytometry data.\n\n-----\n\n**Input files**\n\n  \u2022 One or more FCS files.\n\n**Output files**\n\n  \u2022 full HTML report\n  \u2022 new FCS file containing only high quality events (default)\n  \u2022 new FCS file containing only low quality events (optional)\n  \u2022 original FCS file containing an additional parameter where the low quality events have a value higher than 10,000 (optional)\n\n\nThe files generated will be FCS 3.0.\n\n----\n\nDescription of the approach\n'''''''''''''''''''''''''''\nThis tool identifies anomalies from three fundamental properties of flow cytometry data:\n\n  - *Flow rate*. Surges and substantial shifts of the rate of the cells passing through the capillary tube are detected.\n\n  - *Signal acquisition*. Instability in the signal acquired for each channel are detected. In most cases it corresponds to flow rate surges and shifts.\n\n  - *Dynamic range*. Values recorded in the upper limit (margin events) and negative outliers are removed.\n\n.. class:: infomark\n\nAn HTML report with informative plots is generated. Users are advised to review the report and also::\n\n  1. Eventually adjust the quality control parameters\n  2. Discard the entire FCS file because of an unacceptable number of anomalies\n  3. Program a flow cytometry maintenance because of recurrent issues\n\n\nParameters\n''''''''''\nDefault settings work well in the majority of cases. Setting customization may be needed to address properties of unique datasets. For example, high-dimensional FCS files may perform best with more tolerant setttings for signal acquisition checks.\n\nExample\n'''''''\nThis section provides an example of a flowAI quality control html report with plots:\n\n\nFlow rate check: anomalies are flagged with a green circle. In this instance a surge was detected and discarded as well as a shift from the median value later in the experiment.\n\n.. image:: static/images/autoflowrate.png\n\nSignal acquistion check: Orange background (or yellow depending on the user's computer) highlights the stable region. Signal acquistion shifts are identified on a per channel basis and the largest region containing no anomalies is retained.\n\n.. image:: static/images/autosignal.png\n\nDynamic range check: red and blue lines reflect the detected number of events over time. The x-axis corresponds to that of the signal acquisition plot.\n\n.. image:: static/images/margins.png\n\n \n  ",
        "id": "flowAI",
        "interpreter": null,
        "language": null,
        "name": "flowAI",
        "readme": false,
        "tests": true,
        "version": "1.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1101/gr.107524.110"
            },
            {
                "citation": "10.1038/ng.806"
            },
            {
                "citation": "10.1002/0471250953.bi1110s43"
            }
        ],
        "code_file": null,
        "command": "\n        ############################\n        ## import analysis specific preprocessings by using cheetahs internal searchList\n        ## if not defined, ignore\n        ############################\n        #if $analysis_type.analysis_type_selector + \"Preprocessing\" in vars()['SL'][2]\n            #set $analysisPreprocessing = vars()['SL'][2][$analysis_type.analysis_type_selector + \"Preprocessing\"]\n            #include source=$analysisPreprocessing\n        #end if\n        \n        ############################\n        ## GATK tool unspecific options\n        ############################\n        @GATK_EXEC@\n        \n        --analysis_type ${analysis_type.analysis_type_selector}\n        --reference_sequence    ${ref_file.fields.path}\n\n        --log_to_file           ${output_log}\n\n        #if $cond_intervals.cond_intervals_enabled\n            #for $interval in $cond_intervals.intervals:\n                --intervals ${interval.L}\n            #end for\n        #end if\n\n        #if $cond_BQSR.cond_BQSR_enabled\n          --BQSR $cond_BQSR.BQSR\n        #end if\n\n        ############################\n        ## import analysis specific options by using cheetahs internal searchList\n        ## if not defined throw raw python error until better idea\n        ############################\n        #if $analysis_type.analysis_type_selector + \"Options\" in vars()['SL'][2]\n            #set $analysisOptions = vars()['SL'][2][$analysis_type.analysis_type_selector + \"Options\"]\n            #include source=$analysisOptions\n        #else\n            #set $analysisOptions = vars()['SL'][2][$analysis_type.analysis_type_selector + \"Options\"]\n        #end if\n        \n        ############################\n        ## only put ERROR or FATAL log messages into stderr\n        ## but keep full log for printing into log file\n        ############################\n        2>&1 | awk '\\$1 != \"INFO\" && \\$1 != \"WARN\"' >&2\n",
        "dataFormats": {
            "inputs": [
                "tabular"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": "tool collection Version 3.4-0",
        "help": null,
        "id": "gatk",
        "interpreter": null,
        "language": null,
        "name": "GATK",
        "readme": false,
        "tests": false,
        "version": "3.4-0.d9"
    },
    {
        "ciation": [
            {
                "citation": "\n@misc{githubgoenrichment,\n  author = {Faria, Daniel},\n  year = {2017},\n  title = {GOEnrichment},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  url = {https://github.com/DanFaria/GOEnrichment},\n}"
            }
        ],
        "code_file": null,
        "command": "GOEnrichment.jar \n--go $go\n--annotation $annotation\n--study $study\n#if str($population) != 'None'\n--population $population\n#end if\n--correction $correction\n#if str($summarize) == 'true'\n--summarize_output\n#end if\n#if str($singletons) == 'true'\n--ignore_singletons\n#end if\n#if str($relations) == 'true'\n--use_all_relations\n#end if\n--graph_format $graph\n--cut_off $cutoff\n--mf_result $mf_result\n--bp_result $bp_result\n--cc_result $cc_result\n--mf_graph $mf_graph\n--bp_graph $bp_graph\n--cc_graph $cc_graph\n\t",
        "dataFormats": {
            "inputs": [
                "obo,owl",
                "txt",
                "txt",
                "txt"
            ],
            "outputs": [
                "tabular",
                "tabular",
                "tabular",
                "png",
                "png",
                "png"
            ]
        },
        "description": "performs GO enrichment analysis of a set of gene products",
        "help": "\n.. class:: infomark\n\nGOEnrichment is a Java application that can be used to analyze gene product sets (e.g., from microarray or RNAseq experiments) for enriched GO terms.\n\n-----\n\n.. class:: infomark\n\nGOEnrichment requires:\n\n- -A Gene Ontology file in either OBO or OWL format (see http://geneontology.org/page/download-ontology).\n- -A tabular annotation file in GAF (http://geneontology.org/page/download-annotations) format, BLAST2GO format, or a simple two-column table (e.g. from BioMart) with gene product ids in the first column and GO terms in the second one.\n- -A list of gene products comprising the study set (a flat text file with one gene product per line).\n- -Optionally, a list of gene products comprising the population set (if none is submitted, the population set will be the set of gene products listed in the annotation file).\n\n-----\n\n.. class:: infomark\n\nGOEnrichment produces a tabular result file and a graph file for each GO type (MF, BP and CC):\n\n- -The result file is a tabular list of all GO terms present in the study set and their respective p-values.\n- -The graph file can be either a png image, an svg image, or a text file for importing into cytoscape (together with the result file).\n\n-----\n\n.. class:: infomark\n\nThe graph is colored by p-value: terms with p-value above cut-off appear in white; and the color gets darker as the p-value decreases\n(see the scale at https://github.com/DanFaria/GOEnrichment/blob/master/Scale.png). In addition to the name of each GO term, the graph\nshows its frequency in the study set. Dashed edges indicate that one or more intermediate terms were ommited from the graph.\n\n-----\n\n.. class:: warningmark\n\nGene products listed in either the study or population set files that are not present in the annotation file will be ignored.\n\t",
        "id": "goenrichment",
        "interpreter": "java -jar",
        "language": null,
        "name": "GOEnrichment",
        "readme": false,
        "tests": true,
        "version": "2.0.0"
    },
    {
        "ciation": [
            {
                "citation": "\n@misc{githubgoenrichment,\n  author = {Faria, Daniel},\n  year = {2017},\n  title = {GOEnrichment},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  url = {https://github.com/DanFaria/GOEnrichment},\n}\n        "
            }
        ],
        "code_file": null,
        "command": "goenrichment \n--go '${go}'\n--annotation '${annotation}'\n--study '${study}'\n#if str($population) != 'None'\n--population '${population}'\n#end if\n--correction ${correction}\n$summarize\n$singletons\n$relations\n--graph_format $graph\n--cut_off $cutoff\n--mf_result '${mf_result}'\n--bp_result '${bp_result}'\n--cc_result '${cc_result}'\n--mf_graph '${mf_graph}'\n--bp_graph '${bp_graph}'\n--cc_graph '${cc_graph}'\n    ",
        "dataFormats": {
            "inputs": [
                "obo,owl",
                "tabular,txt",
                "txt",
                "txt"
            ],
            "outputs": [
                "tabular",
                "tabular",
                "tabular",
                "png",
                "png",
                "png"
            ]
        },
        "description": "performs GO enrichment analysis of a set of gene products",
        "help": "\n.. class:: infomark\n\nGOEnrichment is a Java application that can be used to analyze gene product sets (e.g., from microarray or RNAseq experiments) for enriched GO terms.\n\n-----\n\n.. class:: infomark\n\nGOEnrichment requires:\n\n- A Gene Ontology file in either OBO or OWL format (see http://geneontology.org/page/download-ontology).\n- A tabular annotation file in GAF (http://geneontology.org/page/download-annotations) format, BLAST2GO format, or a simple two-column table (e.g. from BioMart) with gene product ids in the first column and GO terms in the second one.\n- A list of gene products comprising the study set (a flat text file with one gene product per line).\n- Optionally, a list of gene products comprising the population set (if none is submitted, the population set will be the set of gene products listed in the annotation file).\n\n-----\n\n.. class:: infomark\n\nGOEnrichment produces a tabular result file and a graph file for each GO type (MF - Molecular Function, BP - Biological Process and CC - Cellular Component):\n\n- The result file is a tabular list of all GO terms present in the study set and their respective p-values.\n- The graph file can be either a png image, an svg image, or a text file for importing into cytoscape (together with the result file).\n\n-----\n\n.. class:: infomark\n\nThe graph is colored by p-value: terms with p-value above cut-off appear in white; and the color gets darker as the p-value decreases\n\n.. image:: https://github.com/DanFaria/GOEnrichment/raw/master/Scale.png\n   :width: 600\n   :height: 315\n\n(see the scale at https://github.com/DanFaria/GOEnrichment/blob/master/Scale.png). In addition to the name of each GO term, the graph\nshows its frequency in the study set. Dashed edges indicate that one or more intermediate terms were ommited from the graph.\n\n-----\n\n.. class:: warningmark\n\nGene products listed in either the study or population set files that are not present in the annotation file will be ignored.\n    ",
        "id": "goenrichment",
        "interpreter": null,
        "language": null,
        "name": "GOEnrichment",
        "readme": false,
        "tests": true,
        "version": "2.0.1"
    },
    {
        "ciation": [
            {
                "citation": "10.1186/gb-2010-11-2-r14"
            }
        ],
        "code_file": null,
        "command": "\nRscript '$__tool_directory__/goseq.r'\n\n--dge_file '$dge_file'\n--length_file '$length_file'\n\n#if $categorySource.catSource == 'getgo':\n    --genome $categorySource.genome\n    --gene_id $categorySource.gene_id\n    --fetch_cats '$categorySource.fetchcats'\n#elif $categorySource.catSource == 'history':\n    --category_file '$categorySource.category_file'\n#end if\n\n#if $methods['wallenius']:\n    --wallenius_tab '$wallenius_tab'\n#end if\n#if $methods['hypergeometric']:\n    --nobias_tab '$nobias_tab'\n#end if\n--repcnt '$methods.repcnt'\n--sampling_tab '$sampling_tab'\n\n--make_plots '$out.make_plots'\n--length_bias_plot '$length_bias_plot'\n--sample_vs_wallenius_plot '$sample_vs_wallenius_plot'\n\n--rdata '$out.rdata_out'\n--p_adj_method '$adv.p_adj_method'\n--use_genes_without_cat '$adv.use_genes_without_cat'\n\n#if $out.topgo_plot:\n    --top_plot '$out.topgo_plot'\n#end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "tabular",
                "tabular",
                "tabular",
                "tabular"
            ],
            "outputs": [
                "tabular",
                "tabular",
                "tabular",
                "pdf",
                "pdf",
                "rdata",
                "pdf"
            ]
        },
        "description": "tests for overrepresented gene categories",
        "help": "\n\n.. class:: infomark\n\n**What it does**\n\n`Gene Ontology`_ (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies, but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts. This tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG_ pathway analysis.\n\nOptions map closely to the excellent goseq manual_.\n\n-----\n\n**Inputs**\n\n*Differentially expressed genes file*\n\ngoseq needs a tabular file containing information on differentially expressed genes. This should contain all genes assayed in the RNA-seq experiment. The file should have two columns with an optional header row. The first column should contain the Gene IDs, which must be unique within the file and not repeated. The second column should contain True or False. True means the gene should count as differentially expressed, False means it is not differentially expressed. You can use the \"Compute an expression on every row\" tool to create a True / False column for your dataset.\n\nExample:\n\n    =============== =====\n    ENSG00000236824 False\n    ENSG00000162526 False\n    ENSG00000090402 True\n    ENSG00000169188 False\n    ENSG00000124103 False\n    =============== =====\n\n*Gene lengths file*\n\ngoseq needs information about the length of a gene to correct for potential length bias in differentially expressed genes using a Probability Weight Function (PWF). The PWF can be thought of, as a function which gives the probability that a gene will be differentially expressed, based on its length alone. The gene length file should have two columns with an optional header row. The first column should contain the Gene IDs, and the second column should contain the gene length in bp. If length data is unavailable for some genes, that entry should be set to NA. The goseq authors recommend using the gene lengths obtained from upstream summarization programs, such as **featureCounts**, if provided. Alternatively, the **Gene length and GC content** tool can produce such a file.\n\nExample:\n\n    =============== =====\n    ENSG00000236824 13458\n    ENSG00000162526 2191\n    ENSG00000090402 6138\n    ENSG00000169188 3245\n    ENSG00000124103 1137\n    =============== =====\n\n*Gene categories file*\n\nThis tool can get GO and KEGG categories for some genomes. The three GO categories are GO:MF (Molecular Function - molecular activities of gene products), GO:CC (Cellular Component - where gene products are active), GO:BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products). If your genome is not available, you will also need a file describing the membership of genes in categories. The category file should have two columns with an optional header row. with Gene ID in the first column and category identifier in the second column. As the mapping between categories and genes is usually many-to-many, this table will usually have multiple rows with the same Gene ID and category identifier.\n\nExample:\n\n    =============== ===========\n    ENSG00000162526 GO\\:0000003\n    ENSG00000198648 GO\\:0000278\n    ENSG00000112312 GO\\:0000278\n    ENSG00000174442 GO\\:0000278\n    ENSG00000108953 GO\\:0000278\n    =============== ===========\n\n-----\n\n**Outputs**\n\n* This tool outputs a tabular file containing a ranked list of gene categories, similar to below. The default output is the Wallenius method table. If the Sampling and/or Hypergeometric methods are also selected, additional tables are produced.\n* Optionally, this tool can also output a plot of the top 10 over-represented GO categories, some diagnostic plots and an RData file, see **Output Options** above.\n\nExample:\n\n=========== =============== ================ ============ ========== ======================================== ========== =================== ====================\n*category*  *over_rep_pval* *under_rep_pval* *numDEInCat* *numInCat* *term*                                   *ontology* *p.adjust.over_rep* *p.adjust.under_rep*\n----------- --------------- ---------------- ------------ ---------- ---------------------------------------- ---------- ------------------- --------------------\nGO\\:0005576  0.000054        0.999975         56           142       extracellular region                     CC         0.394825             1\nGO\\:0005840  0.000143        0.999988         9            12        ribosome                                 CC         0.394825             1\nGO\\:0044763  0.000252        0.999858         148          473       single-organism cellular process         BP         0.394825             1\nGO\\:0044699  0.000279        0.999844         158          513       single-organism process                  BP         0.394825             1\nGO\\:0065010  0.000428        0.999808         43           108       extracellular membrane-bounded organelle CC         0.394825             1\nGO\\:0070062  0.000428        0.999808         43           108       extracellular exosome                    CC         0.394825             1\n=========== =============== ================ ============ ========== ======================================== ========== =================== ====================\n\n-----\n\n**Method options**\n\n3 methods, *Wallenius*, *Sampling* and *Hypergeometric*, can be used to calculate the p-values as follows.\n\n*Wallenius*\n\napproximates the true distribution of numbers of members of a category amongst DE genes by the Wallenius non-central hypergeometric distribution.\nThis distribution assumes that within a category all genes have the same probability of being chosen. Therefore, this approximation works best when the range in probabilities obtained by the probability weighting function is small. This is the method used by default.\n\n*Sampling*\n\nuses random sampling to approximate the true distribution and uses it to calculate the p-values for over (and under) representation of categories.\nAlthough this is the most accurate method given a high enough value of sampling number, its use quickly becomes computationally prohibitive. It may sometimes be desirable to use random sampling to generate the null distribution for category\nmembership. For example, to check consistency against results from the Wallenius approximation. This is easily accomplished by using the method option to additionally specify sampling and the number of samples to generate.\n\n*Hypergeometric*\n\nassumes there is no bias in power to detect differential expression at all and calculates the p-values using a standard hypergeometric distribution (no length bias correction is performed). Useful if you wish to test the effect of length bias on your results.\nCaution: Hypergeometric should NEVER be used for producing results for biological interpretation of RNA-seq data. If length bias is truly not present in your data, goseq will produce a nearly flat PWF plot, no length bias correction will be applied to your data, and all methods will produce the same results.\n\n-----\n\n**More Information**\n\nIn order to account for the length bias inherent to RNA-seq data when performing a GO analysis\n(or other category based tests), one cannot simply use the hypergeometric distribution as the null\ndistribution for category membership, which is appropriate for data without DE length bias, such\nas microarray data. GO analysis of RNA-seq data requires the use of random sampling in order\nto generate a suitable null distribution for GO category membership and calculate each categories\nsignificance for over representation amongst DE genes.\n\nHowever, this random sampling is computationally expensive. In most cases, the Wallenius\ndistribution can be used to approximate the true null distribution, without any significant loss in\naccuracy. The goseq package implements this approximation as its default option. The option\nto generate the null distribution using random sampling is also included as an option, but users\nshould be aware that the default number of samples generated will not be enough to accurately\ncall enrichment when there are a large number of go terms.\n\nHaving established a null distribution, each category is then tested for over and under\nrepresentation amongst the set of differentially expressed genes and the null is used to calculate a\np-value for under and over representation.\n\nHaving performed a GO analysis, you may now wish to interpret the results. If you wish to\nidentify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to\nfirst apply some kind of multiple hypothesis testing correction. For example, you can identify GO categories over\nenriched using a 0.05 FDR (p.adjust) cutoff [Benjamini and Hochberg, 1995].\n\nUnless you are a machine, GO and KEGG category identifiers are probably not very meaningful to you.\nInformation about each identifier can be obtained from the `Gene Ontology`_ and KEGG_ websites.\n\n.. _manual: https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf\n.. _Gene Ontology: http://www.geneontology.org\n.. _KEGG: http://www.genome.jp/kegg\n\n    ",
        "id": "goseq",
        "interpreter": null,
        "language": null,
        "name": "goseq",
        "readme": false,
        "tests": true,
        "version": "1.34.0+galaxy1"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/molbev/msu300"
            },
            {
                "citation": "10.1093/molbev/mst024"
            }
        ],
        "code_file": null,
        "command": "\niqtree\n    -pre PREF\n    -nt \\${GALAXY_SLOTS:-10}\n    -redo\n\n## file\n#if $general_options.s\n    -s '$general_options.s'\n#end if\n\n## file\n#if $general_options.t\n    -t '$general_options.t'\n    $tree_parameters.constructing_consensus.con\n    $tree_parameters.constructing_consensus.net\n    #if str($tree_parameters.constructing_consensus.bi) != ''\n        -bi '$tree_parameters.constructing_consensus.bi'\n    #end if\n\n    ## file\n    #if $tree_parameters.constructing_consensus.sup\n        -sup '$tree_parameters.constructing_consensus.sup'\n    #end if\n\n    #if str($tree_parameters.constructing_consensus.suptag) != ''\n        -suptag '$tree_parameters.constructing_consensus.suptag'\n    #end if\n\n    ## file\n    #if $tree_parameters.computing_robinson_foulds.rf\n        -rf '$tree_parameters.computing_robinson_foulds.rf'\n        $tree_parameters.computing_robinson_foulds.rf_all\n        $tree_parameters.computing_robinson_foulds.rf_adj\n    #end if\n#end if\n\n## file\n#if $general_options.te\n    -te '$general_options.te'\n#end if\n\n#if str($general_options.st) != ''\n    -st '$general_options.st'\n#end if\n\n#if str($general_options.seed) != ''\n    -seed '$general_options.seed'\n#end if\n\n$general_options.keep_ident\n$general_options.safe\n\n#if str($likelihood_mapping.lmap) != ''\n    -lmap '$likelihood_mapping.lmap'\n#end if\n\n## file\n#if $likelihood_mapping.lmclust\n    -lmclust '$likelihood_mapping.lmclust'\n#end if\n\n$likelihood_mapping.wql\n\n#if str($modelling_parameters.automatic_model.cond_model.m) != ''\n    -m '$modelling_parameters.automatic_model.cond_model.m'\n#end if\n\n#if str($modelling_parameters.automatic_model.rcluster) != ''\n    -rcluster '$modelling_parameters.automatic_model.rcluster'\n#end if\n\n#if str($modelling_parameters.automatic_model.mset) != ''\n    -mset '$modelling_parameters.automatic_model.mset'\n#end if\n\n#if str($modelling_parameters.automatic_model.msub) != ''\n    -msub '$modelling_parameters.automatic_model.msub'\n#end if\n\n#if str($modelling_parameters.automatic_model.mfreq) != ''\n    -mfreq '$modelling_parameters.automatic_model.mfreq'\n#end if\n\n#if str($modelling_parameters.automatic_model.mrate) != ''\n    -mrate '$modelling_parameters.automatic_model.mrate'\n#end if\n\n#if str($modelling_parameters.automatic_model.cmin) != ''\n    -cmin '$modelling_parameters.automatic_model.cmin'\n#end if\n\n#if str($modelling_parameters.automatic_model.cmax) != ''\n    -cmax '$modelling_parameters.automatic_model.cmax'\n#end if\n\n#if str($modelling_parameters.automatic_model.merit) != ''\n    -merit '$modelling_parameters.automatic_model.merit'\n#end if\n\n$modelling_parameters.automatic_model.mtree\n\n#if str($modelling_parameters.automatic_model.madd) != ''\n    -madd '$modelling_parameters.automatic_model.madd'\n#end if\n\n## file\n#if $modelling_parameters.automatic_model.mdef\n    -mdef '$modelling_parameters.automatic_model.mdef'\n#end if\n\n$modelling_parameters.specifying_substitution.mwopt\n\n#if str($modelling_parameters.rate_heterogeneity.a) != ''\n    -a '$modelling_parameters.rate_heterogeneity.a'\n#end if\n\n$modelling_parameters.rate_heterogeneity.gmedian\n\n#if str($modelling_parameters.rate_heterogeneity.i) != ''\n    -i '$modelling_parameters.rate_heterogeneity.i'\n#end if\n\n$modelling_parameters.rate_heterogeneity.opt_gamma_inv\n$modelling_parameters.rate_heterogeneity.wsr\n\n## file\n#if $modelling_parameters.partition_model.q\n    -q '$modelling_parameters.partition_model.q'\n#end if\n\n$modelling_parameters.partition_model.spp\n\n## file\n#if $modelling_parameters.partition_model.sp\n    -sp '$modelling_parameters.partition_model.sp'\n#end if\n\n## file\n#if $modelling_parameters.site_specific_frequency.ft\n    -ft '$modelling_parameters.site_specific_frequency.ft'\n#end if\n\n#if str($modelling_parameters.site_specific_frequency.fs) != ''\n    -fs '$modelling_parameters.site_specific_frequency.fs'\n#end if\n\n$modelling_parameters.site_specific_frequency.fmax\n\n#if str($tree_parameters.tree_search.ninit) != ''\n    -ninit '$tree_parameters.tree_search.ninit'\n#end if\n\n#if str($tree_parameters.tree_search.ntop) != ''\n    -ntop '$tree_parameters.tree_search.ntop'\n#end if\n\n#if str($tree_parameters.tree_search.nbest) != ''\n    -nbest '$tree_parameters.tree_search.nbest'\n#end if\n\n#if str($tree_parameters.tree_search.nstop) != ''\n    -nstop '$tree_parameters.tree_search.nstop'\n#end if\n\n#if str($tree_parameters.tree_search.n) != ''\n    -n '$tree_parameters.tree_search.n'\n#end if\n\n#if str($tree_parameters.tree_search.sprrad) != ''\n    -sprrad '$tree_parameters.tree_search.sprrad'\n#end if\n\n#if str($tree_parameters.tree_search.pers) != ''\n    -pers '$tree_parameters.tree_search.pers'\n#end if\n\n$tree_parameters.tree_search.allnni\n$tree_parameters.tree_search.djc\n\n## file\n#if $tree_parameters.tree_search.g\n    -g '$tree_parameters.tree_search.g'\n#end if\n\n#if str($tree_parameters.single_branch.alrt) != ''\n    -alrt '$tree_parameters.single_branch.alrt'\n#end if\n\n$tree_parameters.single_branch.abayes\n\n#if str($tree_parameters.single_branch.lbp) != ''\n    -lbp '$tree_parameters.single_branch.lbp'\n#end if\n\n## file\n#if $tree_parameters.tree_topology.z\n    -z '$tree_parameters.tree_topology.z'\n#end if\n\n#if str($tree_parameters.tree_topology.zb) != ''\n    -zb '$tree_parameters.tree_topology.zb'\n#end if\n\n$tree_parameters.tree_topology.zw\n$tree_parameters.tree_topology.au\n\n#if str($tree_parameters.constructing_consensus.minsup) != ''\n    -minsup '$tree_parameters.constructing_consensus.minsup'\n#end if\n\n\n#if str($tree_parameters.generating_random.r) != ''\n    -r '$tree_parameters.generating_random.r'\n#end if\n\n$tree_parameters.generating_random.ru\n$tree_parameters.generating_random.rcat\n$tree_parameters.generating_random.rbal\n$tree_parameters.generating_random.rcsg\n\n#if str($tree_parameters.generating_random.rlen) != ''\n    -rlen '$tree_parameters.generating_random.rlen'\n#end if\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.bb) != ''\n    -bb '$bootstrap_parameters.ultrafast_bootstrap.bb'\n#end if\n\n$bootstrap_parameters.ultrafast_bootstrap.wbt\n$bootstrap_parameters.ultrafast_bootstrap.wbtl\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.nm) != ''\n    -nm '$bootstrap_parameters.ultrafast_bootstrap.nm'\n#end if\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.bcor) != ''\n    -bcor '$bootstrap_parameters.ultrafast_bootstrap.bcor'\n#end if\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.nstep) != ''\n    -nstep '$bootstrap_parameters.ultrafast_bootstrap.nstep'\n#end if\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.beps) != ''\n    -beps '$bootstrap_parameters.ultrafast_bootstrap.beps'\n#end if\n\n#if str($bootstrap_parameters.ultrafast_bootstrap.bspec) != ''\n    -bspec '$bootstrap_parameters.ultrafast_bootstrap.bspec'\n#end if\n\n$bootstrap_parameters.ultrafast_bootstrap.bnni\n\n#if str($bootstrap_parameters.nonparametric_bootstrap.b) != ''\n    -b '$bootstrap_parameters.nonparametric_bootstrap.b'\n#end if\n\n$bootstrap_parameters.nonparametric_bootstrap.bc\n$bootstrap_parameters.nonparametric_bootstrap.bo\n\n#if str($miscellaneous_options.fconst) != ''\n    -fconst '$miscellaneous_options.fconst'\n#end if\n\n\n    ",
        "dataFormats": {
            "inputs": [
                "txt",
                "nhx",
                "nhx",
                "txt",
                "txt",
                "txt",
                "txt",
                "nhx",
                "txt",
                "txt",
                "nhx",
                "nhx"
            ],
            "outputs": [
                "nhx",
                "nhx",
                "nhx",
                "mldist",
                "nex",
                "iqtree"
            ]
        },
        "description": "Phylogenomic / evolutionary tree construction from multiple sequences",
        "help": "\nNote that -st CODON is always necessary when using codon models and you also need to specify a genetic code like this if differed from the standard genetic code.\n<br/><i>-st</i> NT2AA tells IQ-TREE to translate protein-coding DNA into AA sequences and then subsequent analysis will work on the AA sequences. You can also use a genetic code like -st NT2AA5 for the Invertebrate Mitochondrial Code (see genetic code table).\n                ",
        "id": "iqtree",
        "interpreter": null,
        "language": null,
        "name": "IQ-TREE",
        "readme": false,
        "tests": true,
        "version": "1.5.5.1"
    },
    {
        "ciation": [
            {
                "citation": "10.1007/978-3-319-09192-1_4"
            }
        ],
        "code_file": null,
        "command": "\n\n        ## generate properties file\n        echo -e \"minSplitSize=2000000\\nmaxResultSize=0\\nmaxMatchSize=0\\nworkingDir=.\\ndir.result=.\\nmaxSpacerLength=0\\nmaxLength=0\\nminLength=2\\nparentStrategy=1\\nnbProcessor=1\\nnbJobs=1\\nmaxSolutions=100\\nminTreeIndex=2\\nsuffix.tool = 2\" > logol.properties\n\n        &&\n\n        LogolMultiExec.sh\n        #if str( $options_input.options_input_selector ) == \"model\":\n            -m ${options_input.m}\n        #else\n            -g ${options_input.g}\n        #end if\n\n        -${type}\n        -s ${s}\n        -max ${max}\n\n        ${fasta}\n        ${gff}\n        ${all}\n\n        -maxmatchsize ${maxmatchsize}\n        -maxspacer ${maxspacer}\n        -lmax ${lmax}\n        -lmin ${lmin}\n\n        ${forcesplit}\n\n        -conf logol.properties\n\n        -out 'logol_results.zip'\n\n        > ${log_file}\n\n        &&\n\n        unzip logol_results.zip\n\n    ",
        "dataFormats": {
            "inputs": [
                "txt",
                "txt",
                "text",
                "fasta"
            ],
            "outputs": [
                "txt",
                "zip"
            ]
        },
        "description": "Biological patterns matching",
        "help": "\n\nLogol is a pattern matching grammar language and a set of tools to search a pattern in a sequence (nucleic or proteic).\n\nIt includes a Linux command line tool and a graphical designer.\n\nhttp://logol.genouest.org/web/app.php/logol\n\n-------------------------------------------------------------------------\n\nSoftware is free and open source, under CeCILL license\n\n\n    ",
        "id": "logol_wrapper",
        "interpreter": null,
        "language": null,
        "name": "Logol",
        "readme": false,
        "tests": true,
        "version": "1.7.8"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n        RM_LIB_PATH=\\$(dirname \\$(which RepeatMasker))/../share/RepeatMasker/Libraries &&\n        mkdir lib &&\n        export REPEATMASKER_LIB_DIR=\\$(pwd)/lib &&\n          for file in \\$(ls \\$RM_LIB_PATH) ; do  ln -s \\$RM_LIB_PATH/\\$file lib/\\$file ; done &&\n        #if $repeat_masking.repeat_source.source_type == \"repbase\":\n          cp '${repeat_masking.repeat_source.repbase_file}' 'lib/${repeat_masking.repeat_source.repbase_file_name}' &&\n        #end if\n\n        maker -CTL\n\n        &&\n\n        sed \"s/cpus=/cpus=\\${GALAXY_SLOTS:-4}/g\" '$ctl' > maker_opts.ctl\n\n        &&\n\n        #if $abinitio_gene_prediction.aug_prediction.augustus_mode == 'history'\n\n            ## Using an augustus model from history, we need to unzip it and let augustus find it\n\n            cp -r \"\\$AUGUSTUS_CONFIG_PATH/\" augustus_dir/ &&\n\n            mkdir -p 'augustus_dir/species/' &&\n\n            tar -C 'augustus_dir/species/' -xzvf '${abinitio_gene_prediction.aug_prediction.augustus_model}' > /dev/null &&\n\n            export AUGUSTUS_CONFIG_PATH=`pwd`/augustus_dir/ &&\n        #end if\n\n        maker maker_opts.ctl maker_bopts.ctl maker_exe.ctl\n\n        &&\n\n        gff3_merge -d *.maker.output/*_master_datastore_index.log -o '${output_full}'\n\n        &&\n\n        awk '{if ($2 == \"maker\" || $1 ~ /^\\#/) {print}}' '${output_full}' | sed -n '/^\\#\\#FASTA\\$/q;p' > '${output_gff}'\n\n        &&\n\n        awk '{if ($2 != \"maker\") {print}}' '${output_full}' | sed -n '/^\\#\\#FASTA\\$/q;p' > '${output_evidences}'\n    ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "gff",
                "fasta",
                "fasta",
                "gff",
                "gff",
                "fasta",
                "gff",
                "snaphmm",
                "augustus",
                "embl",
                "fasta",
                "fasta",
                "gff",
                "gff",
                "fasta",
                "gff"
            ],
            "outputs": [
                "gff3",
                "gff3",
                "gff3"
            ]
        },
        "description": "genome annotation pipeline",
        "help": "\n        MAKER is a portable and easily configurable genome annotation pipeline. Its purpose is to allow smaller eukaryotic and prokaryotic genome projects to independently annotate their genomes and to create genome databases. MAKER identifies repeats, aligns ESTs and proteins to a genome, produces ab-initio gene predictions and automatically synthesizes these data into gene annotations having evidence-based quality values. MAKER is also easily trainable: outputs of preliminary runs can be used to automatically retrain its gene prediction algorithm, producing higher quality gene-models on seusequent runs. MAKER's inputs are minimal and its ouputs can be directly loaded into a GMOD database. They can also be viewed in the Apollo genome browser; this feature of MAKER provides an easy means to annotate, view and edit individual contigs and BACs without the overhead of a database. MAKER should prove especially useful for emerging model organism projects with minimal bioinformatics expertise and computer resources.\n\n        .. _Maker: http://www.yandell-lab.org/software/maker.html\n    ",
        "id": "maker",
        "interpreter": null,
        "language": null,
        "name": "Maker",
        "readme": false,
        "tests": true,
        "version": "2.31.9.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": " match $motif $seq $output $nmismatch $rc $bed > $log",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fasta"
            ],
            "outputs": [
                "txt",
                "bed"
            ]
        },
        "description": "find short motif occurrences",
        "help": "\n\n**What it does**\n\nThis tool searches occurrences of a short nucleotide seuqences (allowing mismatches) in a set of longer sequences.\n\nExample motif file::\n\n    >motif1\n    CAGGTAAGT\n    >motif2\n    GTTTGGGGGCC\n    \nExample sequence file::\n\n    >hg18_chr6_122208322_122209078_+\n    CGTCGTAGCTACTAGCTACGTACGTACGTAGCTAGCATGCATGCTACGTA\n    CGTAGCTAGCTAAAAAAAAAAAAAAACTGCGGCTAGCTAGCTAGCTACGT\n    CGATCGTAGCTAC...\n    >hg18_chr6_1208322_122209023_+\n    CGATGCTAGCTAGCTAGCTACGTAGCTAGCTAGTCGATGCTAGCTAGCTA     \n    ATGCTAGCTAGC....\n    \nOutput (bed)::\n\n    chr11\t72790893\t72790902\tACTTAACTG\t1\t-\tantisense\t5ss,G4T:CAGTTAAGT-rc\thg18_chr11_72790846_72791902_+\t47\n    chr11\t72791880\t72791889\tCAGGTAAGA\t1\t+\tsense\t5ss,T9A:CAGGTAAGA\thg18_chr11_72790846_72791902_+\t1034\n\n\nOutput (tab)::\n\n    Tmod4\t802\t5ss:CAGGTAAGT-rc\tACTTACCTG\n    Atp7b\t77\t5ss:CAGGTAAGT\tCAGGTAAGT\n    Fnta\t665\t5ss:CAGGTAAGT\tCAGGTAAGT\n\n\n\n  ",
        "id": "match",
        "interpreter": null,
        "language": null,
        "name": "match",
        "readme": false,
        "tests": false,
        "version": null
    },
    {
        "ciation": [
            {
                "citation": "10.1093/dnares/dsn027"
            }
        ],
        "code_file": null,
        "command": "\n        #set $output_list = str($output_formats).split(',')\n        touch mga_output\n        #for $input in $inputs:\n            && mga ${input} $multiple_species >> mga_output\n        #end for\n        #if 'tsv' in $output_list or 'bed' in $output_list:\n            && python '$__tool_directory__/convert_mga.py' mga_output -v \n            #if 'tsv' in $output_list\n                --tsv '$mga_tsv'\n            #end if\n            #if 'bed' in $output_list\n                --bed '$mga_bed'\n            #end if\n        #end if\n    ",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "txt",
                "tabular",
                "bed"
            ]
        },
        "description": "gene-finding program for prokaryote and phage (used by sixgill)",
        "help": "\n**MetaGeneAnnotator (mga)**\n\nA gene-finding program for prokaryote and phage.\n\nThe gene annotations can be used by sixgill_ when generating metapeptides from metagenomics shotgun sequencing.\n\n.. image:: Sixgill_MetaGeneAnnotator_Workflow.png\n  :height: 213\n  :width: 625\n\nusage:\n    mga [multi-fasta] <-m/-s>\n\n         -m    (multiple species sequences are individually treated)\n         -s    (single species sequences are treated as a unit)\n\n**Input:**\n    *A fasta file of metagenomic sequences*\n\n\n**Outputs:**\n\n    *MetaGeneAnnotator text report*\n        Output from the MetaGeneAnnotator mga application::\n\n            # 1/1\n            # gc = 0.275862, rbs = -1\n            # self: -\n            gene_1\t1812\t1994\t-\t0\t11\t14.1035\tb\t2002\t2007\t2.11797\n            # 2/1\n            # gc = 0.338877, rbs = -1\n            # self: -\n            gene_1\t1\t414\t+\t0\t01\t25.748\tb\t.\t.\t.\n            gene_2\t614\t790\t+\t0\t11\t0.774142\tb\t.\t.\t.\n            gene_3\t822\t1079\t+\t0\t11\t20.6507\tb\t.\t.\t.\n\n        output format description::\n\n            # [sequence name]\n            # gc = [gc%], rbs = [rbs%]\n            # self: [(b)acteria/(a)rchaea/(p)hage/unused(-)]\n            [gene ID] [start pos.] [end pos.] [strand] [frame] [complete/partial] [gene score] [used model] [rbs start] [rbs end] [rbs score]\n\n            explanations of output column:\n                *The value of [frame] (0/1/2) indicates the number of surplus (untranslated) nucleotides at the 5'-end of the predicted ORF.\n                *The value of [score] indicates the estimated score of predicted gene. All predicted genes are more than 0.\n                *The value of [complete/partial] indicates that the predicted gene structure is whether complete (contains both of start and stop codons[11]) or partial (lacks start[01] or stop[10] or both of them[00]).\n                *The value of [model] indicates a selected model ((s)elf/(b)acteria/(a)rchaea/(p)hage) for predicting the gene. \n\n\n    *MetaGeneAnnotator tabular report with sequence columns*\n        The mga output reformated as a tabular file::\n\n            #seq_id\tseq_model\tseq_gc\tseq_rbs\tgene ID\tstart pos\tend pos\tstrand\tframe\tcomplete/partial\tgene score\tused model\trbs start\trbs end\trbs score\n            1/1\t -\t0.275862\t-1\tgene_1\t1812\t1994\t-\t0\t11\t14.1035\tb\t2002\t2007\t2.11797\n            2/1\t -\t0.338877\t-1\tgene_1\t1\t414\t+\t0\t01\t25.748\tb\t.\t.\t.\n            2/1\t -\t0.338877\t-1\tgene_2\t614\t790\t+\t0\t11\t0.774142\tb\t.\t.\t.\n            2/1\t -\t0.338877\t-1\tgene_3\t822\t1079\t+\t0\t11\t20.6507\tb\t.\t.\t.\n\n\n    *MetaGeneAnnotator in BED format*\n        The mga output reformatted as a BED file which can be used to extract the DNA sequences for each gene from the fasta file::\n\n            1/1\t1811\t1994\t1/1:gene_1\t15\t-\t1811\t1994\t0\t1\t183\t0\n            2/1\t0\t414\t2/1:gene_1\t26\t+\t0\t414\t0\t1\t414\t0\n            2/1\t613\t790\t2/1:gene_2\t1\t+\t613\t790\t0\t1\t177\t0\n            2/1\t821\t1079\t2/1:gene_3\t21\t+\t821\t1079\t0\t1\t258\t0\n\n\n.. _sixgill: https://github.com/dhmay/sixgill\n    ",
        "id": "metagene_annotator",
        "interpreter": null,
        "language": null,
        "name": "MetaGeneAnnotator",
        "readme": false,
        "tests": true,
        "version": "1.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1021/pr301024c"
            }
        ],
        "code_file": null,
        "command": "\n        mkdir -p output_reports &&\n        cwd=`pwd` &&\n        #import re\n        #set $searchdb_name = $searchdb\n        #if $searchdb.is_of_type('uniprotxml'): \n          #set $searchdb_name = 'searchdb.xml'\n        #else\n          #set $searchdb_name = 'searchdb.fasta'\n        #end if\n        ln -s $searchdb $searchdb_name\n        ## Need to link each input to a name in cwd, names must be unique\n        #set $input_list = []\n        #for i,input in enumerate($inputs):\n          #set $input_name = $re.sub('(?i)([.]?mzML)*$','.mzML',$re.sub('\\W','_',$input.name))\n          #if $input_name in $input_list:\n            #set $input_name = str($i) + '_' + $input_name\n          #end if\n          #set $input_list = $input_list + [$input_name]\n          && ln -s $input $input_name\n        #end for\n        #set $input_names = ','.join($input_list)\n        &&\n\n        mono `which morpheus`\n        -d='$input_names'\n        -db='$searchdb_name'\n\n        #if $searchdb.is_of_type('uniprotxml'): \n          #if str( $advanced.adv_options_selector) == \"set\":\n            $advanced.noup\n          #end if\n        #end if\n        ## fm vm fdr mvmi precmt precmtv precmtu\n        #if str($fdr) != '':\n            -fdr=$fdr\n        #end if\n        #if  str($mvmi) != '':\n            -mvmi=$mvmi\n        #end if\n        #if  str($precmtv) != '':\n            -precmtv=$precmtv\n        #end if\n        #if  str($precmtu) != 'None':\n            -precmtu=$precmtu\n        #end if\n        #if str( $advanced.adv_options_selector) == \"set\":\n            #if  str($advanced.precmt) != 'None':\n                -precmt=$advanced.precmt\n            #end if\n            #if  str($advanced.minprecz) != '':\n                -minprecz=$advanced.minprecz\n            #end if\n            #if  str($advanced.maxprecz) != '':\n                -maxprecz=$advanced.maxprecz\n            #end if\n            #if  str($advanced.at) != '':\n                -at=$advanced.at\n            #end if\n            #if  str($advanced.rt) != '':\n                -rt=$advanced.rt\n            #end if\n            #if  str($advanced.mp) != '':\n                -mp=$advanced.mp\n            #end if\n            #if  str($advanced.mmc) != '':\n                -mmc=$advanced.mmc\n            #end if\n            #if  str($advanced.prodmt) != 'None':\n                -prodmt=$advanced.prodmt\n            #end if\n            #if  str($advanced.prodmtv) != '':\n                -prodmtv=$advanced.prodmtv\n            #end if\n            #if  str($advanced.prodmtu) != 'None':\n                -prodmtu=$advanced.prodmtu\n            #end if\n            #if  str($advanced.minpmo) != '':\n                -minpmo=$advanced.minpmo\n            #end if\n            #if  str($advanced.maxpmo) != '':\n                -maxpmo=$advanced.maxpmo\n            #end if\n            #if  str($advanced.imb) != 'None':\n                -imb=$advanced.imb\n            #end if\n            #if  str($advanced.ad) != 'None':\n                -ad=$advanced.ad\n            #end if\n            $advanced.acs $advanced.di $advanced.pmc $advanced.cmu $advanced.mmu\n        #end if\n        #if str($fm) != 'None':\n            #set $fmods = str($fm).replace(',',';')\n            -fm=\"$fmods\"\n        #end if\n        #if str($vm) != 'None':\n            #set $vmods = str($vm).replace(',',';')\n            -vm=\"$vmods\"\n        #end if\n        -mt=\\${GALAXY_SLOTS:-4}\n        #set $out_list = 'log.txt summary.tsv aggregate.PSMs.tsv aggregate.unique_peptides.tsv aggregate.protein_groups.tsv aggregate.mzid *.pep.xml'\n        #if len($input_list) == 1:\n          && ln -s *.log.txt log.txt\n          && ln -s *.mzid aggregate.mzid \n          && ln -s *.unique_peptides.tsv aggregate.unique_peptides.tsv\n          && ln -s *.protein_groups.tsv aggregate.protein_groups.tsv\n          && ln -s *.PSMs.tsv aggregate.PSMs.tsv\n        #end if\n          && ( basepath=`grep 'Proteome Database:' log.txt  | sed 's/Proteome Database: \\(.*\\)${$searchdb_name}/\\1/'`;\n             for i in $out_list; do cat \\$i | sed \"s#\\${basepath}\\##\" > output_reports/\\$i; done )\n    ",
        "dataFormats": {
            "inputs": [
                "mzml",
                "fasta,uniprotxml"
            ],
            "outputs": [
                "txt",
                "txt",
                "tabular",
                "tabular",
                "tabular",
                "mzid"
            ]
        },
        "description": "database search algorithm for high-resolution tandem mass spectra",
        "help": "\nMorpheus_ is a database search algorithm for high-resolution tandem mass spectra. \n\nWhen a Uniprot Proteome XML file is used for the search database, Morpheus will include all known modifications from the proteome in searching for peptide spectral matches.  To fetch Uniprot Proteome XML files see:  http://www.uniprot.org/help/retrieve_sets\n\n**INPUTS**\n\n  - spectral data file in mzML format \n  - protein search database, either a fasta file or a uniprot proteome xml file\n\n**OUTPUTS**\n\n  - summary.tsv\n  - input.log.txt\n  - input.PSMs.tsv\n  - input.unique_peptides.tsv\n  - input.protein_groups.tsv\n  - input.pep.xml\n\n.. _Morpheus: http://morpheus-ms.sourceforge.net/\n\n    ",
        "id": "morpheus",
        "interpreter": null,
        "language": null,
        "name": "Morpheus",
        "readme": false,
        "tests": true,
        "version": "2.255.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "chipmunk.sh -f $inputfile -n $motif_number_selector -m $minw -v $maxw -z $mode -o $log_outfile -p $processed_output -i $image_output -x \"$name\" -y ${GALAXY_DATA_INDEX_DIR}",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "log",
                "txt",
                "png"
            ]
        },
        "description": "De novo motif finding",
        "help": "\n**What it does**\n\nChIPmunk detects over-represented non-overlapping motifs in fasta sequences\n\n  ",
        "id": "chipmunk_chiphorde",
        "interpreter": "bash",
        "language": null,
        "name": "ChIPmunk",
        "readme": true,
        "tests": false,
        "version": "1.0.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    snvmix.py\n      -i $input1\n      -d ${input1.metadata.dbkey}\n      -o $output_snvmix\n      -t $type\n      #if $positionFile == \"yes\":\n        -l $pos\n      #else:\n        -l 'none'\n      #end if\n      -x ${GALAXY_DATA_INDEX_DIR}\n      -q $q\n      -Q $Q\n      -f $full\n      -R $keep_dups\n      -c $keep_chastity\n  ",
        "dataFormats": {
            "inputs": [
                "bam",
                "bam",
                "fasta",
                "tabular"
            ],
            "outputs": [
                "tabular"
            ]
        },
        "description": "performs SNV calling on a bam file",
        "help": "\n\n**What it does**\n\nThis tool uses the SNVMix model (Goya et al) to call single nucleotide variants (SNVs) from a bam file.  The various models described in the study are provided (e.g. SNVMix1, MB, Mb etc.).  The most commonly-used models are MB (the default) and SNVMix1.  The former thresholds on base quality and mapping quality and uses both qualities in the variant prediction while the latter filters on both and only uses the allele counts in the model.  The output is a tab-delimited file that should be passed through the accompanying filtering script prior to use.  \n\n  ",
        "id": "snvmix",
        "interpreter": "python",
        "language": null,
        "name": "SNVMix",
        "readme": true,
        "tests": false,
        "version": "0.12.1-rc1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n\n    python '$__tool_directory__/ob_filter.py'\n      -i '${infile}'\n      -o '${outfile}'\n      -iformat \"${infile.ext}\"\n      -oformat \"${infile.ext}\"\n      --filters '{\n        #if $filter_methods.filter_methods_opts == \"ruleof5\":\n          \"hbd\" : [0, 5],\n          \"hba\" : [0, 10],\n          \"molwt\" : [0, 500],\n          \"logp\" : [-5, 5],\n        #elif $filter_methods.filter_methods_opts == \"LeadLike\":\n          \"rotbonds\" : [0, 7],\n          \"molwt\" : [0, 350],\n          \"logp\" : [-5, 3.5],\n        #elif $filter_methods.filter_methods_opts == \"DrugLike\":\n          \"hba\" : [0, 10],\n          \"rotbonds\" : [0, 8],\n          \"molwt\" : [150, 500],\n          \"logp\" : [-5, 5],\n          \"psa\" : [0, 150],\n        #elif $filter_methods.filter_methods_opts == \"FragmentLike\":\n          \"rotbonds\" : [0, 5],\n          \"molwt\" : [0, 250],\n          \"logp\" : [-5, 2.5],\n        #else:\n          #for $filter in $filter_methods.filter_set:\n            #set $filter_selected = $filter.filter_sel.filter_sel_opts\n            #set $filter_min = $filter_selected + \"_min\"\n            #set $filter_max = $filter_selected + \"_max\"\n            \"$filter_selected\" : [$filter.filter_sel[$filter_min], $filter.filter_sel[$filter_max] ],\n          #end for\n        #end if\n      }'\n\n  ",
        "dataFormats": {
            "inputs": [
                "sdf,smi,mol,mol2,cml,inchi"
            ],
            "outputs": []
        },
        "description": " a set of molecules from a file",
        "help": "\n\n\n\n.. class:: infomark\n\n**What this tool does**\n\nFilters a library of compounds based on user-defined physico-chemical parameters or predefined options (e.g. Ro5, lead-like properties, etc.). Multiple parameters can be selected for more specific queries.\n\n-----\n\n.. class:: warningmark\n\n**Hint**\n\n| If your input file is in SDF format you can use the *Compute physico-chemical properties* tool to precalulate the properties and use the filter on that precomputed dataset. It should be faster and can be reused but it's bigger than a SMILES file.\n|\n| For exact matches please use the target value for both minimum and maximum parameters (e.g. a selection of exactly 4 rotatable bonds can be performed by selecting 4 as minimum and maximum value).\n|\n| Selecting the same property multiple times with different parameters will result in querying the largest overlapping subset of values for the parameter (e.g. a selection of between 0 and 3 rotatable bonds plus a selection between 2 and 4 will result in a query for compounds between 2 and 3 rotatable bonds).\n\n-----\n\n.. class:: infomark\n\n**Definition of the pre-defined filtering rules**\n\n  **# Lipinski's Rule of Five:**\n    =< 5 Hydrogen-bond donor groups\n\n    =< 10 Hydrogen-bond acceptor groups\n\n    =< 500 Molecular weight\n\n    =< 5 octanol/water partition coefficient (log P)\n\n  **# Lead Like properties** (Teague, Davis, Leeson, Oprea, Angew Chem Int Ed Engl. 1999 Dec 16;38(24):3743-3748):\n    =< 7 rotatable bonds\n\n    =< 350 Molecular weight\n\n    =< 3.5 octanol/water partition coefficient (log P)\n\n  **# Drug Like properties** (Lipinski, J Pharmacol Toxicol Methods. 2000 Jul-Aug;44(1):235-49):\n    =< 10 Hydrogen-bond acceptor groups\n\n    =< 8 rotatable bonds\n\n    150 =< Molecular weight =< 500\n\n    =< 150 Polar Surface Area\n\n    =< 5 octanol/water partition coefficient (log P)\n\n  **# Fragment Like properties** (Carr RA, Congreve M, Murray CW, Rees DC, Drug Discov Today. 2005 Jul 15;10(14):987):\n    =< 5 rotatable bonds\n\n    =< 250 Molecular weight\n\n    =< 2.5 octanol/water partition coefficient (log P)\n\n-----\n\n.. class:: infomark\n\n**Input**\n\n| - `SD-Format`_\n| - `SMILES Format`_\n\n.. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file\n.. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification\n\n-----\n\n.. class:: infomark\n\n**Output**\n\n| SDF formatted coordinates of the molecules, with selected properties stored as meta-data for each compound.\n|\n| SMILES, InChI or mol2 formatted files containing the 1D strings or 3D coordinates of each compound.\n\n\n\n\n    ",
        "id": "openbabel_filter",
        "interpreter": null,
        "language": null,
        "name": "Filter",
        "readme": false,
        "tests": true,
        "version": "2.4.1.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btt593"
            }
        ],
        "code_file": null,
        "command": "\n\n    pear\n        #if str( $library.type ) == \"paired\":\n            -f \"$library.forward\"\n            -r \"$library.reverse\"\n            #if $library.forward.is_of_type( 'fastqillumina' ):\n                --phred-base 64\n            #else:\n                --phred-base 33\n            #end if\n        #else\n            ## prepare collection\n            -f \"$library.input_collection.forward\"\n            -r \"$library.input_collection.reverse\"\n            #if $library.input_collection.forward.is_of_type( 'fastqillumina' ):\n                --phred-base 64\n            #else:\n                --phred-base 33\n            #end if\n        #end if\n\n        --output pear\n        --p-value $pvalue\n        --min-overlap $min_overlap\n        #if int($max_assembly_length) > 0:\n            --max-asm-length $max_assembly_length\n        #end if\n        --min-asm-length $min_assembly_length\n        --min-trim-length $min_trim_length\n        --quality-theshold $quality_threshold\n        --max-uncalled-base $max_uncalled_base\n        --test-method $test_method\n        --empirical-freqs $empirical_freqs\n        -j \"\\${GALAXY_SLOTS:-8}\"\n        --score-method $score_method\n        --cap $cap\n        $nbase\n\n    ",
        "dataFormats": {
            "inputs": [
                "fastqillumina,fastqsanger",
                "fastqillumina,fastqsanger",
                "fastqillumina,fastqsanger"
            ],
            "outputs": [
                "input",
                "input",
                "input",
                "input"
            ]
        },
        "description": "Paired-End read merger",
        "help": "\n\n\n**What it does**\n\nPEAR_ is an ultrafast, memory-efficient and highly accurate pair-end read merger.\nPEAR evaluates all possible paired-end read overlaps and without requiring the target fragment\nsize as input. In addition, it implements a statistical test for minimizing false-positive results.\nTogether with a highly optimized implementation, it can merge millions of paired end reads within a couple of minutes\non a standard desktop computer.\n\nFor more information please look at the documentation_ and `github repository`_.\n\n.. _PEAR: https://sco.h-its.org/exelixis/web/software/pear/\n.. _documentation: https://sco.h-its.org/exelixis/web/software/pear/doc.html\n\nPlease note that PEAR is released under the\n[CC Attribution-NonCommercial-ShareAlike](https://creativecommons.org/licenses/by-nc-sa/3.0/) license\nand that commercial partners should obtain a license.\n\n\n  ",
        "id": "iuc_pear",
        "interpreter": null,
        "language": null,
        "name": "Pear",
        "readme": false,
        "tests": true,
        "version": "0.9.6.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n  ln -sf $input input_phy;\n \n  phyml -i input_phy\n\t $format\n\n\t -n $nb_data_set\n\t -d $type.type_of_seq\n\n\t#if ($type.type_of_seq == \"nt\"):\n\t\t-t $type.tstv\n\t#end if\n\t\n\t-m $type.model\n\n\t#if (str($bs.bootstrap) in ['0','-1','-2','-4']):\n                    -b $bs.bootstrap\n        #elif (str($bs.bootstrap) == '1'):\n                    -b $bs.bootstrap.replicate_num\n        #end if\n\n\t-f $character_freq\n\t-v $prop_invar\n\t-c $nb_subst_cat\n\t-a $gamma\n\t-s $search_move\n\t-o $optimisation_parameter\n\n\t> ${stdout_output};\n  \n  ",
        "dataFormats": {
            "inputs": [
                "txt"
            ],
            "outputs": [
                "txt",
                "txt",
                "txt"
            ]
        },
        "description": " Phylogeny software based on the maximum-likelihood principle ",
        "help": null,
        "id": "phyml",
        "interpreter": null,
        "language": null,
        "name": "PhyML",
        "readme": false,
        "tests": false,
        "version": "0.1.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/sysbio/syq010"
            }
        ],
        "code_file": null,
        "command": "\n        \r\n          ##PhyML outputs are based on input path and we need to create outputs in working_dir.\r\n          ln -sf $input '${input.name}';\r\n          #if ($usetree.inputTree == \"true\"):\r\n          ln -sf ${$usetree.userInputTree} ${$usetree.userInputTree.name};\r\n          #end if\r\n\r\n          phyml --input '${input.name}'\r\n                ${phylip_format}\r\n                --datatype ${seq.type_of_seq}\r\n                --multiple ${nb_data_set}\r\n                \r\n                #if (str($support_condition.branchSupport) in ['0','-1','-2','-4','-5']):\r\n                    --bootstrap ${support_condition.branchSupport}\r\n                #else:\r\n                    --bootstrap ${support_condition.branchSupport.replicate}\r\n                #end if\r\n                    \r\n                #if ($seq.type_of_seq == \"nt\"):\r\n                    -t ${seq.tstv}\r\n                #end if\r\n            \r\n                --model ${seq.model}\r\n                -f ${equi_freq}\r\n                --pinv ${prop_invar}\r\n                \r\n                --nclasses ${nbSubstCat}\r\n                \r\n                #if (str($nbSubstCat) != \"1\"):\r\n                    --alpha ${gamma}\r\n                #end if\r\n\r\n                --search ${move}\r\n                -o ${optimisationTopology}\r\n                \r\n                #if ($usetree.inputTree == \"true\"):\r\n                    --inputtree ${$usetree.userInputTree.name}\r\n                #end if\r\n                \r\n                #if ($numStartSeed != \"0\"):\r\n                    --r_seed ${numStartSeed}\r\n                #end if\r\n                \r\n                --no_memory_check > ${output_stdout};\r\n                grep 'failed' ${output_stdout} >&2;\r\n        \n    ",
        "dataFormats": {
            "inputs": [
                "phylip, phy"
            ],
            "outputs": [
                "txt",
                "txt",
                "txt"
            ]
        },
        "description": "Phylogeny software based on the maximum-likelihood",
        "help": "\n     \r\n.. class:: infomark\r\n\r\n**PhyML version 3.1, 2013**\r\n\r\n-----\r\n\r\n#########\r\nPhyML 3.1\r\n#########\r\n\r\nA simple, fast, and accurate algorithm to estimate\r\nlarge phylogenies by maximum likelihood''\r\n\r\n==========\r\n Overview:\r\n==========\r\n\r\nPhyML is a phylogeny software based on the maximum-likelihood principle. Early PhyML versions used a fast algorithm to perform Nearest Neighbor Interchanges (NNIs), in order to improve a reasonable starting tree topology. Since the original publication (Guindon and Gascuel 2003), PhyML has been widely used due to its simplicity and a fair accuracy/speed compromise. In the mean time research around PhyML has continued. \r\n\r\nWe designed an efficient algorithm to search the tree space using Subtree Pruning and Regrafting (SPR) topological moves (Hordijk and Gascuel 2005), and proposed a fast branch test based on an approximate likelihood ratio test (Anisimova and Gascuel 2006). However, these novelties were not included in the official version of PhyML, and we found that improvements were still needed in order to make them effective in some practical cases. PhyML 3.0 achieves this task. \r\n\r\nIt implements new algorithms to search the space of tree topologies with user-defined intensity. A non-parametric, Shimodaira-Hasegawa-like branch test is also available. The program provides a number of new evolutionary models and its interface was entirely re-designed. We tested PhyML 3.0 on a large collection of real data sets to ensure that the new version is stable, ready-to-use and still reasonably fast and accurate. \r\n\r\n-----\r\n\r\n\r\nFor further informations, please visite the PhyML_ website.\r\n\r\n.. _PhyML: http://www.atgc-montpellier.fr/phyml/\r\n\r\nPlease cite this paper if you use this software in your publications.\r\n\r\n\r\n-----\r\n\r\nOPTIONS\r\n=======\r\n\r\n    \r\n  -i, --input seq_file_name\r\n      seq_file_name is the name of the nucleotide or amino-acid sequence file in PHYLIP format.\r\n  \r\n  -d, --datatype data_type\r\n      data_type is 'nt' for nucleotide (default), 'aa' for amino-acid sequences, or 'generic',\r\n      (use NEXUS file format and the 'symbols' parameter here).\r\n  \r\n  -q, --sequential\r\n      Changes interleaved format (default) to sequential format.\r\n  \r\n  \r\n  -n, --multiple nb_data_sets\r\n      nb_data_sets is an integer corresponding to the number of data sets to analyse.\r\n  \r\n  -p, --pars\r\n      Use a minimum parsimony starting tree. This option is taken into account when the '-u' option\r\n      is absent and when tree topology modifications are to be done.\r\n  \r\n  -b, --bootstrap int\r\n     \r\n      - *int > 0*: int is the number of bootstrap replicates.\r\n      - *int = 0*: neither approximate likelihood ratio test nor bootstrap values are computed.\r\n      - *int = -1*: approximate likelihood ratio test returning aLRT statistics.\r\n      - *int = -2*: approximate likelihood ratio test returning Chi2-based parametric branch supports.\r\n      - *int = -4*: (default) SH-like branch supports alone.\r\n      - *int = -5*: approximate Bayes branch supports.\r\n  -m, --model model\r\n     model : substitution model name.\r\n\r\n     - Nucleotide-based models : HKY85 (default) | JC69 | K80 | F81 | F84 | TN93 | GTR | custom(*)\r\n\r\n      (*) : *for the custom option, a string of six digits identifies the model. For instance, 00000 corresponds to F81, JC69 provided the distribution of nucleotide frequencies is uniform). 012345 corresponds to GTR. This option can be used for encoding any model that is a nested within GTR.*\r\n  \r\n     * Amino-acid based models : LG (default) | WAG | JTT | MtREV | Dayhoff | DCMut | RtREV | CpREV | VT\r\n       Blosum62 | MtMam | MtArt | HIVw |  HIVb | custom(**)\r\n\r\n      (**) **--aa_rate_file** *filename*  file that provides the amino acid substitution rate matrix in PAML format.\r\n      It is compulsory to use this option when analysing amino acid sequences with the \"custom\" model.\r\n      \r\n  -f frequency   \r\n   - *\"e\"*: the character frequencies are determined as follows : \r\n      - Nucleotide sequences: (Empirical) the equilibrium base frequencies are estimated by counting the occurence of the different bases in the alignment.\r\n      - Amino-acid sequences: (Empirical) the equilibrium amino-acid frequencies are estimated by counting the occurence of the different amino-acids in the alignment.\r\n   - *\"m\"* : the character frequencies are determined as follows : \r\n      - Nucleotide sequences: (ML) the equilibrium base frequencies are estimated using maximum likelihood \r\n      - Amino-acid sequences: (Model) the equilibrium amino-acid frequencies are estimated using   the frequencies defined by the substitution model.\r\n   - *\"fA,fC,fG,fT\"*: only valid for nucleotide-based models. fA, fC, fG and fT are floating numbers that correspond to the frequencies of A, C, G and T respectively *(WARNING: do not use any blank space between your values of nucleotide frequencies, only commas!)*.\r\n\r\n  -t  ts_tv_ratio\r\n      ts_tv_ratio : transition/transversion ratio. DNA sequences only.\r\n      Can be a fixed positive value (ex:4.0) or e to get the maximum likelihood estimate.\r\n      \r\n  -v, --pinv prop_invar\r\n      prop_invar : proportion of invariable sites.\r\n      Can be a fixed value in the [0,1] range or e to get the maximum likelihood estimate.\r\n\r\n  -c, --nclasses nb_subst_cat\r\n      nb_subst_cat : number of relative substitution rate categories. Default : nb_subst_cat=4.\r\n      Must be a positive integer.\r\n  \r\n  -a, --alpha gamma\r\n      gamma : distribution of the gamma distribution shape parameter.\r\n      Can be a fixed positive value or e to get the maximum likelihood estimate.\r\n  \r\n  -s, --search move\r\n      Tree topology search operation option.\r\n      Can be either NNI (default, fast) or SPR (a bit slower than NNI) or BEST (best of NNI and SPR search).\r\n  \r\n  -u, --inputtree user_tree_file\r\n      user_tree_file : starting tree filename. The tree must be in Newick format.\r\n  \r\n  -o params\r\n      This option focuses on specific parameter optimisation.\r\n\r\n      - *tlr* : tree topology (t), branch length (l) and rate parameters (r) are optimised.\r\n      - *tl*  : tree topology and branch length are optimised.\r\n      - *r*  : branch length and rate parameters are optimised.\r\n      - *l*   : branch length are optimised.\r\n      - *r*  : rate parameters are optimised.\r\n      - *n*   : no parameter is optimised.\r\n  --rand_start  This option sets the initial tree to random.\r\n      It is only valid if SPR searches are to be performed.\r\n  --n_rand_starts integer  \r\n      Number of initial random trees to be used.\r\n      It is only valid if SPR searches are to be performed.\r\n  --r_seed integer  \r\n      Number of the seed used to initiate the random number generator.\r\n  --print_site_lnl\r\n      Print the likelihood for each site in file *\\*_phyml_lk.txt*\r\n  --print_trace\r\n      Print each phylogeny explored during the tree search process in file *\\*_phyml_trace.txt*.\r\n  --run_id ID_string  \r\n      Append the string ID_string at the end of each PhyML output file.\r\n      This option may be useful when running simulations involving PhyML. \r\n  --quiet  \r\n      No interactive question (for running in batch mode) and quiet output.\r\n  --no_memory_check  \r\n      No interactive question for memory usage (for running in batch mode). Normal ouput otherwise.\r\n  --alias_subpatt  \r\n      Site aliasing is generalized at the subtree level. Sometimes lead to faster calculations.\r\n      See Kosakovsky Pond SL, Muse SV, Sytematic Biology (2004) for an example.\r\n  --boot_progress_display   num (default=20)\r\n      num is the frequency at which the bootstrap progress bar will be updated.\r\n      Must be an integer.\r\n \n    ",
        "id": "phyml",
        "interpreter": null,
        "language": null,
        "name": "PhyML",
        "readme": false,
        "tests": true,
        "version": "3.1"
    },
    {
        "ciation": [],
        "code_file": null,
        "command": "\n    \n  #if $inputtype.filetype == \"file_all\": \n  Rscript --vanilla $__tool_directory__/enrichment_v3.R \n  --inputtype tabfile \n  --input '$inputtype.genelist' \n  --ontology '$ontocat' \n  --option '$option' \n  --threshold '$threshold' \n  --correction '$correction' \n  --textoutput '$condtext.textoutput' \n  --barplotoutput '$condbar.barplotoutput' \n  --dotplotoutput '$conddot.dotplotoutput' \n  --column '$inputtype.column' \n  --geneuniverse '$geneuniverse' \n  --header '$inputtype.header'\n  #end if\n    \n    \n  #if $inputtype.filetype == \"copy_paste\": \n  Rscript --vanilla $__tool_directory__/enrichment_v3.R \n  --inputtype copypaste \n  --input '$inputtype.genelist' \n  --ontology '$ontocat' \n  --option '$option' \n  --threshold '$threshold' \n  --correction '$correction' \n  --textoutput '$condtext.textoutput' \n  --barplotoutput '$condbar.barplotoutput' \n  --dotplotoutput '$conddot.dotplotoutput' \n  --column c1 \n  --geneuniverse '$geneuniverse' \n  --header None\n  #end if\n\n\n\n    ",
        "dataFormats": {
            "inputs": [
                "txt,tabular"
            ],
            "outputs": [
                "tabular",
                "png",
                "png"
            ]
        },
        "description": "\n    Enrichment analysis for Gene Ontology\n    ",
        "help": "\n      \n      \n**Galaxy component based on R package topGO.** \n\n**Input required**\n\nThis component works with Ensembl gene ids (e.g : ENSG0000013618). You can\ncopy/paste these identifiers or supply a tabular file (.csv, .tsv, .txt, .tab)\nwhere there are contained. \n\n**Principle**\n\nThis component provides the GO terms representativity of a gene list in one ontology category (Biological Process \"BP\", Cellular Component \"CC\", Molecular Function \"MF\"). This representativity is evaluated in comparison to the background list of all human genes associated associated with GO terms of the chosen category (BP,CC,MF). This background is given by the R package \"org.Hs.eg.db\", which is a genome wide association package for **human**.\n\n**Output**\n\nThree kind of outputs are available : a textual output, a barplot output and\na dotplot output. \n\n*Textual output* :\nThe text output lists all the GO-terms that were found significant under the specified threshold.    \n\n\nThe different fields are as follow :\n\n- Annotated : number of genes in org.Hs.eg.db which are annotated with the GO-term.\n\n- Significant : number of genes belonging to your input which are annotated with the GO-term. \n\n- Expected : show an estimate of the number of genes a node of size Annotated would have if the significant genes were to be randomly selected from the gene universe.  \n\n- pvalues : pvalue obtained after the test \n\n- ( qvalues  : additional column with adjusted pvalues ) \n\n \n**Tests**\n\ntopGO provides a classic fisher test for evaluating if some GO terms are over-represented in your gene list, but other options are also provided (elim, weight01,parentchild). For the merits of each option and their algorithmic descriptions, please refer to topGO manual : \nhttps://bioconductor.org/packages/release/bioc/vignettes/topGO/inst/doc/topGO.pdf\n\n**Multiple testing corrections**\n    \nFurthermore, the following corrections for multiple testing can also be applied : \n\n- holm\n\n- hochberg\n\n- hommel\n\n- bonferroni\n\n- BH\n\n- BY\n\n- fdr\n\n-----\n\n.. class:: infomark\n\n**Authors**\n\nAlexa A and Rahnenfuhrer J (2016). topGO: Enrichment Analysis for Gene Ontology. R package version 2.30.0.\n\n**Galaxy integration**\n\nLisa Peru, T.P. Lien Nguyen, Florence Combes, Yves Vandenbrouck CEA, INSERM, CNRS, Grenoble-Alpes University, BIG Institute, FR\n\nSandra D\u00e9rozier, Olivier Ru\u00e9, Christophe Caron, Valentin Loux INRA, Paris-Saclay University, MAIAGE Unit, Migale Bioinformatics platform\n\nThis work has been partially funded through the French National Agency for Research (ANR) IFB project.\n\nContact support@proteore.org for any questions or concerns about the Galaxy implementation of this tool.\n\n",
        "id": "topGO",
        "interpreter": null,
        "language": null,
        "name": "topGO",
        "readme": true,
        "tests": true,
        "version": "2018.09.21"
    },
    {
        "ciation": [
            {
                "citation": "@ARTICLE{a1,\n            author = {Alexey Gurevich, Vladislav Saveliev, Nikolay Vyahhi, Glenn Tesler},\n            title = {QUAST: quality assessment tool for genomce assemblies, Bioinformatics (2013) 29 (8): 1072-1075}\n        }"
            },
            {
                "citation": "@misc{quast41,\n            title = {{Quast} v4.1},\n            howpublished = {http://bioinf.spbau.ru/quast},\n            note = {Released May 2016}}\n        }"
            }
        ],
        "code_file": null,
        "command": "\n    \n#import re\n\n#if str($in.custom) == 'false'\n    #set $labels = ','.join( [re.sub('[^\\w\\-_]', '_', str($x.element_identifier)) for $x in $in.inputs ])\n#else\n    #set $labels = []\n    #for $x in $in.inputs\n        #if str($x.labels) != ''\n            #silent $labels.append(re.sub('[^\\w\\-_]', '_', str($x.labels)))\n        #else\n            #silent $labels.append(re.sub('[^\\w\\-_]', '_', str($x.input.element_identifier)))\n        #end if\n    #end for\n    #set $labels = ','.join($labels)\n#end if\n\n#if $assembly.type == 'metagenome' and $assembly.ref.origin == 'list'\n    #set $temp_ref_list_fp = 'temp_ref_list_fp'\n    #set $temp_ref_list_f = open($temp_ref_list_fp, 'w')\n    #silent $temp_ref_list_f.write('\\n'.join([x.strip() for x in $assembly.ref.references_list.split(',')]))\n    #silent $temp_ref_list_f.close()\n#end if\n\n#if $assembly.type == 'genome'\nquast\n#else\nmetaquast\n#end if\n    -o outputdir\n\n#if $assembly.type == 'genome'\n    #if $assembly.ref.use_ref == 'true'\n    -r '$assembly.ref.r'\n        #if $assembly.ref.features\n    --features '$assembly.ref.features'\n        #end if\n        #if $assembly.ref.operons\n    --operons '$assembly.ref.operons'\n        #end if\n    #else if $assembly.ref.est_ref_size\n    --est-ref-size $assembly.ref.est_ref_size\n    #end if\n    $assembly.orga_type\n#else\n    #if $assembly.ref.origin == 'history'\n    -r '$assembly.ref.r'\n    #else if $assembly.ref.origin == 'list'\n    --references-list '$temp_ref_list_fp'\n    #else if $assembly.ref.origin == 'silva'\n    --test-no-ref\n    --max-ref-num '$assembly.ref.max_ref_num'\n    #end if\n#end if\n    --min-contig $min_contig\n    --threads \\${GALAXY_SLOTS:-4}\n    $split_scaffolds\n    --labels $labels\n    $large\n    $k_mer.k_mer_stats\n#if str($k_mer.k_mer_stats) != ''\n    --k-mer-size $k_mer.k_mer_size\n#end if\n    $circos\n#if str($genes.gene_finding.tool) != 'none'\n    $genes.gene_finding.tool\n    #if $genes.gene_finding.tool == '--gene_finding' or $genes.gene_finding.tool == '--glimmer'\n        #set $gene_threshold = ','.join([x.strip() for x in str($genes.gene_finding.gene_thresholds).split(',')])\n        --gene-thresholds '$gene_threshold'\n    #end if\n#end if\n    $genes.rna_finding\n    $genes.conserved_genes_finding\n    $al.use_all_alignments\n    --min-alignment $al.min_alignment\n    --min-identity $al.min_identity\n    --ambiguity-usage '$al.ambiguity_usage'\n    $al.fragmented\n    #set $contig_thresholds = ','.join([x.strip() for x in str($contig_thresholds).split(',')])\n    --contig-thresholds '$contig_thresholds'\n    $strict_NA\n    --extensive-mis-size $extensive_mis_size\n    --scaffold-gap-max-size $scaffold_gap_max_size\n    --unaligned-part-size $unaligned_part_size\n    $skip_unaligned_mis_contigs\n\n#if str($in.custom) == 'false'\n    #for $k in $in.inputs\n    '$k'\n    #end for\n#else\n    #for $k in $in.inputs\n    '$k.input'\n    #end for\n#end if\n\n&& mkdir -p '$report_html.files_path'\n&& cp outputdir/*.html '$report_html.files_path'\n#if ($assembly.type == 'genome' and $assembly.ref.use_ref) or ($assembly.type == 'metagenome' and $assembly.ref.origin != 'none')\n    && cp -R outputdir/icarus_viewers '$report_html.files_path'\n#end if\n    \n    ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fasta",
                "fasta",
                "gff, gff3, bed",
                "gff, gff3, bed",
                "fasta"
            ],
            "outputs": [
                "tabular",
                "html",
                "pdf",
                "txt",
                "tabular",
                "tabular",
                "tabular"
            ]
        },
        "description": "Genome assembly Quality",
        "help": "\n\n**What it does**\n\nQUAST = QUality ASsessment Tool. The tool evaluates genome assemblies by computing various metrics.\n\nIf you have one or multiple genome assemblies, you can assess their quality with Quast. It works with or without reference genome. If you are new to Quast, start by reading its `manual page <http://quast.bioinf.spbau.ru/manual.html>`_.\n\n**Using Quast without reference**\n\nWithout reference Quast can calculate a number of assembly related-metrics but cannot provide any information about potential misassemblies, inversions, translocations, etc. Suppose you have three assemblies produced by Unicycler corresponding to three different antibiotic treatments *car*, *pit*, and *cef* (these stand for carbenicillin, piperacillin, and cefsulodin, respectively). Evaluating them without reference will produce the following Quast outputs:\n \n * Quast report in HTML format\n * `Contig viewer <http://quast.bioinf.spbau.ru/manual.html#sec3.4>`_ (an HTML file)\n * `Quast report <http://quast.bioinf.spbau.ru/manual.html#sec3.1.1>`_ in Tab-delimited format\n * Quast log (a file technical information about Quast tool execution)\n\nThe **tab delimited Quast report** will contain the following information::\n\n Assembly                  pit_fna cef_fna car_fna\n # contigs (>= 0 bp)           100      91      94\n # contigs (>= 1000 bp)         62      58      61\n Total length (>= 0 bp)    6480635 6481216 6480271\n Total length (>= 1000 bp) 6466917 6468946 6467103\n # contigs                      71      66      70\n Largest contig             848753  848766  662053\n Total length              6473173 6474698 6473810\n GC (%)                      66.33   66.33   66.33\n N50                        270269  289027  254671\n N75                        136321  136321  146521\n L50                             7       7       8\n L75                            15      15      16\n # N's per 100 kbp            0.00    0.00    0.00\n\nwhere values are defined as specified in `Quast manual <http://quast.bioinf.spbau.ru/manual.html#sec3.1.1>`_\n\n**Quast report in HTML format** contains graphs in addition to the above metrics, while **Contig viewer** draws contigs ordered from longest to shortest. This ordering is suitable for comparing only largest contigs or number of contigs longer than a specific threshold. The viewer shows N50 and N75 with color and textual indication. If the reference genome is available or at least approximate genome length is known (see `--est-ref-size`), NG50 and NG75 are also shown. You can also tone down contigs shorter than a specified threshold using Icarus control panel:\n\n.. image:: $PATH_TO_IMAGES/contig_view_noR.png\n   :width: 558\n   :height: 412\n\nAlso see `Plot description <http://quast.bioinf.spbau.ru/manual.html#sec3.2>`_ section of the manual. \n\n**Using Quast with reference**\n\nCar, pit, and cef are in fact assemblies of *Pseudomonas aeruginosa* UCBPP-PA14, so we can use its genome as a reference (by supplying a Fasta file containing *P. aeruginosa* pa14 genome to **Reference genome** input box). The following outputs will be produced (note the alignment viewer):\n\n * Quast report in HTML format\n * `Contig viewer <http://quast.bioinf.spbau.ru/manual.html#sec3.4>`_ (an HTML file)\n * `Alignment viewer <http://quast.bioinf.spbau.ru/manual.html#sec3.4>`_ (an HTML file)\n * `Quast report <http://quast.bioinf.spbau.ru/manual.html#sec3.1.1>`_ in Tab-delimited format\n * Summary of `misassemblies <http://quast.bioinf.spbau.ru/manual.html#sec3.1.2>`_\n * Summary of `unaligned contigs <http://quast.bioinf.spbau.ru/manual.html#sec3.1.3>`_\n * Quast log (a file technical information about Quast tool execution)\n\nWith the reference Quast produces a much more comprehensive set of results::\n\n Assembly                  pit_fna cef_fna car_fna\n # contigs (>= 0 bp)           100      91      94\n # contigs (>= 1000 bp)         62      58      61\n Total length (>= 0 bp)    6480635 6481216 6480271\n Total length (>= 1000 bp) 6466917 6468946 6467103\n # contigs                      71      66      70\n Largest contig             848753  848766  662053\n Total length              6473173 6474698 6473810\n Reference length          6537648 6537648 6537648\n GC (%)                      66.33   66.33   66.33\n Reference GC (%)            66.29   66.29   66.29\n N50                        270269  289027  254671\n NG50                       270269  289027  254671\n N75                        136321  136321  146521\n NG75                       136321  136321  136321\n L50                             7       7       8\n LG50                            7       7       8\n L75                            15      15      16\n LG75                           15      15      17\n # misassemblies                 0       0       0\n # misassembled contigs          0       0       0\n Misassembled contigs length     0       0       0\n # local misassemblies           1       1       2\n # unaligned mis. contigs        0       0       0\n # unaligned contigs         0 + 0   0 + 0   0 + 0\n                              part    part    part\n Unaligned length                0       0       0\n Genome fraction (%)        99.015  99.038  99.025\n Duplication ratio           1.000   1.000   1.000\n # N's per 100 kbp            0.00    0.00    0.00\n # mismatches per 100 kbp     3.82    3.63    3.49\n # indels per 100 kbp         1.19    1.13    1.13\n Largest alignment          848753  848766  662053\n Total aligned length      6473163 6474660 6473792\n NA50                       270269  289027  254671\n NGA50                      270269  289027  254671\n NA75                       136321  136321  146521\n NGA75                      136321  136321  136321\n LA50                            7       7       8\n LGA50                           7       7       8\n LA75                           15      15      16\n LGA75                          15      15      17 \n\nwhere, again, values are defined as specified in `Quast manual <http://quast.bioinf.spbau.ru/manual.html#sec3.1.1>`_. You can see that this report includes a variety of data that can only be computer against a reference assembly. \n\n Using reference also produces an **Alignment viewer**:\n\n.. image:: $PATH_TO_IMAGES/Align_view.png\n   :width: 515\n   :height: 395\n\nAlignment viewer highlights regions of interest as, in this case, missassemblies that can potentially point to genome rearrangements (see more `here <http://quast.bioinf.spbau.ru/manual.html#sec3.4>`_).\n\n    \n    ",
        "id": "quast",
        "interpreter": null,
        "language": null,
        "name": "Quast",
        "readme": false,
        "tests": true,
        "version": "@TOOL_VERSION@+galaxy0"
    },
    {
        "ciation": [
            {
                "citation": "@ARTICLE{a1,\n\t\tauthor = {Alexey Gurevich, Vladislav Saveliev, Nikolay Vyahhi, Glenn Tesler},\n\t\ttitle = {QUAST: quality assessment tool for genomce assemblies, Bioinformatics (2013) 29 (8): 1072-1075}\n\t\t}"
            }
        ],
        "code_file": null,
        "command": "\t\t\n\t\tquast.py \n\t\t -o outputdir\n\t\t\n\t\t#if $gene_selection == \"eukaryote\" :\n\t\t\t--eukaryote\n\t\t#end if\n\t\t\n\t\t#if $gene_selection == \"Metagenes\" :\n\t\t\t--meta\n                #end if\n\t\t\n\t\t--min-contig $min_contig\n\t\t\n\t\t\n\t\t#for $i in $files:\n\t\t\t$i.input\n\t\t#end for\n\t\t;\n\t\tcp outputdir/report.txt $quast_out;\n\t",
        "dataFormats": {
            "inputs": [
                "fasta"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": "QUAST (Quality ASsessment Tool) evaluates genome assemblies.",
        "help": "\n**Description**\n\t\t\nGalaxy tool wrapper for the QUAST tool. Quast stands for QUality ASsessment Tool. It evalutes genome assemblies by computing various metrics. \nFor more information regarding the settings of the tool, please visit the QUAST 3.2 Manual on http://quast.bioinf.spbau.ru/manual.html\n\n-----\n\n**Inputs and Outputs**\n\t\t\n- Input:\n\t+ The tool accepts assemblies and references in FASTA format. \n\n- Output:\n\t+ An assessment summary in plain text format\n\n\n-----\n\n**Tool Information**\n\n- QUAST Tool v3.2: \n\t+ Release Date: November 2015\n\t+ URL: http://bioinf.spbau.ru/quast\n\t+ Source URL: https://downloads.sourceforge.net/project/quast/quast-3.2.tar.gz\n\n-----\n\t\t\n**QUAST Galaxy Wrapper Author and Contact Information**\n\n+ Jacob Jablonski\n+ AAFC-MBB Team\n+ Email: mbb@agr.gc.ca\n+ Agriculture and Agri-foods Canada, Ottawa, ON, Canada\n\n\t",
        "id": "quast",
        "interpreter": null,
        "language": null,
        "name": "Quast",
        "readme": false,
        "tests": true,
        "version": "1.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1371/journal.pcbi.1000502"
            }
        ],
        "code_file": null,
        "command": "\n\n    \n## UNIMPLEMENTED    \n## [SEEDEXTENSIONPARAMS]           \n## -e, --extensionscore <n>        score of a match during extension (default:2)\n## -n, --extensionpenalty <n>      penalty for a mismatch during extension (default:4)\n## -X, --dropoff <n>               dropoff parameter for extension (default:8)\n##  --showalign\n\n## prepare segemehl index if no reference genome is supplied\n        #if $refGenomeSource.genomeSource == \"history\":\n            mkdir ./temp_index/ &&\n            #set $temp_index = './temp_index/temp.idx'\n            segemehl.x -x $temp_index -d $refGenomeSource.own_reference_genome &&\n        #else:\n            #set $temp_index = $refGenomeSource.index.fields.index_path\n        #end if\n        \n        ## execute segemehl\n            segemehl.x\n        \n        ## number of threads\n            -t \"\\${GALAXY_SLOTS:-12}\"\n        \n        #if $refGenomeSource.genomeSource == \"history\":\n            -d $refGenomeSource.own_reference_genome\n        #else:\n            -d ${refGenomeSource.index.fields.db_path}\n        #end if\n        \n        -i $temp_index\n        \n        ## check for single/pair-end\n        #if str( $library.type ) == \"single\":\n            ## prepare inputs\n            -q ${library.input_query}\n        #else       \n            -q ${mate_pair.first_strand_query}\n            -p ${mate_pair.second_strand_query}\n            -I ${library.maxinsertsize}\n        #end if\n        #if str( $bisulfite ) != \"0\":\n            -F $bisulfite\n        #end if\n        -m $minsize\n        -A $accuracy\n        -H $hitstrategy\n        #if str( $prime5 ).strip():\n            -P \"$prime5\"\n        #end if\n        #if str( $prime3 ).strip():\n            -Q \"$prime3\"\n        #end if\n        -R $clipacc\n        $polyA\n        $autoclip\n        $hardclip\n        $order\n        #if $maxout:\n            --maxout $maxout\n        #end if\n        #if str( $splitreads.splits ) == \"splits\":\n            --splits\n            --minsplicecover $splitreads.minsplicecover\n            --minfragscore $splitreads.minfragscore\n            --minfraglen $splitreads.minfraglen\n\t    --splicescorescale $splitreads.splicescorescale\n\t    --maxsplitevalue $splitreads.maxsplitevalue\n        #end if\n        -M $maxinterval\n        -E $evalue\n        -D $differences\n        -J $jump\n        -s\n        -o '$segemehl_out'\n        #if str( $nomatchfilename ) == 'yes':\n           -u '$segemehl_outunmatched'\n        #end if\n    \n  ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fastqsanger,fastqillumina,fastq,fasta",
                "fastqsanger,fastqillumina,fastq,fasta",
                "fastqsanger,fastqillumina,fastq,fasta"
            ],
            "outputs": [
                "sam",
                "fastq"
            ]
        },
        "description": "short read mapping with gaps",
        "help": "\n    \n\n.. class:: infomark\n\n**What it does**\n\nSegemehl_ is a short read mapper with gaps.\n\nSegemehl_ is a software to map short sequencer reads to reference genomes.\nUnlike other methods, segemehl is able to detect not only mismatches but also insertions and deletions.\nFurthermore, segemehl is not limited to a specific read length and is able to mapprimer- or polyadenylation contaminated reads correctly.\nsegemehl implements a matching strategy based on enhanced suffix arrays (ESA). Segemehl_ allows bisulfite sequencing mapping and split read mapping.\n\n.. _Segemehl: http://www.bioinf.uni-leipzig.de/Software/segemehl/\n\n\n    \n  ",
        "id": "segemehl",
        "interpreter": null,
        "language": null,
        "name": "segemehl",
        "readme": false,
        "tests": true,
        "version": "0.2.0.4"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": " ./match $motif $seq $output $nmismatch $rc $bed > $log",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fasta"
            ],
            "outputs": [
                "log",
                "bed"
            ]
        },
        "description": "find short motif occurrences",
        "help": "\n\n**What it does**\n\nThis tool searches occurrences of a short nucleotide seuqences (allowing mismatches) in a set of longer sequences.\n\nExample motif file::\n\n    >motif1\n    CAGGTAAGT\n    >motif2\n    GTTTGGGGGCC\n    \nExample sequence file::\n\n    >hg18_chr6_122208322_122209078_+\n    CGTCGTAGCTACTAGCTACGTACGTACGTAGCTAGCATGCATGCTACGTA\n    CGTAGCTAGCTAAAAAAAAAAAAAAACTGCGGCTAGCTAGCTAGCTACGT\n    CGATCGTAGCTAC...\n    >hg18_chr6_1208322_122209023_+\n    CGATGCTAGCTAGCTAGCTACGTAGCTAGCTAGTCGATGCTAGCTAGCTA     \n    ATGCTAGCTAGC....\n    \nOutput (bed)::\n\n    chr11\t72790893\t72790902\tACTTAACTG\t1\t-\tantisense\t5ss,G4T:CAGTTAAGT-rc\thg18_chr11_72790846_72791902_+\t47\n    chr11\t72791880\t72791889\tCAGGTAAGA\t1\t+\tsense\t5ss,T9A:CAGGTAAGA\thg18_chr11_72790846_72791902_+\t1034\n\n\nOutput (tab)::\n\n    Tmod4\t802\t5ss:CAGGTAAGT-rc\tACTTACCTG\n    Atp7b\t77\t5ss:CAGGTAAGT\tCAGGTAAGT\n    Fnta\t665\t5ss:CAGGTAAGT\tCAGGTAAGT\n\n\n\n  ",
        "id": "match",
        "interpreter": null,
        "language": null,
        "name": "match",
        "readme": false,
        "tests": false,
        "version": null
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btp340"
            }
        ],
        "code_file": null,
        "command": "sicer_wrapper.py \n  --bed_file '${input_bed_file}' \n  #if str( $input_control_file ) != 'None':\n      --control_file '${input_control_file}'\n      --significant_islands_output_file \"${significant_islands_output_file}\"\n      --islands_summary_output_file \"${islands_summary_output_file}\"\n      --significant_islands_summary_output_file \"${significant_islands_summary_output_file}\"\n  #end if\n  ${fix_off_by_one_errors}\n  --dbkey '${input_bed_file.dbkey}'\n  --redundancy_threshold '${redundancy_threshold}'\n  --window_size '${window_size}'\n  --fragment_size '${fragment_size}'\n  --effective_genome_fraction '${effective_genome_fraction}'\n  --gap_size '${gap_size}'\n  --error_cut_off '${error_cut_off}'\n  ##output files\n  --stdout \"${output_log_file}\"\n  --redundancy_removed_test_bed_output_file \"${redundancy_removed_test_bed_output_file}\"\n  --redundancy_removed_control_bed_output_file \"${redundancy_removed_control_bed_output_file}\"\n  --score_island_output_file \"${score_island_output_file}\"\n  --summary_graph_output_file \"${summary_graph_output_file}\"\n  --test_normalized_wig_output_file \"${test_normalized_wig_output_file}\"\n  --island_filtered_output_file \"${island_filtered_output_file}\"\n  --island_filtered_normalized_wig_output_file \"${island_filtered_normalized_wig_output_file}\"\n  ",
        "dataFormats": {
            "inputs": [
                "bed",
                "bed"
            ],
            "outputs": [
                "bed",
                "bed",
                "bedgraph",
                "wig",
                "interval",
                "bed",
                "wig",
                "interval",
                "interval",
                "interval",
                "txt"
            ]
        },
        "description": "Statistical approach for the Identification of ChIP-Enriched Regions",
        "help": "\n**What it does**\n\nSICER first and foremost is a filtering tool. Its main functions are::\n  \n  1. Delineation of the significantly ChIP-enriched regions, which can be used to associate with other genomic landmarks. \n  2. Identification of reads on the ChIP-enriched regions, which can be used for profiling and other quantitative analysis.\n\nView the original SICER documentation: http://home.gwu.edu/~wpeng/Software.htm.\n\n------\n\n.. class:: warningmark\n\n  By default, SICER creates files that do not conform to standards (e.g. BED files are closed, not half-open). This could have implications for downstream analysis.\n  To force the output of SICER to be formatted properly to standard file formats, check the **\"Fix off-by-one errors in output files\"** option.\n\n------\n\n**Citation**\nIf you use this tool in Galaxy, please cite Blankenberg D, et al. *In preparation.*\n\n  ",
        "id": "peakcalling_sicer",
        "interpreter": "python",
        "language": null,
        "name": "SICER",
        "readme": false,
        "tests": true,
        "version": "0.0.2"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n\n        #if $ref.is_of_type(\"fasta\")\n            cp '$ref' 'ref.fna' &&\n        #end if\n        #if $ref.is_of_type(\"genbank\")\n            cp '$ref' 'ref.gbk' &&\n        #end if\n        snippy\n            --outdir 'out'\n            --cpus \\${GALAXY_SLOTS:-1}\n            --ram \\$((\\${GALAXY_MEMORY_MB:-4096}/1024))\n            #if $ref.is_of_type(\"fasta\")\n                --ref 'ref.fna'\n            #end if\n            #if $ref.is_of_type(\"genbank\")\n                --ref 'ref.gbk'\n            #end if\n            --mapqual $adv.mapqual\n            --mincov $adv.mincov\n            --minfrac $adv.minfrac\n            --minqual $adv.minqual\n            #if $adv.rgid\n                --rgid '$adv.rgid'\n            #end if\n            #if $adv.bwaopt\n                --bwaopt '$adv.bwaopt'\n            #end if\n\n            #if str( $fastq_input.fastq_input_selector ) == \"paired\"\n                --R1 '$fastq_input.fastq_input1'\n                --R2 '$fastq_input.fastq_input2'\n            #end if\n            #if str( $fastq_input.fastq_input_selector ) == \"paired_collection\"\n                --R1 '$fastq_input.fastq_input.forward'\n                --R2 '$fastq_input.fastq_input.reverse'\n            #end if\n            #if str( $fastq_input.fastq_input_selector ) == \"single\"\n                --se '$fastq_input.fastq_input'\n            #end if\n            #if str( $fastq_input.fastq_input_selector ) == \"paired_iv\"\n                --peil '$fastq_input.fastq_input'\n            #end if\n\n        &&\n\n        #import re\n        #if str( $fastq_input.fastq_input_selector ) == \"paired\"\n            #set $dir_name = re.sub('[^\\w_]', '_', $fastq_input.fastq_input1.element_identifier)\n        #elif str( $fastq_input.fastq_input_selector ) == \"paired_collection\"\n            #set $dir_name = re.sub('[^\\w_]', '_', $fastq_input.fastq_input.name)\n        #else\n            #set $dir_name = re.sub('[^\\w_]', '_', $fastq_input.fastq_input.element_identifier)\n        #end if\n\n        mkdir -p ${dir_name} && cp -r out/reference out/snps.tab out/snps.aligned.fa out/snps.vcf ${dir_name}/ &&\n        tar -czf out.tgz ${dir_name}\n        #if \"outcon\" in str($outputs) and $adv.rename_cons\n          && sed -i 's/>.*/>${dir_name}/' out/snps.consensus.fa\n        #end if\n\n\n    ",
        "dataFormats": {
            "inputs": [
                "fasta,genbank",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger"
            ],
            "outputs": [
                "vcf",
                "gff3",
                "tabular",
                "tabular",
                "txt",
                "fasta",
                "fasta",
                "tabular",
                "bam",
                "zip"
            ]
        },
        "description": "\n      Snippy finds SNPs between a haploid reference genome and your NGS sequence reads.\n  ",
        "help": "\n\n**Snippy 4.3.6**\n\n    Snippy finds SNPs between a haploid reference genome and your NGS sequence reads. It will find both substitutions (snps) and insertions/deletions (indels).\n\n**Author**\n\n    Torsten Seemann\n\n**Inputs**\n\n    - NGS Reads in fastq format (single or paired end)\n    - Reference file in either fasta or genbank format\n\nIf the reference file is supplied in genbank format, snpeff will be called to determine the effect of any snps found.\n\n**Advanced options**\n\n    - mapping quality - Integer -  Minimum mapping quality to allow (default '60')\n\n    - minimum coverage - Integer - Minimum coverage of variant site (default '10')\n\n    - minimum fraction - Float - Minumum proportion for variant evidence (default '0.9')\n\n    - minimum quality - Float - Minumum QUALITY in VCF column 6 (default '100.0')\n\n    - rgid - String -         Use this @RG ID: in the BAM header (default '')\n\n    - bwaopt - Extra BWA MEM options, eg. -x pacbio (default '')\n\n**Further information**\n\n    For a much more in depth description of snippy and how it works, see https://github.com/tseemann/snippy\n\n    ",
        "id": "snippy",
        "interpreter": null,
        "language": null,
        "name": "snippy",
        "readme": false,
        "tests": true,
        "version": "4.3.6+galaxy1"
    },
    {
        "ciation": [
            {
                "citation": "@UNPUBLISHED{Seemann2013,\n      author = \"Seemann T\",\n      title = \"snippy: fast bacterial variant calling from NGS reads\",\n      year = \"2015\",\n      note = \"https://github.com/tseemann/snippy\"}"
            }
        ],
        "code_file": null,
        "command": "\n      #if str( $reftype.ref_type_selector ) == \"fasta\"\n        cp $reftype.ref foo.fna &&\n      #end if\n      #if str( $reftype.ref_type_selector ) == \"genbank\"\n        cp $reftype.ref foo.gbk &&\n      #end if\n      snippy\n      --outdir out\n      --cpus \"\\${GALAXY_SLOTS:-1}\"\n      #if str( $reftype.ref_type_selector ) == \"fasta\"\n        --ref foo.fna\n      #end if\n      #if str( $reftype.ref_type_selector ) == \"genbank\"\n        --ref foo.gbk\n      #end if\n      $cleanup\n      #if str( $advanced.is_advanced ) == \"advanced\"\n        --mapqual $advanced.mapqual\n        --mincov $advanced.mincov\n        --minfrac $advanced.minfrac\n        #if $advanced.rgid\n          --rgid $advanced.rgid\n        #end if\n        #if $advanced.bwaopt\n          --bwaopt $advanced.bwaopt\n        #end if\n      #end if\n      #if str( $fastq_input.fastq_input_selector ) == \"paired\"\n        --pe1 $fastq_input.fastq_input1\n        --pe2 $fastq_input.fastq_input2\n      #end if\n      #if str( $fastq_input.fastq_input_selector ) == \"paired_collection\"\n        --pe1 $fastq_input.fastq_input1.forward\n        --pe2 $fastq_input.fastq_input1.reverse\n      #end if\n      #if str( $fastq_input.fastq_input_selector ) == \"single\"\n        --se $fastq_input.fastq_input1\n      #end if\n      #if str( $fastq_input.fastq_input_selector ) == \"paired_iv\"\n        --peil $fastq_input.fastq_input1\n      #end if\n\n      &&\n\n      gunzip out/snps.depth.gz\n\n      &&\n\n      tar -czf out.tgz out\n\n\n    ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "genbank",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger,fasta",
                "fastqsanger"
            ],
            "outputs": [
                "vcf",
                "gff3",
                "tabular",
                "tabular",
                "txt",
                "fasta",
                "fasta",
                "tabular",
                "bam",
                "zip"
            ]
        },
        "description": null,
        "help": "\nSynopsis:\n  snippy 3.0 - fast bacterial variant calling from NGS reads\n\nAuthor:\n  Torsten Seemann <torsten.seemann@gmail.com>\n\nUsage:\n  snippy [options] --outdir <dir> --ref <ref> --pe1 <R1.fq.gz> --pe2 <R2.fq.gz>\n\n  snippy [options] --outdir <dir> --ref <ref> --se <454.fastq>\n\n  snippy [options] --outdir <dir> --ref <ref> --peil <velvet.fa.gz>\n\nOptions:\n  --help            This help\n\n  --version         Print version and exit\n\n  --citation        Print citation for referencing snippy\n\n  --quiet           No screen output (default OFF)\n\n  --cpus [N]        Maximum number of CPU cores to use (default '8')\n\n  --reference [X]   Reference genome. Supports FASTA, GenBank, EMBL (not GFF) (default '')\n\n  --outdir [X]      Output folder (default '')\n\n  --prefix [X]      Prefix for output files (default 'snps')\n\n  --force           Force overwrite of existing output folder (default OFF)\n\n  --pe1|R1|left [X] Reads, paired-end R1 (left) (default '')\n\n  --pe2|R2|right [X] Reads, paired-end R2 (right) (default '')\n\n  --se|single [X]   Single-end reads (default '')\n\n  --peil [X]        Reads, paired-end R1/R2 interleaved (default '')\n\n  --mapqual [n.n]   Minimum mapping quality to allow (default '60')\n\n  --mincov [N]      Minimum coverage of variant site (default '10')\n\n  --minfrac [n.n]   Minumum proportion for variant evidence (default '0.9')\n\n  --report          Produce long report with visual alignment (slow) (default OFF)\n\n  --cleanup         Remove all non-SNP files: BAMs, indices etc (default OFF)\n\n  --rgid [X]        Use this @RG ID: in the BAM header (default '')\n\n  --bwaopt [X]      Extra BWA MEM options, eg. -x pacbio (default '')\n\n    ",
        "id": "snippy",
        "interpreter": null,
        "language": null,
        "name": "snippy",
        "readme": false,
        "tests": true,
        "version": "0.2.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "spp_wrapper.py $options_file $output_narrow_peak $output_region_peak $output_peakshift_file $output_rdata_file $output_plot_file $output_default_file",
        "dataFormats": {
            "inputs": [
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam"
            ],
            "outputs": [
                "txt",
                "txt",
                "pdf",
                "txt",
                "txt",
                "txt"
            ]
        },
        "description": "SPP cross-correlation analysis package",
        "help": "\n**What it does**\n\nThis tool allows ChIP-seq peak calling using SPP\n\nThis set of programs operate on mapped Illumina single-end read datasets in tagAlign or BAM format.\n\nView the modified SPP documentation: http://code.google.com/p/phantompeakqualtools/\n\n------\n\n**Usage**\n\n**Determine strand cross-correlation peak**: Compute the predominant insert-size (fragment length) based on strand cross-correlation peak.\n\n**Peak calling**: Call Peaks and regions for punctate binding datasets.\n\n**IDR analysis**: Compute Data quality measures based on relative phantom peak.\n\n**Custom settings**: Enables all options available to SPP for custom analysis. \n\n------\n\n**Citation**\n\nAnshul Kundaje, Computer Science Dept., Stanford University, ENCODE Consortium, Personal Communication, Oct 2010\nKharchenko PK, Tolstorukov MY, Park PJ, Design and analysis of ChIP-seq experiments for DNA-binding proteins Nat Biotechnol. 2008 Dec;26(12):1351-9\n\nIntegration of SPP with Galaxy performed by Ziru Zhou ( ziruzhou@gmail.com ). Please send your comments/questions to help@modencode.org.\n  ",
        "id": "modencode_peakcalling_spp",
        "interpreter": "python",
        "language": null,
        "name": "SPP",
        "readme": false,
        "tests": false,
        "version": "1.10.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "spp_wrapper.py $options_file $output_narrow_peak $output_region_peak $output_peakshift_file $output_rdata_file $output_plot_file $output_default_file \\$SCRIPT_PATH",
        "dataFormats": {
            "inputs": [
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam",
                "bam"
            ],
            "outputs": [
                "txt",
                "txt",
                "pdf",
                "txt",
                "txt",
                "txt"
            ]
        },
        "description": "SPP cross-correlation analysis package",
        "help": "\n**What it does**\n\nThis tool allows ChIP-seq peak calling using SPP\n\nThis set of programs operate on mapped Illumina single-end read datasets in tagAlign or BAM format.\n\nView the modified SPP documentation: http://code.google.com/p/phantompeakqualtools/\n\n------\n\n**Usage**\n\n**Determine strand cross-correlation peak**: Compute the predominant insert-size (fragment length) based on strand cross-correlation peak.\n\n**Peak calling**: Call Peaks and regions for punctate binding datasets.\n\n**IDR analysis**: Compute Data quality measures based on relative phantom peak.\n\n**Custom settings**: Enables all options available to SPP for custom analysis. \n\n------\n\n**Citation**\n\nAnshul Kundaje, Computer Science Dept., Stanford University, ENCODE Consortium, Personal Communication, Oct 2010\nKharchenko PK, Tolstorukov MY, Park PJ, Design and analysis of ChIP-seq experiments for DNA-binding proteins Nat Biotechnol. 2008 Dec;26(12):1351-9\n\nIntegration of SPP with Galaxy performed by Ziru Zhou ( ziruzhou@gmail.com ). Please send your comments/questions to help@modencode.org.\n  ",
        "id": "spp_peak_calling",
        "interpreter": "python",
        "language": null,
        "name": "SPP",
        "readme": false,
        "tests": false,
        "version": "1.11.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btu135"
            },
            {
                "citation": "10.1093/bioinformatics/btq683"
            }
        ],
        "code_file": null,
        "command": "\n    echo \"lib1 $reads1 $reads2 $insert $error $orientation\" > libraryfile &&\n    perl `which SSPACE_Basic_v2.0.pl` -l libraryfile -s $contigs\n    #if $exten\n      -x 1\n    #end if\n    #if str($minoverlap)\n      -m $minoverlap\n    #end if\n    #if str($numofreads)\n      -o $numofreads\n    #end if\n    #if str($max_trim)\n      -t $max_trim\n    #end if\n    #if $unpaired\n      -u $unpaired\n    #end if\n    #if str($min_base_ratio)\n      -r $min_base_ratio\n    #end if\n    #if str($minlink)\n      -k $minlink\n    #end if\n    #if str($maxratio)\n      -a $maxratio\n    #end if\n    #if str($contigoverlap)\n      -n $contigoverlap\n    #end if\n    #if str($mincontig)\n      -z $mincontig\n    #end if\n    -T \\${GALAXY_SLOTS:-1} -b sspace\n  ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "fasta,fastq",
                "fasta,fastq",
                "fasta,fastq"
            ],
            "outputs": [
                "txt",
                "fasta",
                "txt",
                "txt"
            ]
        },
        "description": "scaffolder",
        "help": "\n**What it does**\n\nSSPACE is a script able to extend and scaffold pre-assembled contigs using one or more mate pairs or paired-end libraries, or even a combination.\n\n**License and citation**\n\nThis Galaxy tool is Copyright \u00c2\u00a9 2012-2014 `CRS4 Srl.`_ and is released under the `MIT license`_.\n\n.. _CRS4 Srl.: http://www.crs4.it/\n.. _MIT license: http://opensource.org/licenses/MIT\n\nYou can use this tool only if you agree to the license terms of: `SSPACE basic`_.\n\n.. _SSPACE basic: http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/sspacev12/\n\nIf you use this tool, please cite:\n\n- |Cuccuru2014|_\n- |Boetzer2011|_.\n\n.. |Cuccuru2014| replace:: Cuccuru, G., Orsini, M., Pinna, A., Sbardellati, A., Soranzo, N., Travaglione, A., Uva, P., Zanetti, G., Fotia, G. (2014) Orione, a web-based framework for NGS analysis in microbiology. *Bioinformatics* 30(13), 1928-1929\n.. _Cuccuru2014: http://bioinformatics.oxfordjournals.org/content/30/13/1928\n.. |Boetzer2011| replace:: Boetzer, M., Henkel, C. V., Jansen, H. J., Butler, D., Pirovano, W. (2011) Scaffolding pre-assembled contigs using SSPACE. *Bioinformatics* 27(4), 578-579\n.. _Boetzer2011: http://bioinformatics.oxfordjournals.org/content/27/4/578\n  ",
        "id": "sspace",
        "interpreter": null,
        "language": null,
        "name": "SSPACE",
        "readme": false,
        "tests": false,
        "version": "1.0.7"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "sm_Tax4Fun.pl \n        --input $input\n        --out $output\n        --reference $reference\n           ",
        "dataFormats": {
            "inputs": [
                "csv, tabular"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": " predicts  the  functional or  metabolic capabilities  of  microbial communities  based  on 16S  data  samples",
        "help": "\n\n\n.. class:: infomark\n\nPLEASE READ THIS MANUAL TO USE TAX4FUN: http://genoweb.toulouse.inra.fr/~formation/GALAXY_news/howto3_Tax4Fun.pdf\n\n.. image:: ${static_path}/images/tax4fun_wf.png\n\n\nComplete ready-to-use workflow available here : http://genoweb.toulouse.inra.fr/~formation/GALAXY_news/Galaxy-Workflow-Workflow_constructed_from_history__TAX4FUN_.ga\n\nPipeline:\n\n\n    This pipeline is used after FROGS pipeline.\n\n    A- First, convert BIOM file result of FROGS in standard BIOM thanks to the following Galaxy tool : \u00ab\u00a0FROGS BIOM to std BIOM Converts a FROGS BIOM in fully compatible BIOM. (Galaxy Version 1.1.0)\u00a0\u00bb\n    \n    B-\u00a0Then convert standard BIOM file in TSV file with \u00ab\u00a0FROGS BIOM to TSV Converts a BIOM file in TSV file. (Galaxy Version 2.1.0)\u00a0\u00bb tool.\n\n    C- Select only 2 columns: abundance totale and taxonomy. To recover these data from the previous dataset, you can use \u00ab\u00a0Cut columns from a table (Galaxy Version 1.0.2)\u00a0\u00bb to generate 2 files:  one file with Taxonomy and another with abundance_sum. The tool \u00ab\u00a0Paste two files side by side (Galaxy Version 1.0.0)\u00a0\u00bb paste these 2 files in one (first column: abundance, second column: taxonomy).\n\n    .. image:: ${static_path}/images/tax4fun_inputfile.png\n\n    E- Then \u00ab\u00a0Tax4Fun predicts the functional or metabolic capabilities of microbial communities based on 16S data samples (Galaxy Version 1.0.0)\u00a0\u00bb sort data, rewrite taxonomy, sum abundance for the same taxonomy.  \n\nWarning: The only available reference is SILVA123.\n\n\n.. class:: infomark\n\n\nPackage Tax4Fun : Prediction of functional profiles from amplicon-data\n\n\nDescription:\n\n     Prediction of functional and metabolic profiles from amplicon-data using the Tax4Fun approach\n\nUsage:\n\n    Tax4Fun(Tax4FunInput, folderReferenceData, fctProfiling = TRUE, refProfile = \"UProC\", shortReadMode = TRUE, normCopyNo = TRUE)\n\nArguments:\n\n    Tax4FunInput: (required): list containing the OTU table and sample names, e.g. imported with the functions \u2018importQIIMEBiomData\u2019, \u2018importQIIMEData\u2019, or \u2018importSilvaNgsData\u2019 folderReferenceData: (required): a character vector with one character string indicating the folder location of the unzipped reference data.\n\n\nDetails:\n\n     Tax4Fun predicts the functional and metabolic capabilities of microbial communities based on 16S data samples. Tax4Fun provides a good functional approximation to functional profiles obtained through metagenome sequencing. Tax4Fun can be used as a first instance functional profiling tool for an estimate of the functional capabilities of microbial communities based on amplicon data. Tax4Fun is applicable to output as obtained through the SILVAngs web server or the application of QIIME against the SILVA database.\n\nValue:\n\n     A list containing the predicted functional or metabolic profiles and the FTU statistics\n\n\nAuthor(s) Tax4Fun:\n\n     Kathrin P. Asshauer email: kathrin@gobics.de\n\nAuthor(s) Galaxy Wrapper Tax4Fun:\n\n     Sigenae Team.\n\n\nReferences:\n\n      http://gobics.de/kathrin/Tax4Fun/Tax4Fun.html\n\n\n       \n---\n\nVersion Galaxy Tool : V1.0.1\n\n---\n\nContacts : support.sigenae@inra.fr \n\n    E-learning available : http://genoweb.toulouse.inra.fr/~formation/GALAXY_news/howto3_Tax4Fun.pdf\n\nPlease cite :\n        \n    Depending on the help provided you can cite us in acknowledgements, references or both.\n    \n    Example of acknowledgements: We wish to thank SIGENAE group and UMR-1280 PhAN Inra-Universite de Nantes.\n        \n    Example of references : X. SIGENAE [http://www.sigenae.org/]\n\n  ",
        "id": "sm_Tax4Fun",
        "interpreter": "perl",
        "language": null,
        "name": "Tax4Fun",
        "readme": true,
        "tests": false,
        "version": null
    },
    {
        "ciation": [
            {
                "citation": "\n      @misc{github01_mismatch_removal,\n      author = {LastTODO, FirstTODO},\n      year = {TODO},\n      title = {01_mismatch_removal},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      url = {https://github.com/dktanwar/Galaxy_Tools/tree/master/01_mismatch_removal},\n      }"
            }
        ],
        "code_file": null,
        "command": "\nRscript $__tool_directory__/csaw.R $inputs --output $output\n    ",
        "dataFormats": {
            "inputs": [
                "bam"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": null,
        "help": "\n        Write the tool help section here.\n    ",
        "id": "CSAW",
        "interpreter": null,
        "language": null,
        "name": "CSAW",
        "readme": false,
        "tests": true,
        "version": "0.1.0"
    },
    {
        "ciation": [
            {
                "citation": "\n      @misc{github01_mismatch_removal,\n      author = {LastTODO, FirstTODO},\n      year = {TODO},\n      title = {01_mismatch_removal},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      url = {https://github.com/dktanwar/Galaxy_Tools/tree/master/01_mismatch_removal},\n      }"
            }
        ],
        "code_file": null,
        "command": "\nRscript $__tool_directory__/csaw.R $inputs $output\n    ",
        "dataFormats": {
            "inputs": [
                "bam"
            ],
            "outputs": [
                "txt"
            ]
        },
        "description": null,
        "help": "\n        Write the tool help section here.\n    ",
        "id": "CSAW",
        "interpreter": null,
        "language": null,
        "name": "CSAW",
        "readme": false,
        "tests": true,
        "version": "0.1.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1038/nbt.1621"
            }
        ],
        "code_file": null,
        "command": "\n        cufflinks_wrapper.py \n            --input=$input\n            --assembled-isoforms-output=$assembled_isoforms\n            --num-threads=\"\\${GALAXY_SLOTS:-4}\"\n            -I $max_intron_len\n            -F $min_isoform_fraction\n            -j $pre_mrna_fraction\n            $length_correction\n            \n            ## Include reference annotation?\n            #if $reference_annotation.use_ref == \"Use reference annotation\":\n                -G $reference_annotation.reference_annotation_file\n                $reference_annotation.compatible_hits_norm\n            #end if\n            #if $reference_annotation.use_ref == \"Use reference annotation guide\":\n                -g $reference_annotation.reference_annotation_guide_file\n                --3-overhang-tolerance=$reference_annotation.three_overhang_tolerance\n                --intron-overhang-tolerance=$reference_annotation.intron_overhang_tolerance\n                $reference_annotation.no_faux_reads\n            #end if\n            \n            ## Bias correction?\n            #if $bias_correction.do_bias_correction == \"Yes\":\n                -b\n                #if $bias_correction.seq_source.index_source == \"history\":\n                    --ref_file=$bias_correction.seq_source.ref_file\n                #else:\n                    --index=${bias_correction.seq_source.index.fields.path}\n                #end if\n            #end if\n\n            ## Multi-read correct?\n            #if str($multiread_correct) == \"Yes\":\n            -u\n            #end if\n\n            ## Include global model if available.\n            #if $global_model:\n                --global_model=$global_model\n            #end if\n\n            ## advanced settings\n            #if $advanced_settings.use_advanced_settings == \"Yes\":\n            --library-type=$advanced_settings.library_type\n            #if $advanced_settings.mask_file:\n                --mask-file=$advanced_settings.mask_file\n                #end if\n            --inner-mean-dist=$advanced_settings.inner_mean_dist\n            --inner-dist-std-dev=$advanced_settings.inner_dist_std_dev\n            --max-mle-iterations=$advanced_settings.max_mle_iterations\n            --junc-alpha=$advanced_settings.junc_alpha\n            --small-anchor-fraction=$advanced_settings.small_anchor_fraction\n            --overhang-tolerance=$advanced_settings.overhang_tolerance\n            --max-bundle-length=$advanced_settings.max_bundle_length\n            --max-bundle-frags=$advanced_settings.max_bundle_frags\n            --min-intron-length=$advanced_settings.min_intron_length\n            --trim-3-avgcov-thresh=$advanced_settings.trim_three_avgcov_thresh\n            --trim-3-dropoff-frac=$advanced_settings.trim_three_dropoff_frac\n            #end if\n\n    ",
        "dataFormats": {
            "inputs": [
                "sam,bam",
                "gff3,gtf",
                "gff3,gtf",
                "fasta",
                "gff3,gtf"
            ],
            "outputs": [
                "tabular",
                "tabular",
                "gtf",
                "txt",
                "gtf"
            ]
        },
        "description": "transcript assembly and FPKM (RPKM) estimates for RNA-Seq data",
        "help": "\n**Cufflinks Overview**\n\nCufflinks_ assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. It accepts aligned RNA-Seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one.  Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621\n\n.. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/\n\n------\n\n**Know what you are doing**\n\n.. class:: warningmark\n\nThere is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to **understand** the parameters by carefully reading the `documentation`__ and experimenting. Fortunately, Galaxy makes experimenting easy.\n\n.. __: http://cole-trapnell-lab.github.io/cufflinks/cufflinks/\n\n------\n\n**Input formats**\n\nCufflinks takes a text file of SAM alignments as input. The RNA-Seq read mapper TopHat produces output in this format, and is recommended for use with Cufflinks. However Cufflinks will accept SAM alignments generated by any read mapper. Here's an example of an alignment Cufflinks will accept::\n\n  s6.25mer.txt-913508    16    chr1 4482736 255 14M431N11M * 0 0 \\\n     CAAGATGCTAGGCAAGTCTTGGAAG IIIIIIIIIIIIIIIIIIIIIIIII NM:i:0 XS:A:-\n    \nNote the use of the custom tag XS. This attribute, which must have a value of \"+\" or \"-\", indicates which strand the RNA that produced this read came from. While this tag can be applied to any alignment, including unspliced ones, it must be present for all spliced alignment records (those with a 'N' operation in the CIGAR string).\nThe SAM file supplied to Cufflinks must be sorted by reference position. If you aligned your reads with TopHat, your alignments will be properly sorted already. If you used another tool, you may want to make sure they are properly sorted as follows::\n\n  sort -k 3,3 -k 4,4n hits.sam > hits.sam.sorted\n\nNOTE: Cufflinks currently only supports SAM alignments with the CIGAR match ('M') and reference skip ('N') operations. Support for the other operations, such as insertions, deletions, and clipping, will be added in the future.\n\n------\n\n**Outputs**\n\nCufflinks produces three output files:\n\nTranscripts and Genes:\n\nThis GTF file contains Cufflinks' assembled isoforms. The first 7 columns are standard GTF, and the last column contains attributes, some of which are also standardized (e.g. gene_id, transcript_id). There one GTF record per row, and each record represents either a transcript or an exon within a transcript. The columns are defined as follows::\n\n  Column number   Column name   Example     Description\n  -----------------------------------------------------\n  1               seqname       chrX        Chromosome or contig name\n  2               source        Cufflinks   The name of the program that generated this file (always 'Cufflinks')\n  3               feature       exon        The type of record (always either \"transcript\" or \"exon\").\n  4               start         77696957    The leftmost coordinate of this record (where 0 is the leftmost possible coordinate)\n  5               end           77712009    The rightmost coordinate of this record, inclusive.\n  6               score         77712009    The most abundant isoform for each gene is assigned a score of 1000. Minor isoforms are scored by the ratio (minor FPKM/major FPKM)\n  7               strand        +           Cufflinks' guess for which strand the isoform came from. Always one of '+', '-' '.'\n  7               frame         .           Cufflinks does not predict where the start and stop codons (if any) are located within each transcript, so this field is not used.\n  8               attributes    See below\n  \nEach GTF record is decorated with the following attributes::\n\n  Attribute       Example       Description\n  -----------------------------------------\n  gene_id         CUFF.1        Cufflinks gene id\n  transcript_id   CUFF.1.1      Cufflinks transcript id\n  FPKM            101.267       Isoform-level relative abundance in Reads Per Kilobase of exon model per Million mapped reads\n  frac            0.7647        Reserved. Please ignore, as this attribute may be deprecated in the future\n  conf_lo         0.07          Lower bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, lower bound = FPKM * (1.0 - conf_lo)\n  conf_hi         0.1102        Upper bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, upper bound = FPKM * (1.0 + conf_lo)\n  cov             100.765       Estimate for the absolute depth of read coverage across the whole transcript\n  \n\nTranscripts only:\n  This file is simply a tab delimited file containing one row per transcript and with columns containing the attributes above. There are a few additional attributes not in the table above, but these are reserved for debugging, and may change or disappear in the future.\n    \nGenes only:\nThis file contains gene-level coordinates and expression values.\n    \n-------\n\n**Cufflinks settings**\n\nAll of the options have a default value. You can change any of them. Most of the options in Cufflinks have been implemented here.\n\n------\n\n**Cufflinks parameter list**\n\nThis is a list of implemented Cufflinks options::\n\n  -m INT    This is the expected (mean) inner distance between mate pairs. For, example, for paired end runs with fragments selected at 300bp, where each end is 50bp, you should set -r to be 200. The default is 45bp.\n  -s INT    The standard deviation for the distribution on inner distances between mate pairs. The default is 20bp.\n  -I INT    The minimum intron length. Cufflinks will not report transcripts with introns longer than this, and will ignore SAM alignments with REF_SKIP CIGAR operations longer than this. The default is 300,000.\n  -F         After calculating isoform abundance for a gene, Cufflinks filters out transcripts that it believes are very low abundance, because isoforms expressed at extremely low levels often cannot reliably be assembled, and may even be artifacts of incompletely spliced precursors of processed transcripts. This parameter is also used to filter out introns that have far fewer spliced alignments supporting them. The default is 0.05, or 5% of the most abundant isoform (the major isoform) of the gene.\n  -j        Some RNA-Seq protocols produce a significant amount of reads that originate from incompletely spliced transcripts, and these reads can confound the assembly of fully spliced mRNAs. Cufflinks uses this parameter to filter out alignments that lie within the intronic intervals implied by the spliced alignments. The minimum depth of coverage in the intronic region covered by the alignment is divided by the number of spliced reads, and if the result is lower than this parameter value, the intronic alignments are ignored. The default is 5%.\n  -G        Tells Cufflinks to use the supplied reference annotation to estimate isoform expression. It will not assemble novel transcripts, and the program will ignore alignments not structurally compatible with any reference transcript.  \n  -N        With this option, Cufflinks excludes the contribution of the top 25 percent most highly expressed genes from the number of mapped fragments used in the FPKM denominator. This can improve robustness of differential expression calls for less abundant genes and transcripts.\n    ",
        "id": "cufflinks",
        "interpreter": "python",
        "language": null,
        "name": "Cufflinks",
        "readme": true,
        "tests": true,
        "version": "2.2.1.0"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n            bash $__tool_directory__/run.sh $__tool_directory__ &&\n            python $__tool_directory__/chimerascan_run.py -p 8 $__tool_directory__/myindex\n            #if $input_type_conditional.chimerascan_input_type == \"paired\"\n                    $input_type_conditional.input_1 $input_type_conditional.input_2\n            #else\n                    $input_type_conditional.input.forward $input_type_conditional.input.reverse\n            #end if\n            $galaxy_output\n    \n    ",
        "dataFormats": {
            "inputs": [
                "fastq",
                "fastq",
                "fastq"
            ],
            "outputs": [
                "bed"
            ]
        },
        "description": "A tool for identifying chimeric transcription in sequencing data.",
        "help": "\n\tBowtie index files must be placed inside 'myindex folder'\n        A tool for identifying chimeric transcription in sequencing data.\n    ",
        "id": "chimerascan",
        "interpreter": null,
        "language": null,
        "name": "ChimeraScan",
        "readme": true,
        "tests": true,
        "version": null
    },
    {
        "ciation": [
            {
                "citation": "10.1101/gr.129684.111"
            }
        ],
        "code_file": null,
        "command": "\n        ## Set up samples list file.\n        #if $sample_names.strip() != '':\n           echo $sample_names | awk -F ',' '{ for (i = 1; i <= NF; i++) { print \\$i; } }' > samples_list.txt &&\n        #end if\n\n        ## Set up command + input.\n        varscan ${cmd} ${input}\n        --min-coverage ${min_coverage} \n        --min-reads2 ${min_supporting_reads} \n        --min-avg-qual ${min_avg_qual}\n        --min-var-freq ${min_var_freq}\n        --min-freq-for-hom ${min_freq_for_hom}\n        --p-value ${p_value}\n        #if str($strand_filter) == 'yes':\n          --strand-filter 1\n        #end if\n\n        ## Report only variants in consensus.\n        #if str($cmd) == 'mpileup2cns':\n          --variants\n        #end if\n        \n        ## Set up outputs.\n        --output-vcf 1 > $output\n\n        #if $sample_names.strip() != '':\n            --vcf-sample-list samples_list.txt\n        #end if\n    ",
        "dataFormats": {
            "inputs": [
                "pileup"
            ],
            "outputs": [
                "vcf"
            ]
        },
        "description": "for variant detection",
        "help": "\n**VarScan Overview**\n\nVarScan_ performs variant detection for massively parallel sequencing data, such as exome, WGS, and transcriptome data. It calls variants from a mpileup dataset and produces a VCF 4.1 Full documentation is available online_.\n\n.. _VarScan: http://dkoboldt.github.io/varscan/\n.. _online: http://dkoboldt.github.io/varscan/using-varscan.html\n\n**Input**\n\n::\n\n  mpileup file - The SAMtools mpileup file\n \n\n**Output**\n\nVarScan produces a VCF 4.1 dataset as output.\n\n**Parameters**\n\n::\n\n  analysis type\n    single nucleotide detection     Identify SNPs from an mpileup file\n    insertions and deletion       Identify indels an mpileup file\n    consensus genotype     Call consensus and variants from an mpileup file\n\n  min-coverage  \n    Minimum read depth at a position to make a call [8]\n\n  min-reads2    \n    Minimum supporting reads at a position to call variants [2]\n\n  min-avg-qual  \n    Minimum base quality at a position to count a read [15]\n\n  min-var-freq  \n        Minimum variant allele frequency threshold [0.01]\n\n  min-freq-for-hom\n    Minimum frequency to call homozygote [0.75]\n  \n  p-value\n    Default p-value threshold for calling variants [99e-02]\n  \n  strand-filter\n    Ignore variants with >90% support on one strand [1]\n  \n  output-vcf\n    If set to 1, outputs in VCF format\n  \n  vcf-sample-list\n    For VCF output, a list of sample names in order, one per line\n  \n  variants\n    Report only variant (SNP/indel) positions [0]\n    ",
        "id": "varscan",
        "interpreter": null,
        "language": null,
        "name": "VarScan",
        "readme": false,
        "tests": true,
        "version": "2.4.2"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    vcfPytools.py\n      filter \n      --in=$input1\n      --out=$output1\n      --quality=$quality\n      #for $i in $info_filter:\n        --info ${i.info}\n      #end for\n      $remove_genotypes\n      $mark_as_pass\n  ",
        "dataFormats": {
            "inputs": [
                "vcf"
            ],
            "outputs": [
                "vcf"
            ]
        },
        "description": "a VCF file",
        "help": "\n\n**What it does**\n\nThis tool uses vcfPytools_' filter command\n\n.. _vcfPytools: https://github.com/AlistairNWard/vcfPytools\n\nQuality option will check the variant quality for each record and if it is below the defined value, the filter field will be populated with the filter entry Q[value].\n\nAny value in the info string can be used for filtering by using the 'Filter by info' option.  This option takes three values: the info string tag, the cutoff value and whether to filter out those records with less than (lt) or greater than (gt) this value.  For example:\n\n  DP 10 lt \n\nwould filter out all varianta with a depth (DP) less than 10 and the filter field would be populated with DP10.\n\nThis option can be defined as many times as required.\n\n  ",
        "id": "vcf_filter",
        "interpreter": "python",
        "language": null,
        "name": "Filter",
        "readme": false,
        "tests": true,
        "version": "1.0.0"
    },
    {
        "ciation": [
            {
                "citation": "10.1093/bioinformatics/btr330"
            }
        ],
        "code_file": null,
        "command": "\n    \n        ln -s '${variants}' variants.vcf &&\n        vcf-sort variants.vcf | bgzip -c > variants.sorted.vcf.gz &&\n\n        tabix -p vcf variants.sorted.vcf.gz  &&\n\n        #if $ref_genome_source.index_source == 'history':\n          ln -s '${ref_genome_source.ref_file}' reference.fasta &&\n          samtools faidx reference.fasta &&\n        #end if\n\n        cat\n        #if $ref_genome_source.index_source == 'builtin'\n          '${ ref_genome_source.reference_genome.fields.path }'\n        #else\n          reference.fasta\n        #end if\n        | vcf-consensus variants.sorted.vcf.gz > '${output}'\n    \n    ",
        "dataFormats": {
            "inputs": [
                "fasta",
                "vcf"
            ],
            "outputs": [
                "fasta"
            ]
        },
        "description": "Apply VCF variants to a fasta file to create consensus sequence",
        "help": "\n        Please see the VCFtools `documentation`__ for help and further information.\n\n        .. __: http://vcftools.sourceforge.net/docs.html\n    ",
        "id": "vcftools_consensus",
        "interpreter": null,
        "language": null,
        "name": "Consensus",
        "readme": false,
        "tests": true,
        "version": "0.1.11"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    zcat -f $input | fastx_clipper -s $maxmismatches -l $minlength -a $clip_source.clip_sequence -d $keepdelta -o $output -v $KEEP_N $DISCARD_OPTIONS\n  ",
        "dataFormats": {
            "inputs": [
                "fasta,fastqsolexa"
            ],
            "outputs": [
                "input"
            ]
        },
        "description": "adapter sequences",
        "help": "use this for hairpin barcoding. keep at 0 unless you know what you're doing.",
        "id": "cshl_fastx_clipper",
        "interpreter": null,
        "language": null,
        "name": "Clip",
        "readme": true,
        "tests": true,
        "version": "1.0.1"
    },
    {
        "ciation": null,
        "code_file": null,
        "command": "\n    zcat -f $input | fastx_clipper $GZIPOUT -s $maxmismatches -l $minlength -a $clip_source.clip_sequence -d $keepdelta -o $output -v $KEEP_N $DISCARD_OPTIONS\n  ",
        "dataFormats": {
            "inputs": [
                "fasta,fastqsolexa"
            ],
            "outputs": [
                "input"
            ]
        },
        "description": "adapter sequences",
        "help": "use this for hairpin barcoding. keep at 0 unless you know what you're doing.",
        "id": "cshl_fastx_clipper",
        "interpreter": null,
        "language": null,
        "name": "Clip",
        "readme": true,
        "tests": true,
        "version": "1.0.1"
    }
]